

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="zhichao zhao">
  <meta name="keywords" content="">
  <title>pytorch基本概念 - 假欢畅 又何妨 无人共享</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/tomorrow.min.css" />
    
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_pf9vaxs7x7b.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.1.1"></head>


<body>
  <header style="height: 60vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>假欢畅，又何妨，无人共享</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container page-header text-center fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      <div class="container nopadding-md">
        <div class="py-5" id="board"
          >
          
          <div class="container">
            <div class="row">
              <div class="col-12 col-md-10 m-auto">
                

<div class="page-content">
  <p>Pytorch几个基本概念的区分</p>
<a id="more"></a>
<h3 id="1-numpy-array-和-Tensor-CPU-amp-GPU"><a href="#1-numpy-array-和-Tensor-CPU-amp-GPU" class="headerlink" title="1. numpy array 和 Tensor (CPU &amp; GPU)"></a>1. numpy array 和 Tensor (CPU &amp; GPU)</h3><pre><code class="hljs shell"><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; import torch</span>
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; import numpy as np</span>
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; a = np.ones(5)</span>
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; a</span>
array([1., 1., 1., 1., 1.])
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; b = torch.from_numpy(a)     <span class="hljs-comment"># numpy array-&gt; CPU Tensor</span></span>
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; b </span>
tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y = y.cuda()     <span class="hljs-comment"># CPU Tensor -&gt; GPU Tensor</span></span>
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y</span>
tensor([1., 1., 1., 1., 1.], device=&#x27;cuda:0&#x27;, dtype=torch.float64)
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y = y.cpu()  <span class="hljs-comment"># GPU Tensor-&gt; CPU Tensor</span></span>
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y</span>
tensor([1., 1., 1., 1., 1.], dtype=torch.float64)
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y = y.numpy()  <span class="hljs-comment"># CPU Tensor -&gt; numpy array</span></span>
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y</span>
array([1., 1., 1., 1., 1.])

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y = torch.from_numpy(y)</span>
<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y.to(device)   <span class="hljs-comment"># 这里 x.to(device) 等价于 x.cuda()</span></span>
tensor([1., 1., 1., 1., 1.], device=&#x27;cuda:0&#x27;, dtype=torch.float64)</code></pre>
<p>索引、 view 是不会开辟新内存的，而像 y = x + y 这样的运算是会新开内存的，然后将 y 指向新内存。</p>
<h3 id="2-Variable-和-Tensor-require-grad-True"><a href="#2-Variable-和-Tensor-require-grad-True" class="headerlink" title="2. Variable 和 Tensor (require_grad=True)"></a>2. Variable 和 Tensor (require_grad=True)</h3><p>​    Pytorch 0.4 之前的模式为:　<strong>Tensor 没有梯度计算，加上梯度更新等操作后可以变为Variable</strong>.  Pytorch0.4 将 Tensor 和Variable 合并。默认 Tensor 的 require_grad 为 false，可以通过修改 requires_grad 来为其添加梯度更新操作。</p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>y
tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>], dtype=torch.float64)  
<span class="hljs-meta">&gt;&gt;&gt; </span>y.requires_grad
<span class="hljs-literal">False</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>y.requires_grad = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>y
tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>], dtype=torch.float64, requires_grad=<span class="hljs-literal">True</span>)</code></pre>
<h3 id="3-detach-和-with-torch-no-grad"><a href="#3-detach-和-with-torch-no-grad" class="headerlink" title="3. detach 和　with torch.no_grad()"></a>3. detach 和　with torch.no_grad()</h3><p>一个比较好的 detach和 torch.no_grad区别的解释:</p>
<blockquote>
<p><strong><code>detach()</code> detaches the output from the computationnal graph. So no gradient will be backproped along this variable.</strong></p>
<p><strong><code>torch.no_grad</code> says that no operation should build the graph.</strong></p>
<p><strong>The difference is that one refers to only a given variable on which it’s called. The other affects all operations taking place within the with statement.</strong></p>
</blockquote>
<p><code>detach()</code> 将一个变量从计算图中分离出来，也没有了相关的梯度更新。<code>torch.no_grad()</code>只是说明该操作没必要建图。不同之处在于，前者只能指定一个给定的变量，后者 则会影响影响在 with 语句中发生的所有操作。</p>
<h3 id="4-model-eval-和-torch-no-grad"><a href="#4-model-eval-和-torch-no-grad" class="headerlink" title="4. model.eval()　和 torch.no_grad()"></a>4. model.eval()　和 torch.no_grad()</h3><blockquote>
<p><strong>These two have different goals:</strong></p>
<ul>
<li><p><code>model.eval()</code> will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.</p>
</li>
<li><p><code>torch.no_grad()</code> impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script).</p>
</li>
</ul>
</blockquote>
<p><code>model.eval()</code>和<code>torch.no_grad()</code>的区别在于，<code>model.eval()</code>是将网络切换为测试状态，例如 BN 和随机失活（dropout）在训练和测试阶段使用不同的计算方法。<code>torch.no_grad()</code>是关闭PyTorch张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行<code>loss.backward()</code></p>
<h3 id="5-model-train-和-model-eval-的不同"><a href="#5-model-train-和-model-eval-的不同" class="headerlink" title="5. model.train() 和 model.eval() 的不同"></a>5. model.train() 和 model.eval() 的不同</h3><ul>
<li><code>model.train()</code>  ——  训练时候启用， 会计算对应 Tensor 的梯度<br>启用 BatchNormalization 和 Dropout，将 BatchNormalization 和 Dropout 置为 True</li>
<li><code>model.eval()</code>  ——  验证和测试时候启用， 不会计算对应 Tensor 的梯度<br>不启用 BatchNormalization 和 Dropout，将 BatchNormalization 和 Dropout 置为 False</li>
</ul>
<h3 id="6-xx-data-和-xx-detach"><a href="#6-xx-data-和-xx-detach" class="headerlink" title="6. xx.data 和 xx.detach()"></a>6. xx.data 和 xx.detach()</h3><p>​      在 0.4.0 版本之前,  .data 的语义是 获取 Variable 的 内部 Tensor, 在 0.4.0 版本将 Variable 和 Tensor merge 之后,  <code>.data</code> 和之前有类似的语义， 也是内部的 Tensor 的概念。<code>x.data</code> 与 <code>x.detach()</code> 返回的 tensor 有相同的地方, 也有不同的地方:</p>
<p><strong>相同:</strong></p>
<ul>
<li>都和 x 共享同一块数据</li>
<li>都和 x 的 计算历史无关</li>
<li>requires_grad = False</li>
</ul>
<p><strong>不同:</strong></p>
<ul>
<li>y= x.data 在某些情况下不安全, 某些情况, 指的就是上述 inplace operation 的第二种情况, 所以, release note 中指出, 如果想要 detach 的效果的话, 还是 detach() 安全一些.</li>
</ul>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.FloatTensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>w1 = torch.FloatTensor([[<span class="hljs-number">2.</span>], [<span class="hljs-number">1.</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>w2 = torch.FloatTensor([<span class="hljs-number">3.</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>w1.requires_grad = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>w2.requires_grad = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>d = torch.matmul(x, w1)
<span class="hljs-meta">&gt;&gt;&gt; </span>d_ = d.data
<span class="hljs-meta">&gt;&gt;&gt; </span>f = torch.matmul(d, w2)
<span class="hljs-meta">&gt;&gt;&gt; </span>d_[:] = <span class="hljs-number">1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>f.backward()</code></pre>
<p><strong>如果需要获取其值，可以使用  xx.cpu().numpy() 或者 xx.cpu().detach().numpy() 然后进行操作，不建议再使用 volatile和  xx.data操作。</strong></p>
<h3 id="7-ToTensor-amp-ToPILImage-各自都做了什么"><a href="#7-ToTensor-amp-ToPILImage-各自都做了什么" class="headerlink" title="7. ToTensor &amp; ToPILImage 各自都做了什么?"></a>7. ToTensor &amp; ToPILImage 各自都做了什么?</h3><p><strong>ToTensor:</strong></p>
<ul>
<li>取值范围：   [0, 255]  —&gt;  [0, 1.0]</li>
<li>NHWC  —&gt; NCHW</li>
<li>PILImage  —&gt; FloatTensor</li>
</ul>
<pre><code class="hljs python"><span class="hljs-comment"># PIL.Image -&gt; torch.Tensor.</span>
tensor = torch.from_numpy(np.asarray(PIL.Image.open(path))
    ).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>).float() / <span class="hljs-number">255</span>
<span class="hljs-comment">#　等价于</span>
tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path))</code></pre>
<p><strong>ToPILImage:</strong></p>
<ul>
<li>取值范围:  [0, 1.0] —&gt;  [0, 255]</li>
<li>NCHW —&gt; NHWC</li>
<li>类型: FloatTensor -&gt; numpy Uint8 -&gt; PILImage</li>
</ul>
<pre><code class="hljs python"><span class="hljs-comment"># torch.Tensor -&gt; PIL.Image.</span>
image = PIL.Image.fromarray(torch.clamp(tensor * <span class="hljs-number">255</span>, min=<span class="hljs-number">0</span>, max=<span class="hljs-number">255</span>
    ).byte().permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>).cpu().numpy())
<span class="hljs-comment">#　等价于</span>
image = torchvision.transforms.functional.to_pil_image(tensor)</code></pre>
<h3 id="8-torch-nn-xxx-与-torch-nn-functional-xxx"><a href="#8-torch-nn-xxx-与-torch-nn-functional-xxx" class="headerlink" title="8. torch.nn.xxx 与 torch.nn.functional.xxx"></a>8. torch.nn.xxx 与 torch.nn.functional.xxx</h3><p>建议统一使用　<code>torch.nn.xxx</code>　模块，<code>torch.functional.xxx</code> 可能会在下一个版本中去掉。</p>
<p><code>torch.nn</code>　模块和　<code>torch.nn.functional</code>　的区别在于，<code>torch.nn</code>　模块在计算时底层调用了<code>torch.nn.functional</code>，但　<code>torch.nn</code>　模块包括该层参数，还可以应对训练和测试两种网络状态。使用　<code>torch.nn.functional</code>　时要注意网络状态，如:</p>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>
    ...
    x = torch.nn.functional.dropout(x, p=<span class="hljs-number">0.5</span>, training=self.training)</code></pre>
<h3 id="9-pytorch-中-torch-Tensor-和-torch-tensor-有什么相同点和区别"><a href="#9-pytorch-中-torch-Tensor-和-torch-tensor-有什么相同点和区别" class="headerlink" title="9. pytorch 中 torch.Tensor() 和 torch.tensor() 有什么相同点和区别?"></a>9. pytorch 中 torch.Tensor() 和 torch.tensor() 有什么相同点和区别?</h3><ul>
<li><p>相同点 </p>
<p>都能用于生成新的张量</p>
</li>
<li><p>不同点：</p>
</li>
</ul>
<p>torch.Tensor()是 python类，更明确地说，是默认张量类型torch.FloatTensor() 的别名，torch.Tensor([1,2])会调用Tensor类的构造函数 _<em>init_</em>，生成单精度浮点类型的张量。</p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a=torch.Tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>a.type()
<span class="hljs-string">&#x27;torch.FloatTensor&#x27;</span>
<span class="hljs-number">123</span></code></pre>
<p>而 torch.tensor()  是 python 函数：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#torch.tensor">https://pytorch.org/docs/stable/torch.html#torch.tensor</a> ，函数原型是：</p>
<pre><code class="hljs python">torch.tensor(data, dtype=<span class="hljs-literal">None</span>, device=<span class="hljs-literal">None</span>, requires_grad=<span class="hljs-literal">False</span>)</code></pre>
<p>其中 data 可以是：list, tuple, NumPy ndarray, scalar 和其他类型。<br>torch.tensor 会从 data 中的数据部分做拷贝，根据原始数据类型生成相应的torch.LongTensor、torch.FloatTensor 和torch.DoubleTensor。</p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a=torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>a.type()
<span class="hljs-string">&#x27;torch.LongTensor&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a=torch.tensor([<span class="hljs-number">1.</span>,<span class="hljs-number">2.</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>a.type()
<span class="hljs-string">&#x27;torch.FloatTensor&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a=np.zeros(<span class="hljs-number">2</span>,dtype=np.float64)
<span class="hljs-meta">&gt;&gt;&gt; </span>a=torch.tensor(a)
<span class="hljs-meta">&gt;&gt;&gt; </span>a.type()
<span class="hljs-string">&#x27;torch.DoubleTensor&#x27;</span></code></pre>
<h3 id="10-tensorflow-和-pytorch-构建网络的差异"><a href="#10-tensorflow-和-pytorch-构建网络的差异" class="headerlink" title="10. tensorflow 和 pytorch 构建网络的差异"></a>10. tensorflow 和 pytorch 构建网络的差异</h3><p>[如果是比较 Tensorflow 和 Pytorch 的差异， 可以从: 上手难易程度、静态图vs动态图、调试、可视化、部署等方面进行分析]</p>
<p> <strong>图的动态定义 vs 静态定义</strong></p>
<p>​      两个框架都是在张量上进行运算，并将任意一个模型看成是有向非循环图（DAG），但是它们在其定义方面有很大的区别。</p>
<p>​      Tensorflow 基于静态图。 TensorFlow遵循“数据即代码，代码即数据”的理念。在TensorFlow中，你可以<strong>在模型能够运行之前静态地定义图， 然后在运行时将 数据 feed 到计算图中。</strong>与外部世界的<strong>所有通信都通过tf.Session对象和tf.Placeholder来执行</strong>，这两个张量在运行时会被外部数据替代。<strong>静态计算允许编译器进行更大程度的优化，但是其调试相对比较困难。</strong></p>
<p>​      Pytorch 基于动态图， 在PyTorch中，图的定义则更为重要和动态化：<strong>你可以随时定义、随时更改、随时执行节点，并且没有特殊的会话接口或占位符</strong>。总体而言，该框架与Python语言集成地更为紧密，并且在大多数时候用起来感觉<strong>更加本地化，更加容易将大脑中的想法转化为程序.</strong></p>
<p>​     近年来的发展， tf 引入了 eager 模式实现动态图··， pytorch 也可以借助于 onnx、caffe2和 torchscript 来实现静态图， tensorflow 和 pytorch 的之间静态图和动态图的界限也逐渐变的模糊。</p>

</div>

              </div>
            </div>
          </div>
        </div>
      </div>
    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>









  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "pytorch基本概念&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>






















</body>
</html>
