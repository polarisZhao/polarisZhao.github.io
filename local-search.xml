<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>浅谈DeepLearning落地与工程部署</title>
    <link href="/2020/12/14/%E6%B5%85%E8%B0%88DeepLearning%E8%90%BD%E5%9C%B0%E4%B8%8E%E5%B7%A5%E7%A8%8B%E9%83%A8%E7%BD%B2/"/>
    <url>/2020/12/14/%E6%B5%85%E8%B0%88DeepLearning%E8%90%BD%E5%9C%B0%E4%B8%8E%E5%B7%A5%E7%A8%8B%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<p>浅谈 Deep Learning 落地与工程部署问题</p><a id="more"></a><h4 id="一-概述"><a href="#一-概述" class="headerlink" title="一. 概述"></a>一. 概述</h4><ol><li>软件: 模型的的压缩加速： 轻量级架构设计、剪枝、量化、低秩分解、蒸馏</li><li>框架层面<br>（1）算子的优化(conv 实现)、 图优化( DAG、op 融合)、不同精度的推理( fp16、int8、混合精度)<br>（2）内存优化、Cache 命中、SIMD、多线程、并行计算 (openmp &amp; cuda &amp; sse)<br>（3）代码层：循环展开、汇编<br>（4）平台层：arm、gpu、cpu、x86、npu </li><li>编译器：TVM 和 MLIR</li><li>硬件： 硬件设计</li></ol><h4 id="二-常用模型压缩与加速方法"><a href="#二-常用模型压缩与加速方法" class="headerlink" title="二. 常用模型压缩与加速方法"></a>二. 常用模型压缩与加速方法</h4><p>（1）轻量级网络设计与搜索</p><ul><li>设计轻量化的网络架构，如 mobilenet 、shufflenet、ghostnet 等</li><li>使用 NAS 搜索较为高效的网络结构</li><li>模型蒸馏：使用复杂模型( teacher )去训练另一个轻量化的网络(student model)</li></ul><p>（2）网络的压缩技术（常见的手段包括： 剪枝、稀疏化、量化）</p><ul><li>在训练时使用稀疏约束（加入权重的稀疏正则项，引导模型的大部分权重趋向于0），然后在完成训练后，剪去滤波器上的这些权重较低的节点 。</li><li>量化：模型量化是指权重或激活输出可以被聚类到一些离散、低精度(reduced precision) 的数值点上。常见的有二值网络、三值网络、int8 量化。</li></ul><h4 id="三-常见的推理框架"><a href="#三-常见的推理框架" class="headerlink" title="三. 常见的推理框架"></a>三. 常见的推理框架</h4><p>​        一般会提供模型优化和推断引擎两个模块。模型优化模块用于将给定的模型转化为标准的 Intermediate Representation (IR) ，并对模型优化。推断引擎 (Inference Engine) 则会根据特定的硬件进行算子的优化，以实现高效的前向推导。 </p><ul><li><p>国外：</p><ul><li>google：tf-lite    <a href="https://www.tensorflow.org/lite/performance/post_training_quantization">https://www.tensorflow.org/lite/performance/post_training_quantization</a></li><li>facebook：caffe2  + qnnpack: <a href="https://github.com/pytorch/QNNPACK">https://github.com/pytorch/QNNPACK</a></li><li>intel： <strong>open-vino(for intel CPU)</strong>  <a href="https://software.intel.com/en-us/openvino-toolkit">https://software.intel.com/en-us/openvino-toolkit</a></li><li>apple： core ml</li><li>nvidia： <strong>TensorRT(for nvidia GPU)</strong></li></ul></li><li><p>国内：</p><ul><li>腾讯：<strong>ncnn -&gt; TNN(for arm chip)</strong>  <a href="https://github.com/Tencent/ncnn">https://github.com/Tencent/ncnn</a></li><li>阿里： mnn  <a href="https://github.com/alibaba/MNN">https://github.com/alibaba/MNN</a></li><li>百度： paddlelite  <a href="https://github.com/PaddlePaddle/Paddle-Lite">https://github.com/PaddlePaddle/Paddle-Lite</a></li><li>小米：mace  <a href="https://github.com/XiaoMi/mace">https://github.com/XiaoMi/mace</a></li></ul></li><li><p>其他</p><ul><li>TVM: <a href="https://github.com/dmlc/tvm">https://github.com/dmlc/tvm</a></li><li>TC: TensorComprehensions</li><li>onnx: <a href="https://github.com/onnx/onnx">https://github.com/onnx/onnx</a></li></ul></li></ul><p>ps：MLIR（MachineLearning Intermediate Represent）深度学习中间表示已经越来越受重视，但是最大的问题是众多的框架无法统一 IR，前途漫漫（参见 ONNX 项目）。</p><p>目前做 MLIR 比较出色的包括（这些 IR 都仅限这些框架自己内部使用）：MLIR(<a href="https://github.com/tensorflow/mlir)、TVM、XLA">https://github.com/tensorflow/mlir)、TVM、XLA</a></p><h4 id="四、硬件发展"><a href="#四、硬件发展" class="headerlink" title="四、硬件发展"></a>四、硬件发展</h4><p>各类硬件的发展都离不开芯片制程、核心数量、指令架构优化三个主要方向</p><p><strong>2.1 服务器端硬件分类</strong></p><p>x86_64 CPU、<strong>NVIDIA GPU</strong>、服务器 NPU</p><p><strong>2.2 移动端硬件分类</strong></p><p><strong>ARM CPU</strong>、ARM GPU（Adreno、mali系列）、移动端NPU、NVIDIA Jeston系列、<strong>Apple 家的芯片（自家GPU、NPU）</strong> </p><p> <strong>2.3 其他</strong></p><p> DSP、<strong>FPGA</strong>、外接式加速设备，各种云平台形式的部署</p><h4 id="五-一些基本的问题"><a href="#五-一些基本的问题" class="headerlink" title="五. 一些基本的问题"></a>五. 一些基本的问题</h4><h5 id="1-工程上对卷积操作如何进行优化的？"><a href="#1-工程上对卷积操作如何进行优化的？" class="headerlink" title="1. 工程上对卷积操作如何进行优化的？"></a>1. 工程上对卷积操作如何进行优化的？</h5><p><strong>目前，卷积的计算大多采用间接计算的方式，主要有以下几种实现方式</strong>：</p><ul><li>滑窗机制。这种方法是最直观最简单的方法。 但是，该方法不容易实现大规模加速，因此，通常情况下不采用这种方法（但是也不是绝对不会用，在一些特定的条件下该方法反而是最高效的）</li><li>im2col + GEMM。 caffe/MXNet等很多框架中都使用了这种计算方式，原因是将问题转化为矩阵乘法后可以方便的使用很多矩阵运算库(如MKL、openblas、Eigen等)。</li><li>FFT变换。 时域卷积等于频域相乘，因此可将问题转化为简单的乘法问题。傅里叶变换和快速傅里叶变化是在经典图像处理里面经常使用的计算方法，但是，在 ConvNet 中通常不采用，主要是因为在 ConvNet 中的卷积模板通常都比较小，例如 3×3 等，这种情况下，FFT 的时间开销反而更大，所以很少在CNN中利用FFT实现卷积。</li><li>Winograd：快速卷积算法，针对不同大小的卷积核进行优化，减少计算中的乘法运算次数，提升运行速度。-&gt; 大部分的前向推导框架都实现了 winograd算法。</li><li>MEC：改进了 im2col+GEMM 算法</li></ul><h5 id="2、算法底层加速相关的技术"><a href="#2、算法底层加速相关的技术" class="headerlink" title="2、算法底层加速相关的技术"></a>2、算法底层加速相关的技术</h5><p><strong>代码层级：</strong>图优化(conv + bn + relu 融合)、卷积计算方式优化（im2col + gemm、winograd、FFT）、尽量避免乘法和除法运算（使用位移、加减法运算替代）、精简代码减少不必要的运算、循环优化、高效存储(CSR)等</p><p><strong>汇编优化：</strong>并非汇编代码的效率一定比 C 快，如果 C 程序写的足够好，则编译器生成的汇编可能质量会更高，只是针对特定场景下的情况编译器的优化可能还赶不上高效优化后的手写汇编，因此 ncnn、MNN 都采用了汇编实现算子，但是 TVM 针对 CPU 的代码生成并不是生成的汇编，但是对生成的 LLVM IR 做了很多的优化，目前基本上在 ARM 上已经与 ncnn、MNN 的性能相当了。</p><p><strong>硬件层级：</strong>SIMD（单指令多数据类指令如NEON，SSE），Cache 优化，内存复用（内存优化以及内存池，但要避免程序内存占用过大），多线程（并行计算），访存优化（内存对齐，内存重排提高cache命中率）</p><p><strong>计算库或加速接口：</strong>cuda+cudnn、OpenGL、OpenCL、Vulkan</p><h5 id="3-为什么-mobilenet-理论上速度很快，工程上并没有特别大的提升？"><a href="#3-为什么-mobilenet-理论上速度很快，工程上并没有特别大的提升？" class="headerlink" title="3. 为什么 mobilenet 理论上速度很快，工程上并没有特别大的提升？"></a>3. 为什么 mobilenet 理论上速度很快，工程上并没有特别大的提升？</h5><p>(1) 硬件相关：GPU 偏重于并行、CPU 侧重于侧重于串行。很多细粒度的操作没法很好的并行。</p><p>(2) 和对应的 DL 框架实现细节有关：没有对 dw 和 pw 算子进行优化，比如访存次数、cache miss 较多等。</p><p>(3) 参数量和计算量：设计衡量的指标是 FLOPS、而不是时间。很多 element-wise 操作以及细粒度操作会增加推导时间。</p><h5 id="4-知识蒸馏的原理及其思路"><a href="#4-知识蒸馏的原理及其思路" class="headerlink" title="4. 知识蒸馏的原理及其思路"></a>4. 知识蒸馏的原理及其思路</h5><p>​    基本思想: 蒸馏模型采用的是迁移学习， 通过采用预先训练好的教师模型( teacher model) 的输出作为监督信号去训练另外一个轻量化的网络( student model ) 。从而实现将复杂网络(老师模型)的知识迁移到小网络(学生模型) 中， 提高小网络的精度。常见的蒸馏方案有两种：</p><p><img src="/2020/12/14/%E6%B5%85%E8%B0%88DeepLearning%E8%90%BD%E5%9C%B0%E4%B8%8E%E5%B7%A5%E7%A8%8B%E9%83%A8%E7%BD%B2/5.png" alt="img" style="zoom:35%;"></p><ul><li>用老师学习到的 soft label 来指导学生模型</li></ul><p>​        使用复杂的网络作为 teacher 模型去监督训练一个参数量和运算量更少的 student 模型。teacher 模型可以是一个或者多个提前训练好的高性能模型。student 模型的训练有两个目标：<strong>一个是原始的目标函数，为 student 模型输出的类别概率和 label 的交叉熵</strong>，记为 hard-target；另一个是 <strong>student 模型输出的类别概率和 teacher 模型输出的类别概率的交叉熵</strong>，记为soft target，这两个loss加权后得到最终的训练loss，共同监督studuent模型的训练。 </p><p>参考论文：<a href="https://arxiv.org/pdf/1503.02531.pdf">Distilling the Knowledge in a Neural Network</a></p><ul><li>用老师学习到的学习方法来指导学生模型</li></ul><p>​     该方法用小模型去拟合大模型不同层特征之间的转换关系，其用一个 FSP 矩阵（特征的内积）来表示不同层特征之间的关系，<strong>大模型和小模型不同层之间分别获得多个 FSP 矩阵，然后使用 L2 loss 让小模型的对应层 FSP 矩阵和大模型对应层的 FSP 矩阵尽量一致</strong>，具体如下图所示。这种方法的优势，通俗的解释是，比如将蒸馏类比成 teacher（大模型）教 student（小模型）解决一个问题，传统的蒸馏是直接告诉小模型问题的答案，让小模型学习，而<strong>学习 FSP 矩阵是让小模型学习解决问题的中间过程和方法，因此其学到的信息更多</strong>。</p><p><img src="/2020/12/14/%E6%B5%85%E8%B0%88DeepLearning%E8%90%BD%E5%9C%B0%E4%B8%8E%E5%B7%A5%E7%A8%8B%E9%83%A8%E7%BD%B2/7.png" alt="img" style="zoom:50%;"></p><p>参考论文：<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf">A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning</a></p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>efficient-conv</title>
    <link href="/2020/12/13/efficient-conv/"/>
    <url>/2020/12/13/efficient-conv/</url>
    
    <content type="html"><![CDATA[<p>这次要填坑了！！ 阐述一些底层的卷积实现方式：包含：滑动窗口、im2col + gemm、winograd 等</p><a id="more"></a><p>卷积的具体实现有如下几种方法：</p><ul><li>直接卷积 direct：按照定义直接计算， 一般来说，访存很差。在特定情况下可能是最优方案。</li><li>im2col+gemm：先将特征图换成矩阵，然后用kernel矩阵乘以特征图转换的矩阵，得到输出，访存性能能好一些</li><li>winograd：用加法换乘法，通过减少乘法次数来提高卷积执行速度。</li></ul><p>此外还有 Strassen(减少卷积操作数) 和 FFT(通过 FFT 来减少 conv 的计算量、在大卷积核下收益明显) 、MEC 等方法。但是这几种方案并非主流。下面我们主要详细来阐述主流的三种方案。</p><p>本文主要从工程的角度来阐述这三种主流方案，对于具体的理论推导和细节原理，则不涉及。</p><h3 id="一-direct"><a href="#一-direct" class="headerlink" title="一. direct"></a>一. direct</h3><p>直接按照定义进行计算。需要进行多重 for 循环的展开。</p><h3 id="二-im2col-gemm"><a href="#二-im2col-gemm" class="headerlink" title="二. im2col  + gemm"></a>二. im2col  + gemm</h3><p>Im2col(Image to Column)把输入 feature map 按照卷积核的形式一一展开并拼接成列，接着通过高性能 MatMul（Matrix Multiplication） Kernel 进行矩阵乘，得到输出 feature map。它的本质是把卷积运算转换成矩阵运算。</p><p><img src="/2020/12/13/efficient-conv/8.jpeg" alt></p><p>Pack 优化: </p><p>​        理论上来说， 获得了输入特征图和卷积核的 im2col 变换矩阵之后其实就可以利用 Sgemm 计算出卷积的结果了。</p><p>​        但是如果直接使用矩阵乘法计算，在卷积核尺寸比较大并且输出特征图通道数也比较大的时候，我们会发现这个时候 im2col 获得矩阵是一个行非常多列非常少的矩阵，在做矩阵乘法的时候访存会变得比较差，从而降低计算效率。这里有一个优化技巧，就是数据打包(Pack)。具体来说，对于卷积核我们进行的Pack（所谓 4 的Pack就是在 im2Col获得的二维矩阵的高维度进行压缩，在宽维度进行膨胀，每四行进行交叉拼接）。 如下图所示， 这是一个的卷积核并且输出通道数为，它经过Im2Col之后首先变成上图的上半部分，然后经过Pack 之后变成了上图的下半部分，即变成了每个卷积核的元素按列方向交织排列。</p><p><img src="/2020/12/13/efficient-conv/7.jpeg" style="zoom:35%;"></p><p>还有一个技巧是，每次在执行卷积计算时，对于 Image 的 Im2col 和 Pack 每次都会执行，但对于卷积核，Im2col 和 Pack 在任意次只用做一次，所以我们可以在模型初始化的时候提前把卷积核给 Pack 好，这样就可以节省卷积核 im2col 和 pack 耗费的时间。</p><p>代码实现可以参考： <a href="https://github.com/msnh2012/Msnhnet/blob/master/src/layers/arm/MsnhConvolutionSgemm.cpp">https://github.com/msnh2012/Msnhnet/blob/master/src/layers/arm/MsnhConvolutionSgemm.cpp</a></p><h3 id="三-Winograd"><a href="#三-Winograd" class="headerlink" title="三. Winograd"></a>三. Winograd</h3><p>论文参见： <a href="https://arxiv.org/abs/1509.09308v2">https://arxiv.org/abs/1509.09308v2</a></p><p>​        可以看到 img2col 提高了访存速度，但是它并没有降低运算的时间复杂度。于是在卷积的实践中诞生了 Winograd 算法。Winograd 的 本质是通过降低乘法的次数来提高卷积运算速度。 Winograd 算法主要应用于卷积核为 3x3，步幅为 1 的 2D 卷积神经网络，其参数表示为 F(mxm, rxr)，其中 mxm 是运算之后输出块的大小，rxr 是卷积核的大小，以 F(2x2, 3x3) 和 F(6x6, 3x3) 使用最多，前者加速比可达 2.25x，后者加速比则高达 5.06x。</p><p><img src="/2020/12/13/efficient-conv/5.jpeg" style="zoom:75%;"></p><p>​        首先以一维卷积 $F(2,3)$ 为例。设输入信号为  </p><script type="math/tex; mode=display">d=[d0, d1, d2, d3]^T</script><p>卷积核为：</p><script type="math/tex; mode=display">g=[g0, g1, g2]^T</script><p>滑动步长为 1，不做 padding 操作，则输出结果可以写成</p><script type="math/tex; mode=display">F(2,3)=\left[\begin{array}{ccc}d_0 &d_1  &d_2 \\d_1 &d_2  &d_3\end{array}\right]\left[\begin{array}{ccc}g_0  \\g_1\\g_2\end{array}\right]=\left[\begin{array}{c}r_0\\r_1\end{array}\right]</script><p>其中</p><script type="math/tex; mode=display">r_0=d_0*g_0+d_1*g_1+d_2*g_2 \\r_1=d_1*g_0+d_2*g_1+d_3*g_2</script><p>可以看到如果用一般的矩阵乘法，则需要 6 次乘法和 4 次加法。</p><p>winograd 做法如下</p><script type="math/tex; mode=display">F(2,3)=\left[\begin{array}{ccc}d_0 &d_1  &d_2 \\d_1 &d_2  &d_3\end{array}\right]\left[\begin{array}{ccc}g_0  \\g_1\\g_2\end{array}\right]=\left[\begin{array}{c}m_1+m_2+m_3\\m_2-m_3-m_4\end{array}\right]</script><p>其中</p><script type="math/tex; mode=display">m_1=(d_0-d_2)g_0, \quad m_2=(d_1+d_2)\frac{g_0+g_1+g_2}{2}\\m_4=(d_1-d_3)g_2, \quad m_3=(d_2-d_1)\frac{g_0-g_1+g_2}{2}\\</script><p>可以看到利用 winograd 算法需要 4 次乘法，8次加法，相比一般矩阵乘法，通过增加加法运算减少乘法的运算，可以实现加速。</p><p>由于 winograd 算法证明比较复杂暂时不写了，直接丢计算公式，一维卷积计算公式如下：</p><script type="math/tex; mode=display">Y=A^T\left[[Gg]\bigodot[B^Td]\right]</script><p>其中 ⨀ 表示 element-wise，A,G,B 都是根据输出大小和卷积核提前确定好的(有人已经写好了,可参考 <a href="https://github.com/andravin/wincnn">wincnn</a>)， g 表示卷积核，d 表示输入数据(也就是需要进行卷积计算的数据)。 可以看到，它可以将 $m * r$ 次乘法 降低为 $m + r - 1$ 次乘法。</p><p>对应二维卷积计算公式如下:</p><script type="math/tex; mode=display">Y=A^T\left[[GgG^T]\bigodot[B^TdB]\right]A</script><p>基于上面介绍的二维的 Winograd的原理，我们现在只需要分 4 步即可实现 winograd 算法：</p><ol><li>根据卷积核的大小，确定变换矩阵 G， 对输入卷积核的变换   $U = GgG^T$</li><li>根据输入数据的大小， 确定变换矩阵 B，并对输入数据的变换  $V = B^T d B$ </li><li>计算 M 矩阵：  $M = \sum U \bigodot V$</li><li>计算最终结果: $Y = A^T M A$</li></ol><p>需要注意以下几点：</p><ol><li>当输入较大的时候， 需要把输入拆成多个 4x4 的 block，stride 2 即可。</li><li>当对卷积核和输入数据进行转换后， 需要对其进行内存重拍， 方便去利用 gemm 。</li></ol><p>具体的代码实现可以参照： <a href="https://github.com/Tencent/ncnn/blob/master/src/layer/arm/convolution_3x3.h">https://github.com/Tencent/ncnn/blob/master/src/layer/arm/convolution_3x3.h</a>  line 1702 - line 1864 </p><h3 id="四-其他"><a href="#四-其他" class="headerlink" title="四. 其他"></a>四. 其他</h3><ul><li><p>不同的卷积的快慢并非一成不变， 特定的情况下可能通常较快的方法反而运行较快。旷视的 megengine 提出了 Fast-Run 来寻找最优算子，其工程实现分为选择和执行两个阶段：</p><ul><li>选择阶段，测速模型每个算子，选出最优实现，保存算子名称和最优实现的映射表；</li><li>执行阶段，根据映射表直接调用相应实现完成计算。</li></ul></li><li><p>数据排布和 cache 命中是极其重要的。</p></li><li>数据排布(Tensor Layout) ：数据排布是推理侧卷积计算优化方面首先面临的问题。选择合适的数据排布不仅会使卷积优化事半功倍，还可作为其他优化方法的基础， 目前，深度学习框架中常见的数据排布格式有3种：<ul><li>NHWC：[Batch, Height, Width, Channels]</li><li>NCHW：[Batch, Channels, Height, Width]</li><li>NCHWX：[Batch, Channels/X, Height, Width, X=4/8]</li></ul></li></ul><p>​    数据的排布对卷积计算有着整体性的直接影响。NHWC 和 NCHW 的空间复杂度相同，区别在于访存行为，NCHWX 介于两者之间，但是有其他优点。</p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>img2col</title>
    <link href="/2020/12/12/img2col/"/>
    <url>/2020/12/12/img2col/</url>
    
    <content type="html"><![CDATA[<h4 id="1-im2col-基本原理"><a href="#1-im2col-基本原理" class="headerlink" title="1. im2col 基本原理"></a>1. im2col 基本原理</h4><p><strong>(1) 对于图像的变换：</strong></p><p>首先将每个卷积所卷积的区域作为一行，所有的卷积区域纵向排列，作为右乘矩阵。需要注意的是多个通道应该堆叠在下面。如下图所示：</p><p>单通道：</p><p><img src="/2020/12/12/img2col/1.jpeg" alt="img"></p><p>多通道：</p><p><img src="/2020/12/12/img2col/2.jpeg" alt="img"></p><p><strong>输入特征图转化得到的矩阵尺度 = (卷积组输入通道数*卷积核高*卷积核宽) * (卷积层输出单通道特征图高 * 卷积层输出单通道特征图宽)</strong></p><p><strong>(2) 对于卷积核的变换</strong></p><p>将一个卷积核拉伸为一个横行，作为左乘矩阵：(为什么要拉伸为横行，在于对应的卷积区域拉伸为竖行，这样才能与之相对应，进行矩阵乘法)</p><p><img src="/2020/12/12/img2col/3.jpeg" alt="img"></p><p><strong>权值矩阵尺度 = (输出层通道数) * (卷积输入通道数*卷积核高*卷积核宽)</strong></p><p><strong>(3) 将卷积核矩阵核图像矩阵相乘即可</strong></p><p><img src="/2020/12/12/img2col/4.jpeg" alt="img"></p><p><strong>卷积层输出尺度 = (卷积层输出通道数) * (卷积层输出单通道特征图高 * 卷积层输出单通道特征图宽)</strong></p><p>最后需要将结果调整成为需要的大小： </p><p><img src="/2020/12/12/img2col/5.jpeg" alt="img"></p><p><strong>(4) 对于多个输出通道图像的卷积</strong></p><p>应该纵向堆叠每个卷积层</p><p><img src="/2020/12/12/img2col/6.jpeg" alt="img"></p><p>最后再调整输出矩阵：</p><p><img src="/2020/12/12/img2col/7.jpeg" alt="img"></p><h4 id="2-总体简图："><a href="#2-总体简图：" class="headerlink" title="2. 总体简图："></a>2. 总体简图：</h4><p>下面是一个整体矩阵乘法的简图：</p><p><img src="/2020/12/12/img2col/8.jpeg" alt="img"></p><h4 id="3-im2col-cpu-源码剖析"><a href="#3-im2col-cpu-源码剖析" class="headerlink" title="3. im2col_cpu 源码剖析"></a>3. im2col_cpu 源码剖析</h4><p>im2col_cpu函数将卷积层输入转化为矩阵相乘的右元，核心是5个for循环，首先第一个for循环表示按照输入的通道数逐个处理卷积层输入的特征图，下面笔者将用图示表示剩余的四个for循环操作，向读者朋友们展示卷积层输入的单通道特征图是通过怎样的方式转化为一个矩阵。在这里我们假设，卷积层输入单通道特征图原大小为5*5，高和宽方向的pad为1，高和宽方向步长为2，卷积核不进行扩展。</p><p>  我们先计算一下，卷积层输入单通道特征图转化得到的矩阵的尺度，矩阵的行数应该为卷积核高<em>卷积核宽，即为9，列数应该为卷积层输出特征图高(output_h)</em>卷积层输出特征图宽(output_w)，也为9，那么，im2col算法起始由下图开始：</p><p><img src="/2020/12/12/img2col/11.png" alt></p><p>首先kernel_row为0，kernel_col也为0。按照input_row = -pad_h + kernel_row * dilation_h计算input_row的值，在这里，pad_h为1，kernel_row为0，dilation_h为1，计算出input_row为-1，此时output_row为3，满足函数中的第一个if条件，那么在输出图像上先置output_w个零，因为output_w为3，因此得到下图：</p><p><img src="/2020/12/12/img2col/12.png" alt></p><p> 然后input_row加上步长2，由-1变成1，此时output_rows为2，计算input_col等于-1，此时执行input_col定义下面的for循环，得到3个值：依次往目标矩阵中填入0，data_im[1*5+1]和data_im[1*5+3]，即填入0,7和9。得到下图：</p><p><img src="/2020/12/12/img2col/13.png" alt></p><p>再接着执行，此时input_row再加上2变为3，此时output_rows变为1，计算input_col等于-1，执行input_col定义下面的for循环，得到3个值，分别为0，data_im[3*5+1]和data_im[3*5+3]，即填入0,17和19。得到下图：(以上操作是将第一个通道的卷积对应第一个对应位置进放好)</p><p><img src="/2020/12/12/img2col/14.png" alt></p><p> 接着，kernel_col变成1，此时kernel_row为0，kernel_col为1。计算input_row又变成-1，第一个if条件成立，那么，再在输出矩阵上输出3个0。然后，input_row变成1，input_col分别为0(-1+1)，2(-1+1+2)和4(-1+1+2+2)时，输出矩阵上分别输出data_im[1*5+0]，data[1*5+2]，data[1*5+4]，即分别填入6,8,10。然后，input_row变成3，input_col分别为0，2，4时，输出矩阵上分别输出data_im[3*5+0]，data[3*5+2]，data[3*5+4]，即分别输出16,18,20。（将第一个通道的卷积对应第二个对应位置进放好）</p><p><img src="/2020/12/12/img2col/15.png" alt></p><p>然后，kernel_col变成2，此时kernel_row为0，kernel_col为2。计算input_row又变成-1，第一个if条件成立，那么，再在输出矩阵上输出3个0。然后，input_row变成1，input_col分别为1(-1+2)，3(-1+2+2)和5(-1+2+2+2)时，输出矩阵上分别输出data_im[1*5+1]，data[1*5+3]，0，即分别填入7,9,0。然后，input_row变成3，input_col分别为1，3，5时，输出矩阵上分别输出data_im[3*5+0]，data[3*5+2]，0，即分别输出17,19,0。见下图：（将第一个通道的卷积对应第三个对应位置进放好）</p><p><img src="/2020/12/12/img2col/16.png" alt></p><p>接着，kernel_row变成1，kernel_col变成0。计算input_row又变成0，input_col分别为-1(-1+0)，1(-1+0+2)和3(-1+0+2+2)，输出矩阵上分别输出0，data[0*5+1]，data[0*5+3]，即分别填入0,2,4。然后，input_row变成2，input_col分别为-1，1和3时，输出矩阵上分别输出0，data[2*5+1]，data[2*5+3]，即分别填入0,12,14。然后，input_row变成4，input_col分别为-1，1，3时，输出矩阵上分别输出0，data[4*5+1]，data[4*5+3]，即分别输出0,22,24。见下图：（将第一个通道的卷积对应第四个对应位置进放好）</p><p><img src="/2020/12/12/img2col/17.png" alt></p><p>然后，kernel_row为1，kernel_col变成1。计算input_row为0，input_col分别为0(-1+1)，2(-1+1+2)和4(-1+1+2+2)，输出矩阵上分别输出data[0*5+0]，data[0*5+2]，data[0*5+4]，即分别填入1,3,5。然后，input_row变成2，input_col分别为0，2和4时，输出矩阵上分别输出data[2*5+0]，data[2*5+2]，data[2*5+4]，即分别填入11,13,15。然后，input_row变成4，input_col分别为0，2，4时，输出矩阵上分别输出data[4*5+0]，data[4*5+2]，data[4*5+4]，即分别输出21,23,25。见下图：（将第一个通道的卷积对应第五个对应位置进放好）</p><p><img src="/2020/12/12/img2col/18.png" alt></p><p>然后，kernel_row为1，kernel_col变成2。计算input_row为0，input_col分别为1(-1+2)，3(-1+2+2)和5(-1+2+2+2)，输出矩阵上分别输出data[0*5+1]，data[0*5+3]，0，即分别填入2,4,0。然后，input_row变成2，input_col分别为1，3和5时，输出矩阵上分别输出data[2*5+1]，data[2*5+3]，0，即分别填入12,14,0。然后，input_row变成4，input_col分别为1，3，5时，输出矩阵上分别输出data[4*5+1]，data[4*5+3]，0，即分别输出22,24,0。见下图：（将第一个通道的卷积对应第六个对应位置进放好）</p><p><img src="/2020/12/12/img2col/19.png" alt></p><p>   接着，kernel_row变成2，kernel_col变成0。计算input_row为1，input_col分别为-1(-1+0)，1(-1+0+2)和3(-1+0+2+2)，输出矩阵上分别输出0，data[1*5+1]，data[1*5+3]，即分别填入0,7,9。然后，input_row变成3，input_col分别为-1，1和3时，输出矩阵上分别输出0，data[3*5+1]，data[3*5+3]，即分别填入0,17,19。然后，input_row变成5，满足第一个if条件，直接输出三个0。见下图：（**将第一个通道的卷积对应第七个对应位置进放好）</p><p><img src="/2020/12/12/img2col/20.png" alt="Center"></p><p>   然后，kernel_row为2，kernel_col变成1。计算input_row为1，input_col分别为0(-1+1)，2(-1+1+2)和4(-1+1+2+2)，输出矩阵上分别输出data[1*5+0]，data[1*5+2]，data[1*5+4]，即分别填入6,8,10。然后，input_row变成3，input_col分别为0，2和4时，输出矩阵上分别输出data[3*5+0]，data[3*5+2]，data[3*5+4]，即分别填入16,18,20。然后，input_row变成5，满足第一个if条件，直接输出三个0。见下图：（将第一个通道的卷积对应第八个对应位置进放好）</p><p><img src="/2020/12/12/img2col/21.png" alt></p><p>   最后，kernel_row为2，kernel_col变成2。计算input_row为1，input_col分别为1(-1+2)，3(-1+2+2)和5(-1+2+2+2)，输出矩阵上分别输出data[1*5+1]，data[1*5+3]，0，即分别填入7,9,0。然后，input_row变成3，input_col分别为1，3和5时，输出矩阵上分别输出data[3*5+1]，data[3*5+3]，0，即分别填入17,19,0。然后，input_row变成5，满足第一个if条件，直接输出三个0。见下图：（将第一个通道的卷积对应第五个对应位置进放好）</p><p><img src="/2020/12/12/img2col/22.png" alt></p><p>   到此卷积层单通道输入特征图就转化成了一个矩阵，请读者朋友们仔细看看，矩阵的各列就是卷积核操作的各小窗口。</p><p><img src="/2020/12/12/img2col/23.png" alt></p><p><strong>!!! 注意点：</strong></p><p><strong>（1）如果是多个通道的话，就要将其他通道放置在这个通道的下面，最终的结果是生成矩阵的每一列对应一个卷积核(多个通道)。</strong></p><p><strong>（2）卷积中的zero-padding操作的实现，并不是真正在原始输入特征图周围添加0，而是在特征图转化得到的矩阵上的对应位置添加0。</strong></p><p><strong>（3）这种算法的核心点在于，先去找卷积核(单通道)对应所有卷积结果对应的某个位置的像素值，将其放置在一行，从而完整单通道的复制。当然也可以找到某个卷积核(单通道)对应的位置，将其拉伸为列。</strong></p><p><strong>而im2col_cpu函数功能的相反方向的实现则有由col2im_cpu函数完成，笔者依旧把该函数的代码注释放在下面：</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">/*col2im_cpu为im2col_cpu的逆操作接收13个参数，分别为输入矩阵数据指针(data_col)，卷积操作处理的一个卷积组的通道 数(channels)，输入图像的高(height)与宽(width)，原始卷积核的高(kernel_h)与宽(kernel_w)， 输入图像高(pad_h)与宽(pad_w)方向的pad，卷积操作高(stride_h)与宽(stride_w)方向的步长， 卷积核高(stride_h)与宽(stride_h)方向的扩展，输出图像数据指针(data_im)*/</span>  <span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> Dtype&gt;  <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">col2im_cpu</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Dtype* data_col, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> channels,  </span></span><span class="hljs-function"><span class="hljs-params">    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> height, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> width, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> kernel_h, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> kernel_w,  </span></span><span class="hljs-function"><span class="hljs-params">    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> pad_h, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> pad_w,  </span></span><span class="hljs-function"><span class="hljs-params">    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> stride_h, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> stride_w,  </span></span><span class="hljs-function"><span class="hljs-params">    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> dilation_h, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> dilation_w,  </span></span><span class="hljs-function"><span class="hljs-params">    Dtype* data_im)</span> </span>&#123;      caffe_set(height * width * channels, Dtype(<span class="hljs-number">0</span>), data_im);   <span class="hljs-comment">//首先对输出的区域进行初始化，全部填充0  </span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> output_h = (height + <span class="hljs-number">2</span> * pad_h - (dilation_h * (kernel_h - <span class="hljs-number">1</span>) + <span class="hljs-number">1</span>)) / stride_h + <span class="hljs-number">1</span>;  <span class="hljs-comment">//计算卷积层输出图像的宽  </span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> output_w = (width + <span class="hljs-number">2</span> * pad_w - (dilation_w * (kernel_w - <span class="hljs-number">1</span>) + <span class="hljs-number">1</span>)) / stride_w + <span class="hljs-number">1</span>;  <span class="hljs-comment">//计算卷积层输出图像的高  </span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> channel_size = height * width;   <span class="hljs-comment">//col2im输出的单通道图像容量</span>      <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> channel = channels; channel--; data_im += channel_size) &#123;<span class="hljs-comment">//按照输出通道数一个一个处理  </span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> kernel_row = <span class="hljs-number">0</span>; kernel_row &lt; kernel_h; kernel_row++) &#123;              <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> kernel_col = <span class="hljs-number">0</span>; kernel_col &lt; kernel_w; kernel_col++) &#123;              <span class="hljs-keyword">int</span> input_row = -pad_h + kernel_row * dilation_h;<span class="hljs-comment">//在这里找到卷积核中的某一行在输入图像中的第一个操作区域的行索引  </span>         <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> output_rows = output_h; output_rows; output_rows--) &#123;               <span class="hljs-keyword">if</span> (!is_a_ge_zero_and_a_lt_b(input_row, height)) &#123;<span class="hljs-comment">//如果计算得到的输入图像的行值索引小于零或者大于输入图像的高(该行为pad)  </span>                data_col += output_w;  <span class="hljs-comment">//那么，直接跳过这output_w个数，这些数是输入图像第一行上面或者最后一行下面pad的0  </span>             &#125; <span class="hljs-keyword">else</span> &#123;                   <span class="hljs-keyword">int</span> input_col = -pad_w + kernel_col * dilation_w;<span class="hljs-comment">//在这里找到卷积核中的某一列在输入图像中的第一个操作区域的列索引  </span>                 <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> output_col = output_w; output_col; output_col--) &#123;                        <span class="hljs-keyword">if</span> (is_a_ge_zero_and_a_lt_b(input_col, width)) &#123;<span class="hljs-comment">//如果计算得到的输入图像的列值索引大于等于于零或者小于输入图像的宽(该列不是pad)  </span>                      data_im[input_row * width + input_col] += *data_col;<span class="hljs-comment">//将矩阵上对应的元放到将要输出的图像上  </span>                 &#125; <span class="hljs-comment">//这里没有else，因为如果紧挨的if条件不成立的话，input_row*width + input_col这个下标在data_im中不存在，同时遍历到data_col的对应元为0  </span>                 data_col++;<span class="hljs-comment">//遍历下一个data_col中的数  </span>                 input_col += stride_w;<span class="hljs-comment">//按照宽方向步长遍历卷积核上固定列在输入图像上滑动操作的区域  </span>             &#125;            &#125;            input_row += stride_h;<span class="hljs-comment">//按照高方向步长遍历卷积核上固定行在输入图像上滑动操作的区域  </span>             &#125;              &#125;          &#125;      &#125;  &#125;</code></pre>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>network-pruning</title>
    <link href="/2020/11/24/network-pruning/"/>
    <url>/2020/11/24/network-pruning/</url>
    
    <content type="html"><![CDATA[<p>模型剪枝的相关工作记录</p><a id="more"></a><p> 剪枝是指裁减掉不重要的冗余的卷积参数，减少参数量，加快推理速。剪枝可以分为结构化剪枝和非结构化剪枝。在非结构化剪枝中， 是将其置零的，并没有进行剪枝，通常需要搭配固定的硬件。结构化剪枝则是指在 filter 层面，通道层面，或者 shape 层面的剪枝。</p><p><img src="/2020/11/24/network-pruning/3.png" style="zoom:35%;"></p><p>本文主要关注与结构化剪枝的相关工作</p><h3 id="一-解决方案"><a href="#一-解决方案" class="headerlink" title="一. 解决方案"></a>一. 解决方案</h3><h5 id="1-基于度量"><a href="#1-基于度量" class="headerlink" title="1. 基于度量"></a>1. 基于度量</h5><p><strong>(1) 基于权重</strong></p><ul><li><strong>Pruning Filters for Efficient ConvNets  ——  l1 norm</strong> 🌟</li></ul><p>衡量标准: Filter的L1 norm</p><ul><li><strong>Filter pruning via geometric median for deep convolutional neural networks acceleration</strong>    🌟</li></ul><p>衡量标准： 卷积核的几何中位数 </p><p>   <strong>-</strong> <strong>SFP Soft filter pruning —— l2 norm: soft pruning</strong>  🌟</p><p>​    <strong>-</strong> <strong>Learning both Weights and Connections for Efficient Neural Networks  —— no structured prune: weight</strong> 🌟</p><p><strong>(2) feature map</strong></p><ul><li><p><strong>APoZ: Network trimming: A data-driven neuron pruning approach towards efficient deep architectures</strong></p><p>衡量标准: 激活层输出的feature map的的稀疏程度</p></li></ul><p>​    <strong>- HRank</strong></p><p><strong>（3）loss func</strong></p><p>​    <strong>Pruning Convolutional Neural Networks for Resource Efficient Inference ZAS</strong></p><p>​    衡量标准: 修剪网络参数引起的损失函数的变化</p><h5 id="2-基于重建误差"><a href="#2-基于重建误差" class="headerlink" title="2. 基于重建误差"></a>2. 基于重建误差</h5><ul><li><strong>Channel pruning for accelerating very deep neural networks</strong>  🌟</li></ul><p>​      衡量标准: 通过最小化裁剪后特征图和裁剪前特征图之间的误差</p><ul><li><p><strong>ThiNet (Luo et al., 2017)</strong> 🌟</p><p>衡量标准: 用输入子集代替原来的输入得到输出的相似度</p></li></ul><h5 id="3-稀疏化"><a href="#3-稀疏化" class="headerlink" title="3. 稀疏化"></a>3. 稀疏化</h5><ul><li><strong>(SSL) Learning Structured Sparsity in Deep Neural Networks</strong>  <strong>🌟</strong></li></ul><p>​       使用 group Lasso 给损失函数加入相应的惩罚，进行结构化稀疏</p><ul><li><p><strong>(Network Slimming)  l1 norm: bn gamma</strong>  <strong>🌟</strong></p><p>衡量标准: BN层的 γ 参数</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">updateBN</span>():</span>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> model.modules():        <span class="hljs-keyword">if</span> isinstance(m, nn.BatchNorm2d):            m.weight.grad.data.add_(args.s*torch.sign(m.weight.data))  <span class="hljs-comment"># L1</span>              <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">epoch</span>):</span>    <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> enumerate(train_loader):        // ...        loss.backward()        <span class="hljs-keyword">if</span> args.sr:            updateBN()        // ...</code></pre></li></ul><h5 id="4-NAS"><a href="#4-NAS" class="headerlink" title="4. NAS"></a>4. NAS</h5><ul><li><p>AMC</p></li><li><p>OneforAll</p></li><li><p>AutoSlim</p></li></ul><h5 id="5-理论思考"><a href="#5-理论思考" class="headerlink" title="5. 理论思考"></a>5. 理论思考</h5><ul><li><p>Rethinking the value of network pruning   🌟</p><p>​    这篇论文的作者实验过程中发现了模型修剪过后，微调得到的效果和重新从头训练几乎相同，于是便做了多组实验证明这一结论，推翻了包括自己的方法在内的很多方法，提出对参数修剪的一个新的认识：参数修剪的实际作用在于得到架构而非权值。</p></li><li><p>The lottery ticket hypothesis: Finding sparse, trainable neural networks  🌟</p></li><li>Recovering from Random Pruning: On the Plasticity of Deep Convolutional Neural Networks</li><li>Pruning from Scratch</li></ul><h5 id="6-简单应用"><a href="#6-简单应用" class="headerlink" title="6.简单应用"></a>6.简单应用</h5><p>(1) YOLO 剪枝</p><ul><li><a href="https://zhuanlan.zhihu.com/p/153496637"><strong>https://zhuanlan.zhihu.com/p/153496637</strong></a></li></ul><p>(2) GAN</p><ul><li><p>GAN Slimming: All-in-One GAN Compression by A Unified Optimization Framework</p></li><li><p>GAN Compression: Efficient Architectures for Interactive Conditional GANs</p></li></ul><h5 id="7-软硬件联合设计"><a href="#7-软硬件联合设计" class="headerlink" title="7. 软硬件联合设计"></a>7. 软硬件联合设计</h5><ul><li>ADMM</li><li>MCUNet</li><li>One for All </li></ul><h3 id="二-注意事项："><a href="#二-注意事项：" class="headerlink" title="二. 注意事项："></a>二. 注意事项：</h3><ol><li><p>剪裁一个 conv layer 的 filter，需要修改后续 conv 的 filter. 即剪掉 $X<em>i$ 的一个 filter，会导致 $X</em>{i+1}$ 少一个 channel,  $X_{i+1}$ 对应的 filter 在 input_channel 维度上也要减 1。</p></li><li><p>剪裁完 $X<em>i$之后，根据注意事项 1 我们从$X</em>{i+1}$ 的 filter 中删除了一行，在计算$X_{i+1}$的filters的l1_norm(图中绿色一列)的时候，有两种选择</p></li></ol><ul><li>算上被删除的一行：independent pruning </li><li>减去被删除的一行：greedy pruning</li></ul><ol><li>在对ResNet等复杂网络剪裁的时候，还要考虑到后当前卷积层的修改对上一层卷积层的影响。在对 residual block 剪裁时，$X<em>{i+1}$ 层如何剪裁取决于 project shortcut 的剪裁结果，因为我们要保证 project shortcut 的 output 和 $X</em>{i+1}$ 的 output 能被正确的 concat.</li></ol><h5 id="2-敏感度的理解"><a href="#2-敏感度的理解" class="headerlink" title="2. 敏感度的理解"></a>2. 敏感度的理解</h5><p><img src="https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/release/1.0.1/docs/images/algo/pruning_3.png" style="zoom:33%;"></p><p>如图所示，横坐标是将filter剪裁掉的比例，竖坐标是精度的损失，每条彩色虚线表示的是网络中的一个卷积层。 以不同的剪裁比例<strong>单独</strong>剪裁一个卷积层，并观察其在验证数据集上的精度损失，并绘出图中的虚线。虚线上升较慢的，对应的卷积层相对不敏感，我们优先剪不敏感的卷积层的filter.</p><ul><li><p>选择最优的剪裁率组合</p><p>我们将上图中的折线拟合为下图中的曲线，每在竖坐标轴上选取一个精度损失值，就在横坐标轴上对应着一组剪裁率，如<strong>图8</strong>中黑色实线所示。 用户给定一个模型整体的剪裁率，我们通过移动<strong>图5</strong>中的黑色实线来找到一组满足条件的且合法的剪裁率。</p></li></ul><p><img src="https://raw.githubusercontent.com/PaddlePaddle/PaddleSlim/release/1.0.1/docs/images/algo/pruning_4.png" style="zoom:33%;"></p><ul><li>迭代剪裁</li></ul><p>考虑到多个卷积层间的相关性，一个卷积层的修改可能会影响其它卷积层的敏感度，我们采取了多次剪裁的策略，步骤如下：</p><ul><li>step1: 统计各卷积层的敏感度信息</li><li>step2: 根据当前统计的敏感度信息，对每个卷积层剪掉少量 filter, 并统计 FLOPS，如果 FLOPS 已满足要求，进入step4，否则进行step3。</li><li>step3: 对网络进行简单的fine-tune，进入step1</li><li>step4: fine-tune训练至收敛</li></ul><p>​    </p><p>​    </p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>lightweight-cnn-architecture-design</title>
    <link href="/2020/11/15/lightweight-cnn-architecture-design/"/>
    <url>/2020/11/15/lightweight-cnn-architecture-design/</url>
    
    <content type="html"><![CDATA[<p>常见的移动端模型：mobilenet 系列和 shufflenet 系列和 GhostNet。对于 MnasNet、PorxylessNas、FBNet 等轻量级搜索架构则不涉及。</p><a id="more"></a><h4 id="1-Mobilenet-v1"><a href="#1-Mobilenet-v1" class="headerlink" title="1. Mobilenet v1"></a>1. Mobilenet v1</h4><p>MobileNet模型的核心就是<strong>将原本标准的卷积操作因式分解成一个depthwise convolution和一个1*1的卷积（文中叫pointwise convolution）操作。简单讲就是将原来一个卷积层分成两个卷积层，其中前面一个卷积层的每个filter 都只跟 input 的每个 channel 进行卷积，然后后面一个卷积层则负责 combining，即将上一层卷积的结果进行合并。 </strong></p><h5 id="具体实现："><a href="#具体实现：" class="headerlink" title="具体实现："></a>具体实现：</h5><p>(1) 传统的卷积计算：假设 M 表示 input 的 channel 个数，N 表示 output 的 channel 个数（也是本层的卷积核个数）。因此如果假设卷积核大小是 <code>DK*DK*M*N</code>，输出是 <code>DF*DF*N</code>，那么标准卷积的计算量是 <code>DK*DK*M*N*DF*DF</code>。如下图(a)所示。</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/1.png" alt="x" style="zoom:50%;"></p><ul><li><p>首先使用 <code>M</code> 个 <code>Dk*Dk</code>的卷积核对输入( <code>M*DF*DF</code> )进行卷积,这里要注意的是每个 filter 只跟输入的一个通道进行卷积(也就是说，卷积核的通道数为 1)，将不同卷积核的结果进行组合，可以得到大小为：<code>M*DF*DF</code>的输出。其计算量为  <code>M*DK*DK*DF*DF</code>。(这里使用的 3*3 的卷积核，padding 为 1 )。如上图 (b) 所示。</p></li><li><p>第二步是对之前的 <code>M*DF*DF</code> 的输出进行卷积，卷积核为N个 <code>1*1*M</code>  的卷积，这样可以得到大小 <code>N*DF*DF</code>  的输出。其计算量为 <code>1*1*N*M*DF*DF</code>。如上图 (c) 所示。</p></li></ul><p>所以其总的计算量相比传统的卷积计算减少了：</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/2.png" alt="x" style="zoom:50%;"></p><p>（2）标准卷积（左边）和因式分解后的卷积（右边）的差别如下所示。注意到卷积操作后都会跟一个Batchnorm和ReLU操作。</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/3.png" alt="x" style="zoom:50%;"></p><p>​      总的来说，由于采用 depth-wise convolution 会有一个问题，就是导致「信息流通不畅」，即 <strong>输出的 feature map 仅包含输入的 feature map 的一部分</strong>，MobileNet 用 point-wise convolution 解决这个问题。在后来，ShuffleNet 采用同样的思想对网络进行改进，只不过把 point-wise convolution 换成了 channel shuffle。</p><h4 id="2-Shufflenet-v1"><a href="#2-Shufflenet-v1" class="headerlink" title="2. Shufflenet v1"></a>2. Shufflenet v1</h4><p>​    Shufflenet v1 主要采用 <strong>channel shuffle、pointwise group convolutions 和 depthwise convolution来修改原来的ResNet单元</strong>。从而大幅度降低深度网络计算量。</p><ul><li>channel shuffle：不同group的通道进行组合。</li><li>pointwise group convolutions：带有 group 的 1 * 1 卷积。</li><li>depthwise convolutions：group 等于通道数的组卷积。</li></ul><h5 id="具体实现：-1"><a href="#具体实现：-1" class="headerlink" title="具体实现："></a>具体实现：</h5><p>（1）a 图是一个带有 depthwise convolution 的resnet 结果，所谓的 depthwise convolution 可以参见</p><p>（2）a 图 —&gt; b 图，用带group的 1 <em> 1卷积(2个)代替原来的1 </em> 1卷积，同时添加一个channel shuffle 操作。</p><p>（3）b 图 —&gt; c 图，添加了一个步长为2的 Average pooling，将Resnet最后的Add操作c改为concat操作，也就是按channel合并，类似googleNet的Inception操作。</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/5.png" alt="x" style="zoom:38%;"></p><p>​    shuffle 具体来说是 channel shuffle，将各部分的 feature map 的 channel 进行有序的打乱，构成新的 feature map，以解决 group convolution 带来的「信息流通不畅」问题。（MobileNet 用 pointwise convolution 解决）因此可知道 shuffle 不是什么网络都需要用的，有一个前提，就是采用了 group convolution。采用 shuffle 替换掉 1<em>1 卷积，可以减少权值参数，而且是大量减少。<em>*文中提到两次，对于小型网络，多多使用通道会比较好。所以，以后若涉及小型网络，可考虑如何提升通道使用效率。</em></em></p><p>ShuffleNet V1 与 Mobilenet V1的区别：</p><ol><li>与 MobileNet 一样采用了 depth-wise convolution，但是针对 depth-wise convolution 带来的副作用——「信息流通不畅」，ShuffleNet 用 channel shuffle 来解决，MobileNet 用 point-wise convolution 解决。</li><li>在网络拓扑方面，ShuffleNet 采用的是 resnet 的思想，而 mobielnet 采用的是 VGG 的堆叠思想（SqueezeNet 也是采用 VGG 思想）。</li></ol><h4 id="3-Mobilenet-V2"><a href="#3-Mobilenet-V2" class="headerlink" title="3. Mobilenet V2"></a>3. Mobilenet V2</h4><h5 id="MobileNet-v2的主要贡献：Inverted-Residual-和-Linear-Bottleneck。具体实现："><a href="#MobileNet-v2的主要贡献：Inverted-Residual-和-Linear-Bottleneck。具体实现：" class="headerlink" title="MobileNet v2的主要贡献：Inverted Residual 和 Linear Bottleneck。具体实现："></a>MobileNet v2的主要贡献：Inverted Residual 和 Linear Bottleneck。具体实现：</h5><p>(1) <strong>添加 residual connection</strong>。</p><p>(2) <strong>通过 1x1 卷积先提升通道数，再通过 depthwise 的 3x3 空间卷积，再用 1x1 卷积降低维度。作者称之为Inverted residual block，两边窄中间宽，像柳叶，仅用较小的计算量就能得到较好的性能</strong>。</p><p>(3) <strong>使用ReLU6替换传统的ReLU,  并将最后输出的 ReLU6 去掉，直接线性输出</strong>。</p><p>如下图是 mobilenet v1与 mobilenet v2 的对比图。两者的区别在于：</p><ul><li>v2在原有的 dw 之前加了一个 pw 专门用来升维。这么做是因为不能改变通道数量，先加 pw 升维后，dw就能在高维提特征了。</li><li>v2 把原本 dw 之后用来降维的 pw 后的激活函数给去掉了。这么做是因为他认为非线性在高维有益处，但在低维（例如pw降维后的空间）不如线性好。<strong>ReLU 会对 channel 数低的张量造成较大的信息损耗。ReLU 会使负值置零，channel 数较低时会有相对高的概率使某一维度的张量值全为 0，即张量的维度减小了，而且这一过程无法恢复。</strong></li></ul><p><img src="/2020/11/15/lightweight-cnn-architecture-design/8.png" alt="x" style="zoom:50%;"></p><p>如下图是 Resnet 与 mobilenet v2 的对比图。可以看到两者的结果很相似。不过ResNet是先降维（0.25倍）、提特征、再升维。而v2则是先升维（6倍）、提特征、再降维。另外 v2 也用 DW 代替了标准卷积来做特征提取。why ? <strong>原始的 ResNet block 之所以 1x1 卷积降通道，是为了减少计算量，不然中间的 3x3 卷积计算量太大。所以是两边宽中间窄的沙漏形。但现在中间的 3x3 卷积为 Depthwise 的，计算量很少了，所以通道多一点效果更好，所以通过 1x1 卷积先提升通道数。</strong>两端的通道数都很小，所以 1x1 卷积升通道或降通道计算量都不大，而中间通道数虽然多，但 Depthwise 的卷积计算量也不大。是两边窄中间宽的柳叶形。</p><p>注：示意表达式省略了Shortcut。</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/7.png" alt="x" style="zoom:50%;"></p><h4 id="4-shufflenet-V2"><a href="#4-shufflenet-V2" class="headerlink" title="4. shufflenet V2"></a>4. shufflenet V2</h4><h5 id="shufflenetV2-提出了一种channel-split操作-1-将输入channels分为两部分，-2-一部分保持不变，另一部分由三个卷积组成-3-卷积之后，将两部分拼接起来-4-最后在通过channel-shuffle操作来保持两个分支间的信息交流。"><a href="#shufflenetV2-提出了一种channel-split操作-1-将输入channels分为两部分，-2-一部分保持不变，另一部分由三个卷积组成-3-卷积之后，将两部分拼接起来-4-最后在通过channel-shuffle操作来保持两个分支间的信息交流。" class="headerlink" title="shufflenetV2 提出了一种channel split操作, (1) 将输入channels分为两部分， (2) 一部分保持不变，另一部分由三个卷积组成  (3) 卷积之后，将两部分拼接起来 (4) 最后在通过channel shuffle操作来保持两个分支间的信息交流。"></a><strong>shufflenetV2 提出了一种channel split操作, (1) 将输入channels分为两部分， (2) 一部分保持不变，另一部分由三个卷积组成  (3) 卷积之后，将两部分拼接起来 (4) 最后在通过channel shuffle操作来保持两个分支间的信息交流。</strong></h5><h5 id="具体实现：-2"><a href="#具体实现：-2" class="headerlink" title="具体实现："></a>具体实现：</h5><p><strong>(1) 基本单元</strong></p><p>如下左图所示</p><p>(1) 在每个单元的开始，<code>c</code> 特征通道的输入被分为两支，分别带有 <code>c−c&#39;</code> 和 <code>c&#39;</code>个通道。</p><p>(2)  一个分支仍然保持不变。另一个分支由三个卷积组成，令输入和输出通道相同。与 ShuffleNet V1 不同的是，两个 1×1 卷积不再是组卷积。是因为分割操作已经产生了两个组。</p><p>(3) 卷积之后，把两个分支拼接起来，从而通道数量保持不变。</p><p>(4) 然后进行与 ShuffleNet V1 相同的「Channel Shuﬄe」操作来保证两个分支间能进行信息交流。「Shuffle」之后，下一个单元开始运算。</p><p>注意，! ShuﬄeNet V1 [15] 中的「加法」操作不再存在。! 像 ReLU 和深度卷积这样的操作只存在一个分支中。另外，! 三个连续的操作「拼接」、「Channel Shuﬄe」和「通道分割」合并成一个操作。</p><p><strong>空间下采样单元：</strong></p><p>对于空间下采样，该单元经过稍微修改，详见下图右， 通道分割运算被移除。因此，输出通道数量翻了一倍。</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/9.png" alt="x" style="zoom:50%;"></p><p>ShuffleNet V2 对高效的网络架构设计得出了 4 个实用的指导原则：</p><ol><li>卷积的输入通道 c1 和输出通道 c2 相同时，有最小的内存访问成本 (MAC)</li><li>过多的分组卷积增加 MAC，MAC 随着组数 g 的增长而增加</li><li>降低网络结构的破碎程度（减少分支以及所包含的基本单元）。”multi-path“结构使用许多碎片操作符，虽然有利于提高精度，但它降低了并行度</li><li>减少 element-wise 操作。包括 ReLU、AddTensor、AddBias等。它们的 FLOPs 较小，但 MAC 相对较大</li></ol><p>ShuffleNet V2 的设计原则基于以上四点：pointwise group 卷积和瓶颈结构都增加了 MAC (1和2)。这一成本是不可忽视的，特别是对轻量的模型。此外，使用太多组违反了 3。 快捷连接中的 element-wise Add”操作也是不可取的 (4)。因此，为了达到高的模型容量和效率，关键问题是保持同样宽的通道，既不密集卷积，也不过多组。</p><h4 id="5-MobileNet-V3"><a href="#5-MobileNet-V3" class="headerlink" title="5. MobileNet V3"></a>5. MobileNet V3</h4><p>​    用神经结构搜索（NAS）来完成 V3。参考了三种模型：MobileNe tV1 的深度可分离卷积、MobileNet V2 的具有线性瓶颈的反向残差结构、MnasNe+SE 的自动搜索模型。</p><p>相关技术：</p><ol><li>网络的架构为基于 <strong>NAS</strong> 实现的 MnasNet（效果比 MobileNet V2 好）。借鉴与 Mnasnet， 相对于 mobilenet V2, V3 启用 5×5 的 deepwise 卷积。</li><li>网络结构搜索中，结合两种技术：资源受限的 NAS（platform-aware NAS）与 NetAdapt。</li><li>引入 MobileNet V1 的深度可分离卷积、MobileNet V2 的具有线性瓶颈的倒残差结构、SENet 的 squeeze and excitation 结构、新的激活函数 h-swish。</li><li>修改 MobileNet V2 网络端部最后阶段为 1x1 卷积，减小计算。作者认为， V2 模型中的倒残差结构使用 1×1 卷积来构建最后层，以扩展到高维特征空间，可以提取更多更丰富的特征，但同时也引入了额外的计算成本与延时。所以，需要在保留高维特征的前提下减小延时。方法是将 1×1 层放到最终的平均池化之后。这样的话最后一组特征由 <strong>avgpool 7x7</strong> 变为 <strong>pool 7x7+conv2d 1x1</strong>。</li></ol><p>综合以上，V3 的 block 结构如下所示：</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/11.png" alt="x" style="zoom:60%;"></p><h4 id="6-ShuffleNetV2"><a href="#6-ShuffleNetV2" class="headerlink" title="6. ShuffleNetV2+"></a>6. ShuffleNetV2+</h4><p>没有相关论文， 只是旷视提出的一个简单的 shufflenetv2 的加强版本， 发布在 github 上。主要的操作是替换其中的激活函数为 h-swish， 并且添加了 SE 模块。</p><p>参考网址：<a href="https://github.com/megvii-model/ShuffleNet-Series/tree/master/ShuffleNetV2%2B">https://github.com/megvii-model/ShuffleNet-Series/tree/master/ShuffleNetV2%2B</a></p><h4 id="7-MobileNeXt"><a href="#7-MobileNeXt" class="headerlink" title="7. MobileNeXt"></a>7. MobileNeXt</h4><p>​        本文主要针对MobileNetV2中倒置残差块（inverted residual block）的设计和不足进行了分析，并基于分析结果提出了新颖的sandglass模块，这种轻量级模块有原生残差块和倒置残差块的影子，是一种正向残差设计。</p><ul><li><p>原生残差块组成：1x1卷积（降维<strong>降低模型复杂度</strong>）、3x3卷积（空间信息变换）、1x1卷积（升维<strong>用于和 shortcut 分支相加</strong>）</p></li><li><p>逆残差块组成：1x1卷积（升维<strong>提升模型效果</strong>）、3x3深度可分卷积（空间信息变换）、1x1卷积（降维）</p></li></ul><p>​        本文作者认为逆残差模块会削弱梯度跨层传播的能力，将特征从高维空间压缩到低维空间，会造成信息丢失，同时这也容易引起梯度混淆问题(特指梯度消失或梯度爆炸)，从而影响训练的收敛和最终模型的性能。本文提出的sandglass模块如下图所示：</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/15.png" alt="x" style="zoom:35%;"></p><p>​        sandglass 模块是在轻量级的残差块上依据逆残差块所存在的问题所改进的，同时这个模块还用到了MobileNetV2中的线性瓶颈设计，即只在第一个 3x3 卷积后边和第二个 1x1 卷积后边使用非线性激活函数 relu6，其他层后边使用线性激活函数(y=x), 这有助于避免零化现象的出现，进而减少信息损失</p><h4 id="8-xception"><a href="#8-xception" class="headerlink" title="8. xception"></a>8. xception</h4><p>Xception 的结构基于 ResNet，但是将其中的卷积层换成了 Separable Convolution（极致的 Inception模块）。 Xception（极致的 Inception）: 先进行1x1 卷积操作，再对 1×1 卷积后的每个channel分别进行 3×3 卷积操作，最后将结果 concat：</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/18.png" alt="x" style="zoom:35%;"></p><h4 id><a href="#" class="headerlink" title=" "></a> </h4><h4 id="9-efficinet-原理"><a href="#9-efficinet-原理" class="headerlink" title="9. efficinet 原理"></a>9. efficinet 原理</h4><p><strong>原论文: EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks ICML 2019</strong> </p><p><strong>利用复合系数统一缩放模型的所有维度，达到精度最高效率最高，符合系数包括 w, d, r，其中，w表示卷积核大小，决定了感受野大小；d表示神经网络的深度；r表示分辨率大小。</strong></p><p>文中总结了我们常用的三种网络调节方式：<strong>增大感受野w，增大网络深度d，增大分辨率大小r</strong>，三种方式示意图如下：其中，(a)为基线网络，也可以理解为小网络；(b)为增大感受野的方式扩展网络；(c)为增大网络深度d的方式扩展网络；(d)为增大分辨率r的方式扩展网络；(e)为本文所提出的混合参数扩展方式；</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/19.png" alt="x" style="zoom:75%;"></p><p><strong>复合系数的数学模型</strong></p><p>文中给出了一般卷积的数学模型如下：</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/20.png" alt="x" style="zoom:100%;"></p><p>其中 H, W为卷积核大小，C为通道数，X为输入tensor。 则复合系数的确定转为如下的优化问题：</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/21.png" alt="x" style="zoom:100%;"></p><p>调节 d, w, r 使得满足内存Memory和浮点数量都小于阈值要求； 为了达到这个目标，文中提出了如下的方法：</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/22.png" alt="x" style="zoom:100%;"></p><p>对于这个方法，我们可以通过一下两步来确定d, w, r参数： 第一步是固定Φ=1，然后通过网格搜索找到满足公式3的最优α、β、γ，比如对于EfficientNet-B0网络而言，最佳的参数分别是α=1.2、β=1.1、γ=1.15（此时得到的也就是EfficientNet-B1）。第二步是固定第一步求得的α、β、γ参数，然后用不同的Φ参数得到EfficientNet-B1到EfficientNet-B7网络。</p><h4 id="10-GhostNet"><a href="#10-GhostNet" class="headerlink" title="10. GhostNet"></a>10. GhostNet</h4><h5 id="1-出发点："><a href="#1-出发点：" class="headerlink" title="(1) 出发点："></a>(1) 出发点：</h5><p>​      通过对比分析 ResNet-50 网络第一个残差组( Residual group )输出的特征图可视化结果，发现一些特征图高度相似。如果按照传统的思考方式，可能认为这些相似的特征图存在冗余，是多余信息，想办法避免产生这些高度相似的特征图。本文的思路是<strong>不去刻意的避免产生这种Ghost对，而是尝试利用简单的线性操作来获得更多的Ghost对。</strong></p><p>其基本操作如下：</p><ul><li>使用常规卷积(这里使用的 conv1x1)来获得本征特征层。</li><li>通过一个线性操作(这里使用的是 depthwise 操作)对本征特征层进行线性变化，获得 ghost 特征层。</li><li>将 ghost 特征层和 ghost 特征层进行拼接。</li></ul><p><img src="/2020/11/15/lightweight-cnn-architecture-design/13.png" alt="x" style="zoom:50%;"></p><p>通过将 ResNet 中的 Residual Block 中的卷积操作用 Ghost Module 替换就可以得到 Ghost BottleNeck。图下图所示:</p><p><img src="/2020/11/15/lightweight-cnn-architecture-design/14.png" style="zoom:50%;"></p><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><ul><li><p>关于网络结构的变迁， 如下图所示(所有 conv 层之后，都使用了 BN 层， 且 BN 层在激活函数之前)</p><ul><li>mobilenet v1 将普通卷积分解为  DepthWise 和 PointWise 两个操作</li><li>shufflenet v1 引入 channel shuffle 操作，初步形成 GConv + shuffle + DW + GConv 的卷积组合</li><li>mobilenet v2 引入了 逆残差模块， 此时的形成了 PW+DW+PW 的组合。 注意这里最后一个 PW 使用线性函数</li><li>shufflent v2 引入了 channels split 操作， 并将 concat + shuffle + split 进行合并</li></ul></li></ul><p><img src="/2020/11/15/lightweight-cnn-architecture-design/17.png" alt></p><ul><li><p>一些小经验</p><ul><li>使用 DW 和 PW 进行加速是常规操作， 但是对 DW和 PW 的顺序还需要进一步探索。（比如 mobilenext 则使用了 DW + PW + PW+ DW 的组合形式）</li><li><strong>MobileNetV2的倒置残差模块 + SE block + H-swish 是一个屡试不爽的组合</strong>，虽然速度会有所损失。</li><li><strong>激活函数的放置位置还没有定论</strong>（虽然 mobilenet v2 进行了探索，提出了线性瓶颈）</li><li>有一些网络(ghostnet 和 bsconv ) 提出了<strong>对卷积核/特征层的冗余进行探索分析</strong>，从而降低运算量， 这也算一个方向。</li></ul></li><li><p>一些对于轻量级检测和分割网络的探索： 轻量级检测的核心问题还是低层特征和高层特征的融合问题，在 backbone 通常选择上面的 轻量级分类网络：</p><ul><li>Pelee: A Real-Time Object Detection System on Mobile Devices</li><li>Tiny-dsod: Lightweight object detection for resource-restricted usages.</li><li>Light-head R-cnn: In defense of two-stage object detector</li><li>ThunderNet: Towards Real-time Generic Object Detection</li><li><p>NanoDet</p></li><li><p>BiSeNet（分割）</p></li><li>DFANet（分割）</li></ul></li><li><p>MIT Han Lab 在模型压缩和加速方向做了很多探索性的工作， 可以重点关注。</p></li><li><p>nas 在相关领域一度很火，诞生了 MnasNet、PorxylessNas、FBNet  等工作。但是由于需要资源较多等原因，一直未加尝试。</p></li></ul><h4 id="参考论文："><a href="#参考论文：" class="headerlink" title="参考论文："></a>参考论文：</h4><ul><li><p>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size.</p></li><li><p>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</p></li><li><p>MobileNetV2: Inverted Residuals and Linear Bottlenecks</p></li><li><p>Searching for MobileNetV3(nas for efficient conv neural network) </p></li><li><p>ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</p></li><li><p>ShuffleNet V2: Practical Guidelines for Ecient CNN Architecture Design </p></li><li><p>Xception: Deep Learning with Depthwise Separable Convolutions</p></li></ul><p>mobilent 逆残差结构的实现：</p><pre><code class="hljs python"><span class="hljs-comment"># Mobilenet V2</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">InvertedResidual</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, inp, oup, stride, expand_ratio</span>):</span>        super(InvertedResidual, self).__init__()        self.stride = stride        self.use_res_connect = self.stride == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> inp == oup        self.conv = nn.Sequential(            <span class="hljs-comment"># pw</span>            nn.Conv2d(inp, inp * expand_ratio, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),            nn.BatchNorm2d(inp * expand_ratio),            nn.ReLU6(inplace=<span class="hljs-literal">True</span>),            <span class="hljs-comment"># dw</span>            nn.Conv2d(inp * expand_ratio, inp * expand_ratio, <span class="hljs-number">3</span>, stride, <span class="hljs-number">1</span>, groups=inp * expand_ratio, bias=<span class="hljs-literal">False</span>),            nn.BatchNorm2d(inp * expand_ratio),            nn.ReLU6(inplace=<span class="hljs-literal">True</span>),            <span class="hljs-comment"># pw-linear</span>            nn.Conv2d(inp * expand_ratio, oup, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span>),            nn.BatchNorm2d(oup),        )    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        <span class="hljs-keyword">if</span> self.use_res_connect:            <span class="hljs-keyword">return</span> x + self.conv(x)        <span class="hljs-keyword">else</span>:            <span class="hljs-keyword">return</span> self.conv(x)</code></pre>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>quantizing</title>
    <link href="/2020/11/13/quantizing/"/>
    <url>/2020/11/13/quantizing/</url>
    
    <content type="html"><![CDATA[<p>​        模型量化是指权重或激活输出可以被聚类到一些离散、低精度(reduced precision) 的数值点上，这通常依赖于特定算法库或硬件平台的支持：</p><h4 id="1-二值网络"><a href="#1-二值网络" class="headerlink" title="1. 二值网络"></a>1. 二值网络</h4><p><strong>Binary Weight</strong> (只对权重进行二值化)</p><p>🌟 [BinaryConnect] BinaryConnect: Training Deep Neural Networks with binary weights during propagations</p><p>🌟 [BWN]  Binary-Weights-Networks</p><p><strong>Binary Weight &amp; activation</strong> (对权重和激活都进行二值化)</p><p>🌟 [BNN] <a href="https://arxiv.org/pdf/1602.02830.pdf">Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1</a></p><p>🌟 [XNOR Net] <a href="https://arxiv.org/pdf/1603.05279.pdf">XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</a></p><p>-&gt; [ABCNet] Towards Accurate Binary Convolutional Neural Network by Xiaofan Lin, Cong Zhao, and Wei Pan.</p><p>-&gt; [Bi-Real Net] Enhancing the Performance of 1bit CNNs with Improved Representational Capacity and Advanced Training Algorithm</p><p>-&gt; [HORQ]Performance Guaranteed Network Acceleration via High-Order Residual Quantization</p><h4 id="2-三值化网络"><a href="#2-三值化网络" class="headerlink" title="2. 三值化网络"></a>2. 三值化网络</h4><p>🌟 [TWN] <a href="https://arxiv.org/pdf/1605.04711.pdf">Ternary weight networks</a></p><p>[TNN] Ternary Neural Networks for Resource-Efficient AI Applications </p><p>[TTQ] Trained Ternary Quantization </p><h4 id="3-2bit-8bit"><a href="#3-2bit-8bit" class="headerlink" title="3. 2bit - 8bit"></a>3. 2bit - 8bit</h4><p>🌟 [DOREFA-NET] DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients</p><p><img src="/Users/zhichaozhao/Documents/hexo-blog/source/_posts/神经网络量化与压缩综述/2.png" alt></p><h4 id="4-int8-量化"><a href="#4-int8-量化" class="headerlink" title="4. int8 量化"></a>4. int8 量化</h4><p>🌟 <strong>8-bit Inference with TensorRT  [Nvidia]</strong></p><p>🌟 Quantizing deep convolutional networks for efficient inference: A whitepaper [Google]</p><h4 id="2-量化"><a href="#2-量化" class="headerlink" title="(2) 量化"></a>(2) 量化</h4><p>首先说明，量化还有几个有意思的名称，定点化、离散化。</p><p>​       <strong>低精度的量化：如2bit、4bit 的量化也是有一些进展的， 但是具体还需要结合硬件，这个之后再说， 现在比较成熟的解决方案是 8bit 量化。可以真实应用于实际场景中</strong>。</p><p>1：对称量化算法</p><p>对称算法就是指将待量化数据的绝对值的最大值映射到新数据范围内的最大值。</p><p><img src="https://pic2.zhimg.com/80/v2-005fcae44d9732a08c50958de2636d25_1440w.jpg" alt="img"></p><p>2：非对称量化算法</p><p>非对称算法是指将待量化数据的最大值和最小值映射到新数据范围内的最大值最小值。</p><p><img src="https://pic3.zhimg.com/80/v2-376579c1f4b7e192b0592ae76a87d8f6_1440w.jpg" alt="img"></p><p><strong>量化最核心的内容是找到这个 scale。</strong></p><p>​     将浮点数映射到低比特int， 从而减少模型的体积和计算量， 加快推理速度。目前，学术界主要讲量化分为两大类： Post Training Quantization 和 Quantization Aware Training。</p><p>Quantization Aware Training 是在训练过程中对量化进行建模以确定量化参数。它可以提供更高的预测精度。</p><p>Post Training Quantization： 使用 KL 散度， 滑动平均等方法确定量化参数且不需要重新训练的定点量化方法。</p><p><img src="/2020/11/13/quantizing/4.png" style="zoom:30%;"></p><p>下面着重介绍训练后量化： 训练后量化是基于采样数据，采用KL散度等方法计算量化比例因子的方法。相比量化训练，训练后量化不需要重新训练，可以快速得到量化模型。</p><p>​        训练后量化的目标是求取量化比例因子，主要有两种方法：</p><ul><li><p>非饱和量化方法 ( No Saturation) ：计算FP32类型Tensor中绝对值的最大值<code>abs_max</code>，将其映射为127，则量化比例因子等于<code>abs_max/127</code>。 ———&gt;  <strong>这种方式被称作 minmax 量化方式， Tensorflow 主要采用， 精度还是很差的</strong>。</p><p>主要流程如下：</p><p>（1）统计出网络某一层的最大值与最小值：</p><p><img src="https://www.zhihu.com/equation?tex=x_%7Bfloat%7D%5Cepsilon+%5Bx_%7Bfloat%7D%5E%7Bmax%7D%2Cx_%7Bfloat%7D%5E%7Bmin%7D%5D" alt="[公式]"></p><p>（2）计算scale与zero_point</p><p><img src="https://www.zhihu.com/equation?tex=x_%7Bscale%7D%3D%5Cfrac%7Bx_%7Bfloat%7D%5E%7Bmax%7D-x_%7Bfloat%7D%5E%7Bmin%7D%7D%7Bx_%7Bquantized%7D%5E%7Bmax%7D-x_%7Bquantized%7D%5E%7Bmin%7D%7D" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=x_%7Bzeropoint%7D%3Dx_%7Bquantized%7D%5E%7Bmax%7D-%5Cfrac%7Bx_%7Bfloat%7D%5E%7Bmax%7D%7D%7Bx_%7Bscale%7D%7D" alt="[公式]"></p><p>（3）通过以下公式计算出任意float32量化后的int8结果</p><p><img src="https://www.zhihu.com/equation?tex=x_%7Bquantized%7D%3D%7B%5Cfrac%7Bx_%7Bfloat%7D%7D%7Bx_%7Bscale%7D%7D%7D%2Bx_%7Bzeropoint%7D" alt="[公式]"></p><p>由公式可以看出量化中的精度损失不可避免的，当浮点数的分布均匀时，精度损失较小。但当浮点数分布不均匀时，按照最大最小值映射，则实际有效的int8动态范围就更小了，精度损失变大。</p></li><li><p>饱和量化方法 (Saturation)： 使用KL散度计算一个合适的阈值<code>T</code> (<code>0&lt;T&lt;mab_max</code>)，将其映射为127，则量化比例因子等于<code>T/127</code>。一般而言，对于待量化op的权重Tensor，采用非饱和量化方法，对于待量化op的激活Tensor（包括输入和输出），采用饱和量化方法 。       ——&gt;   <strong>这种方法主要是 Tensor RT 在采用， 量化效果还是不错的</strong></p></li></ul><p>训练后量化的实现步骤如下:</p><p>（1）读取样本数据，执行模型的前向推理， 保存激活 tensor 的数值。</p><p>（2）基于激活 Tensor 的采样数据， 使用饱和量化方法计算出它的量化比例因子（用于量化输入和输出）</p><p>（3）使用非饱和量化的方法计算每个通道的绝对值的最大值，作为每个通道的量化比例因子，对模型参数进行量化，将 fp32 模型转化成 int8模型进行保存。</p><p> ADMM</p><p>🌟 ACIQ</p><p>🌟 easyquant</p><p>​     若模型压缩之后，推理精度存在较大损失，可以通过fine-tuning予以恢复，并在训练过程中结合适当的Tricks，例如 Label Smoothing、Mix-up、Knowledge Distillation、Focal Loss等。 此外，模型压缩、优化加速策略可以联合使用，进而可获得更为极致的压缩比与加速比。</p><h4 id="5-其他"><a href="#5-其他" class="headerlink" title="5. 其他"></a>5. 其他</h4><p>🌟 [INQ] Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights</p><p>[CNNPack] Packing Convolutional Neural Networks in the Frequency Domain</p><p>[HWGQ]  Deep Learning with Low Precision by Half-wave Gaussian Quantization</p><p>[FFN] Fixed-point Factorized Networks</p><p>[QNN] Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations </p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>int8_quantize</title>
    <link href="/2020/11/07/int8-quantize/"/>
    <url>/2020/11/07/int8-quantize/</url>
    
    <content type="html"><![CDATA[<h3 id="1-FP32-vs-int8"><a href="#1-FP32-vs-int8" class="headerlink" title="1. FP32 vs int8"></a>1. FP32 vs int8</h3><p>​        首先来看一下 <strong>FP32、FP16 和 int8</strong>之间的动态范围和精度的对比， 可以看到float32 的取值范围几乎是无穷的，而int8只有<strong>-128~127</strong>. 因此需要建立映射关系将float32类型的浮点数映射到指定范围的int8类型。 </p><p><img src="/2020/11/07/int8-quantize/fpvsint8.png" alt></p><p>​        目前最常见的两种<strong>int8量化实现方式</strong>是 <strong>Nvidia的TensorRT方案</strong> 和 <strong>google的方案</strong>，其中前者直接量化，无需retrain，实现简单；而后者需要重新训练，稍显复杂。 大部分的开源方案是基于前者的，<strong>本文基于ncnn框架，主要讲述第一种解决方案。</strong></p><h3 id="2-TensorRT-int8-量化方案"><a href="#2-TensorRT-int8-量化方案" class="headerlink" title="2. TensorRT int8 量化方案"></a>2. TensorRT int8 量化方案</h3><p>​    Nvidia 的 TensorRT提供了一种量化方案，但是它仅仅提供相应的SDK和解决方案， 没有公布对应的源代码， 诸多第三方厂家则根据该解决方案自己造轮子，产生了对应的解决方案。该量化方案的最重要的两份参考资料如下所示:</p><ul><li><p>TensorRT Develop guide: <a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#work-with-qat-networks">https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#work-with-qat-networks</a></p></li><li><p>PDF 链接： <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf</a></p></li></ul><p><strong>(1)</strong> <strong>max-max 映射</strong>： 最简单粗暴的方式如下左图所示</p><p>​    首先求出一个laye 的激活值范围， 然后按照绝对值的最大值作为阈值， 然后把这个范围按照比例映射到-127到128的范围内, 其fp32和int8的转换公式为:</p><pre><code class="hljs lisp">FP32 Tensor (<span class="hljs-name">T</span>) = scale_factor(<span class="hljs-name">sf</span>) * <span class="hljs-number">8</span>-bit Tensor(<span class="hljs-name">t</span>) + FP32_bias (<span class="hljs-name">b</span>)</code></pre><p>​    通过实验得知，bias值去掉对精度的影响不是很大，因此我们直接去掉, 所以该公式可以简化为:</p><pre><code class="hljs excel"><span class="hljs-built_in">T</span> = sf * <span class="hljs-built_in">t</span></code></pre><p><img src="/2020/11/07/int8-quantize/tensorrt_qua.png" alt></p><p><strong>(2) 饱和映射</strong></p><p>​    如上方法会有一个问题：<strong>不饱和，即通常在正负上会有一些量化值未被利用，且会产生的精度损失较大。</strong>针对 max-max 映射存在的问题， TensorRT提出了如上右图的饱和映射。 <strong>选取一个阈值T，然后将 -|T|~|T| 之间的值映射到 -127 到 128 这个范围内。这样确定了阈值T之后，其实也能确定Scale，一个简单的线性公式是: Scale = T/127。 所以要计算Scale，只要找到合适的阈值T就可以了。那么问题来了，T应该取何值? 其基本流程如下:</strong></p><p>​    <strong>(a) 选取不同的 T 阈值进行量化, 将 P(fp32) 映射到 Q(int8)。</strong></p><p>​    <strong>(b) 将 Q(int8) 反量化到 P(fp32) 一样长度，得到分布 Q_expand；</strong></p><p>​    <strong>(c) 计算P和Q_expand 的相对熵(KL散度)，然后选择相对熵最少的一个，也就是跟原分布最像的一个,</strong> <strong>从而确定Scale</strong>。</p><p><strong>(3) KL 散度</strong></p><p>​    KL散度可以用来<strong>描述P、Q两个分布的差异</strong>。<strong>散度越小，两个分布的差异越小，概率密度函数形状和数值越接近</strong>。这里的所有分布、计算，都是离散形式的。分布是以统计直方图的方式存在，KL散度公式也是离散公式：</p><p><img src="/2020/11/07/int8-quantize/kl.png" alt="img"></p><p>​        从上式中我们还发现一个问题：KL散度计算公式要求P、Q两个统计直方图长度一样（也就是bins的数量一样）。Q一直都是-127～127；可是P的数量会随着T的变化而变化。那这怎么做KL散度呢？</p><p>ncnn 的做法是将 Q扩展到和P一样的长度，下面举个例子(NVIDIA PPT中的例子)：</p><pre><code class="hljs python">P = [<span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">5</span> <span class="hljs-number">3</span> <span class="hljs-number">1</span> <span class="hljs-number">7</span>]     // fp32的统计直方图，T=<span class="hljs-number">8</span> // 假设只量化到两个bins，即量化后的值只有<span class="hljs-number">-1</span>/<span class="hljs-number">0</span>/+<span class="hljs-number">1</span>三种 Q=[<span class="hljs-number">1</span>+<span class="hljs-number">0</span>+<span class="hljs-number">2</span>+<span class="hljs-number">3</span>, <span class="hljs-number">5</span>+<span class="hljs-number">3</span>+<span class="hljs-number">1</span>+<span class="hljs-number">7</span>] = [<span class="hljs-number">6</span>, <span class="hljs-number">16</span>]  // P和Q现在没法做KL散度，所以要将Q扩展到和P一样的长度 Q_expand = [<span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>]=[<span class="hljs-number">2</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span>]  // P中有<span class="hljs-number">0</span>时不算在内 D = KL(P||Q_expand)  // 这样就可以做KL散度计算了</code></pre><p>​    这个扩展的操作，就像图像的上采样一样，将低精度的统计直方图(Q)，上采样的高精度的统计直方图上去(Q_expand)。由于Q中一个bin对应P中的4个bin，因此在Q上采样的Q_expand的过程中，所有的数据要除以4。另外，在计算fp32的分布P时，被T截断的数据，是要算在最后一个bin里面的。</p><h3 id="3-ncnn的conv量化计算流程"><a href="#3-ncnn的conv量化计算流程" class="headerlink" title="3. ncnn的conv量化计算流程"></a>3. ncnn的conv量化计算流程</h3><p>正常的 fp32 计算中， 一个conv 的计算流程如下所示， 所有的数据均是 fp32， 没什么特殊的</p><p><img src="/2020/11/07/int8-quantize/fpconv.png" alt></p><p>在 ncnn conv 进行Int8计算时， 计算流程如下所示，ncnn首先<strong>将输入(bottom_blob)和权重量化成Int8，在Int8下计算卷积，然后反量化到 fp32，再和未量化的bias相加，得到输出 top_blob</strong>(ncnn并没有对bias做量化)</p><p><img src="/2020/11/07/int8-quantize/int8conv.png" alt></p><p>输入和权重的<strong>量化公式</strong>为:</p><script type="math/tex; mode=display">bottom\_blob[int8] = bottom\_blob\_in8t\_scale * bottom[fp32] \\weight\_blob[int8] = weight\_data\_int8\_scale * weight[fp32]</script><p><strong>反量化</strong>的目的是将int8映射回到原来的fp32,范围保持要一致, 由于 weight_blob(int8) 和 bottom_blob(int8) 相乘， 所以此处的量化反量化的 scale 应该为:</p><script type="math/tex; mode=display">dequantize\_scale = 1/(bottom\_blob\_int8\_scale * weight\_data\_int8\_scale) \\innner\_blob[fp32] = dequantize\_scale * inner\_blob</script><p>! 值得注意的是， 权重是在网络初始化时候就进行量化了， 而输入则是在前向推导时进行量化。</p><h3 id="4-ncnn-量化工具的使用"><a href="#4-ncnn-量化工具的使用" class="headerlink" title="4. ncnn 量化工具的使用"></a>4. ncnn 量化工具的使用</h3><p>(1)  Optimization graphic</p><pre><code class="hljs smali"><span class="hljs-keyword">.</span>/ncnnoptimize mobilenet-fp32.param mobilenet-fp32.bin mobilenet-nobn-fp32.param mobilenet-nobn-fp32.bin</code></pre><p>(2) Create the calibration table file</p><pre><code class="hljs angelscript">./ncnn2table --param mobilenet-nobn-fp32.param --bin mobilenet-nobn-fp32.bin --images images/ --output mobilenet-nobn.table --mean <span class="hljs-number">104</span>,<span class="hljs-number">117</span>,<span class="hljs-number">123</span> --norm <span class="hljs-number">0.017</span>,<span class="hljs-number">0.017</span>,<span class="hljs-number">0.017</span> --size <span class="hljs-number">224</span>,<span class="hljs-number">224</span> --thread <span class="hljs-number">2</span></code></pre><p>(3) Quantization</p><pre><code class="hljs stylus">./ncnn2int8 mobilenet-nobn-fp32<span class="hljs-selector-class">.param</span> mobilenet-nobn-fp32<span class="hljs-selector-class">.bin</span> mobilenet-int8<span class="hljs-selector-class">.param</span> mobilenet-int8<span class="hljs-selector-class">.bin</span> mobilenet-nobn.table</code></pre><h3 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h3><p>[1] <a href="https://me.csdn.net/sinat_31425585">https://me.csdn.net/sinat_31425585</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/c_1064124187198705664">https://zhuanlan.zhihu.com/c_1064124187198705664</a></p><p>[3] <a href="https://github.com/BUG1989/caffe-int8-convert-tools">https://github.com/BUG1989/caffe-int8-convert-tools</a></p><p>[4] <a href="https://github.com/Tencent/ncnn/wiki/quantized-int8-inference">Tencent/ncnn</a></p><p>[5] QNNPACK</p><p>[6] Nvidia solution： Szymon Migacz. 8-bit Inference with TensorRT</p><p>[7] Google solution：Quantizing deep convolutional networks for efficient inference: A whitepaper</p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn前向计算流程浅析</title>
    <link href="/2020/09/21/ncnn%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B%E6%B5%85%E6%9E%90/"/>
    <url>/2020/09/21/ncnn%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B%E6%B5%85%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p><strong>1. 前向计算</strong></p><p>​    使用ncnn进行前向计算的步骤很简单，就如下十几行代码即可完成。</p><pre><code class="hljs groovy"><span class="hljs-comment">/* Step 1.1 : 加载.parma 文件 */</span>NSString *paramPath = [[NSBundle mainBundle] <span class="hljs-attr">pathForResource:</span>@<span class="hljs-string">&quot;squeezenet_v1.1&quot;</span> <span class="hljs-attr">ofType:</span>@<span class="hljs-string">&quot;param&quot;</span>];ncnn_net.load_param(paramPath.UTF8String);<span class="hljs-comment">/* Step 1.2 : 加载.bin 文件 */</span>NSString *binPath = [[NSBundle mainBundle] <span class="hljs-attr">pathForResource:</span>@<span class="hljs-string">&quot;squeezenet_v1.1&quot;</span> <span class="hljs-attr">ofType:</span>@<span class="hljs-string">&quot;bin&quot;</span>];ncnn_net.load_model(binPath.UTF8String);<span class="hljs-comment">/* Step 2.1 : 构建并配置 提取器 */</span><span class="hljs-attr">ncnn:</span>:Extractor extractor = ncnn_net.create_extractor();extractor.set_light_mode(<span class="hljs-literal">true</span>);<span class="hljs-comment">/* Step 2.2 : 设置输入（将图片转换成ncnn::Mat结构作为输入） */</span>UIImage *srcImage = [UIImage <span class="hljs-attr">imageNamed:</span>@<span class="hljs-string">&quot;mouth&quot;</span>];<span class="hljs-attr">ncnn:</span>:Mat mat_src;ts_image2mat(mat_src, srcImage);extractor.input(<span class="hljs-string">&quot;data&quot;</span>, mat_src);<span class="hljs-comment">/* Step 2.3 : 提取输出 */</span><span class="hljs-attr">ncnn:</span>:Mat mat_dst;extractor.extract(<span class="hljs-string">&quot;prob&quot;</span>, mat_dst);</code></pre><p>​    如果你仅仅想使用ncnn，上面的参考足够了；但若你想要了解，甚至去更改一些其中的源代码，可以跟我一起看看上面这十多行代码的底层运作原理。</p><p><strong>2 代码分析</strong></p><p>我姑且将其分为：<strong>加载模型</strong>、前向检测、输出处理(半划水)、模型封装(全划水) 四个部分来加以分析。</p><p><strong>2.1 加载模型</strong></p><p>ncnn 在 iOS 端使用 <strong>.param</strong> 和 <strong>.bin</strong> 两个文件来描述一个神经网络模型，</p><p>其中：</p><p><strong>.param</strong>：描述神经网络的结构，包括层名称，层输入输出信息，层参数信息（如卷积层的kernal大小等）等。</p><p><strong>.bin</strong> 文件则记录神经网络运算所需要的数据信息（比如卷积层的权重、偏置信息等）</p><p><img src="/2020/09/21/ncnn%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B%E6%B5%85%E6%9E%90/1.png" alt="x" style="zoom:50%;"></p><p><strong>2.1.1 load_param 加载神经网络配置信息</strong></p><pre><code class="hljs objective-c">&#x2F;* Step1.1 : 加载.parma 文件 *&#x2F;NSString *paramPath &#x3D; [[NSBundle mainBundle] pathForResource:@&quot;squeezenet_v1.1&quot; ofType:@&quot;param&quot;];ncnn_net.load_param(paramPath.UTF8String);</code></pre><p>load_param 的根本目的是将.param文件的信息加载到目标神经网络（一个ncnn::Net结构）中</p><p><strong>2.1.1.1 .param文件的结构</strong></p><p>首先我们看一下 .param 文件的内容格式</p><p><img src="/2020/09/21/ncnn%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B%E6%B5%85%E6%9E%90/2.png" style="zoom:50%;"></p><p><strong>一个.param文件由以下几部分组成：</strong></p><p><strong>1）MagicNum</strong></p><p>固定位7767517，为什么这个数字，不知道问倪神去吧</p><p>2）layer、blob个数</p><p>上图示例的文件两个数字分别为：75、83</p><p><strong>layer：我们知道神经网络是一层一层向前推进计算的，每一层我们用一个layer表示；</strong></p><p><strong>blob：每一个layer都可能会有输入、输出，在ncnn中，它们统一用一个多维（3维）向量表示，我们称每一个输入、输出的原子为一个blob，并为它起名</strong></p><p>2.1.1.2 layer的描述</p><p>layer 在 .param 中是一个相对复杂的元素（从第3行起的每一行描述一个layer），所以我们把它单独抽出来一小节进行说明。</p><p><img src="/2020/09/21/ncnn%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B%E6%B5%85%E6%9E%90/3.png" alt></p><p>如图，<strong>每一行层描述的内容包括以下几部分</strong>：</p><p><strong>1）层类型</strong>     比如<strong>Input、Convolution、ReLU</strong></p><p><strong>2）层名</strong>       模型训练者为该层起得名字（毕竟相同类型的层可能多次使用，我们要区分它们）</p><p><strong>3）层输入输出  包含：层输入blob数量，层输出blob数量，层输入、输出blob的名称</strong></p><p><strong>4）层配置参数</strong></p><p>比如 <strong>卷积层（Convolution Layer）的 卷积核大小、步长信息</strong> 等</p><p>在 具体层里面都有一个函数: load_param, 从里面可以查询到相关信息。</p><p><strong>data层： 0=长 1=宽 3=通道</strong></p><p><strong>Convolution层 0=输出单元 1=卷积核大小  2=核膨胀[见膨胀卷积]    3=stride</strong>    </p><p>​                                 <strong>4=padding    5=是否存在偏置    6=权重数量</strong></p><p><strong>pooling 层    0=池化类型   1=卷积核大小   2=步长stride   3=padding   4=全局池化  5=padding类型</strong></p><p><strong>ReLU 层 0=0.000000 无参数</strong></p><p><strong>softmax 层 0=0 无参数</strong></p><p><strong>Concat Split Dropout 无参数</strong></p><p><strong>ConvolutionDepthWise    7=group 数目</strong></p><p><strong>2.1.1.3 ncnn的加载的效果</strong></p><p>​    其实了解了param文件的数据结构后，我们就大致知道ncnn做了哪些事情了。无非是 <strong>读取文件</strong>—&gt;<strong>解析神经网络信息</strong> —&gt; <strong>缓存神经网络信息</strong>，那么，信息缓存在哪里呢？</p><pre><code class="hljs cpp"><span class="hljs-comment">/* in net.h */</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span></span><span class="hljs-class">&#123;</span>...<span class="hljs-keyword">protected</span>:    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Blob&gt; blobs;    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Layer*&gt; layers;...&#125;;</code></pre><p>​    原来，ncnn::Net 结构中有 <strong>blobs</strong> 和 <strong>layers</strong> 两个 vector，它们保存了 .param文件 中加载的信息。关于 Blob、Layer 的数据结构，在此暂不赘述。</p><p><strong>2.1.2 load_model 加载模型训练数据</strong></p><pre><code class="hljs objective-c">&#x2F;* Step1.2 : 加载.bin 文件 *&#x2F;NSString *binPath &#x3D; [[NSBundle mainBundle] pathForResource:@&quot;squeezenet_v1.1&quot; ofType:@&quot;bin&quot;];ncnn_net.load_model(binPath.UTF8String);</code></pre><p>​    load_model的根本目的是将 .bin文件 的信息加载到 目标神经网络（一个ncnn::Net结构）中。</p><p><strong>2.1.2.1 .bin文件的内容</strong></p><p><strong>.bin 文件存储了对应模型中部分层的计算需求参数。</strong>比如2.1.1.1节中的第四行的Convolution层</p><p><img src="/2020/09/21/ncnn%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B%E6%B5%85%E6%9E%90/4.png" alt></p><p>​    <strong>.bin 文件中就存储了其 1728(3 * 3 * 3 * 64) 个float类型的 权重数据(weight_data) 和 64个float类型的 偏置数据(bias_data)。</strong></p><p>2.1.2.2 .bin文件的结构</p><p>​    <strong>.bin 文件中就存储了其 1728(3 * 3 * 3 * 64) 个float类型的 权重数据(weight_data) 和 64个float类型的 偏置数据(bias_data)。</strong></p><p>2.1.2.2 .bin文件的结构</p><p><img src="/2020/09/21/ncnn%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B%E6%B5%85%E6%9E%90/5.png" alt></p><p>bin = binary，.bin 文件的基本结构就是 [二进制]。但这 并不代表我们失去了 [手动修改它] 的权利！</p><p>惊不惊喜，意不意外？下节即揭晓！</p><p><strong>2.1.2.3 手撕二进制</strong></p><p>1）bin文件信息存储说明</p><p>假设 bin 文件存储 0.3342, 0.4853, 0.2843, 0.1231 四个数字，这四个数字使用float32的数据结构来描述，分别为：3eab1c43、3ef8793e、3e918fc5、3dfc1bda，那么bin文件中的内容就是 3eab1c433ef8793e3e918fc53dfc1bda，我们进行读取的时候使用一个float的数组去承载这些二进制数据即可。</p><p>2）手撕</p><p>你当然也可以自己写一段bin文件数据的读取方法，比如这么一段</p><pre><code class="hljs lsl">const void * __log_binInfo_conv1(const void *dataOffset) &#123;    printf(<span class="hljs-string">&quot;<span class="hljs-subst">\n</span> conv1 层类型为 Convolution(卷积层)<span class="hljs-subst">\n</span>&quot;</span>)     printf(<span class="hljs-string">&quot;<span class="hljs-subst">\n</span> 加载weight_data数据类型标志（固定为自动类型）<span class="hljs-subst">\n</span>&quot;</span>);    unsigned char *p_load1_1 = (unsigned char *)dataOffset;    for (int i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">4</span>; i++) &#123;        printf(<span class="hljs-string">&quot;Flag %d : %d<span class="hljs-subst">\n</span>&quot;</span>, i, p_load1_1[i]);    &#125;    p_load1_1 += <span class="hljs-number">4</span>;        printf(<span class="hljs-string">&quot;<span class="hljs-subst">\n</span> Load1_2: 加载weight_data数据（1728项，自动为float32类型）<span class="hljs-subst">\n</span>&quot;</span>);    <span class="hljs-type">float</span> *p_load1_2 = (<span class="hljs-type">float</span> *)p_load1_1;    for (int i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">1728</span>; i++) &#123;        if (i &lt; <span class="hljs-number">10</span> || i &gt; <span class="hljs-number">1720</span>) &#123;            printf(<span class="hljs-string">&quot;Weight %d : %.9f<span class="hljs-subst">\n</span>&quot;</span>, i, p_load1_2[i]);        &#125;    &#125;    p_load1_2 += <span class="hljs-number">1728</span>;        printf(<span class="hljs-string">&quot;<span class="hljs-subst">\n</span> Load3: 加载bias偏置数据（64项，固定为float32类型）<span class="hljs-subst">\n</span>&quot;</span>);    <span class="hljs-type">float</span> *p_load2 = (<span class="hljs-type">float</span> *)p_load1_2;    for (int i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">64</span>; i++) &#123;        if (i &lt; <span class="hljs-number">5</span> || i &gt; <span class="hljs-number">60</span>) &#123;            printf(<span class="hljs-string">&quot;Bias %d : %.9f<span class="hljs-subst">\n</span>&quot;</span>, i, p_load2[i]);        &#125;    &#125;    p_load2 += <span class="hljs-number">64</span>;        return p_load2;&#125;</code></pre><p><strong>Demo：</strong></p><p><a href="https://github.com/chrisYooh/ncnnSrcDemo">https://github.com/chrisYooh/ncnnSrcDemo</a></p><p>1）打开其下的 NcnnSrcDemo 工程</p><p>2）进入 ViewController，解除 自定义bin文件加载测试的 注释</p><p>/<em> 自定义 bin 文件加载测试 </em>/</p><p>[self loadModel_myAnalysis];</p><p>3）运行看看结果吧，也可以用 ncnn的loadModel 去跑，然后打断点看看解读的 .bin 文件数据一致不。</p><p><img src="/2020/09/21/ncnn%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B%E6%B5%85%E6%9E%90/6.png" alt></p><p>​    了解了bin文件的信息存储形式，我们当然就可以进行信息修改咯！不同的框架模型进行转化时，就要做这样的事情。哇，那我们可以 自己写转模型的工具啦！从技术上说，完全没错！</p><p><strong>2.2 Detect 检测</strong></p><p>   完成了 网络初始化 <strong>load_param()</strong>、 <strong>load_bin()</strong>之后，我们可以填写一个输入并使用 网络提取器Extractor 计算输出了。</p><p><strong>2.2.1 创建提取器 Extractor</strong></p><pre><code class="hljs reasonml"><span class="hljs-comment">/* Step2.1 : 构建并配置 提取器 */</span>ncnn::Extractor extractor = ncnn_net.create<span class="hljs-constructor">_extractor()</span>;extractor.set<span class="hljs-constructor">_light_mode(<span class="hljs-params">true</span>)</span>;</code></pre><p>   提取器 extractor 使用 目标网络 通过 友元函数 创建实例，因为它需要获取对应神经网络的信息；同时，extractor 还可以自定义部分配置信息。</p><p>Extractor含3个关键类变量</p><p>1）<strong>net： 指向对应网络的指针</strong></p><p>2）<strong>blob_mats： 计算的过程中存储输入、输出的临时数据</strong></p><p>3）<strong>opt： 配置参数</strong></p><p><strong>2.2.2 extractor.input 配置输入</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">/* Step2.2 : 设置输入（将图片转换成ncnn::Mat结构作为输入） */</span>UIImage *srcImage = [UIImage imageNamed:@<span class="hljs-string">&quot;mouth&quot;</span>];ncnn::Mat mat_src;ts_image2mat(mat_src, srcImage);extractor.input(<span class="hljs-string">&quot;data&quot;</span>, mat_src);</code></pre><p>1）我们要 构造一个ncnn::Mat的结构，将我们的输入填入其中</p><p>2）利用 Extractor的input()函数 将输入mat填入对应的位置。</p><p>注意：input() 函数中的第一个字符串参数输入的是blob的名称 而不是layer的名称 哦！</p><p><strong>2.2.3 extractor.extract 提取输出</strong></p><pre><code class="hljs less"><span class="hljs-comment">/* Step2.3 : 提取输出 */</span><span class="hljs-attribute">ncnn</span>::Mat mat_dst;<span class="hljs-selector-tag">extractor</span><span class="hljs-selector-class">.extract</span>(<span class="hljs-string">&quot;prob&quot;</span>, mat_dst);</code></pre><p>1）我们要 构造一个ncnn::Mat的结构，用以承载输出；</p><p>2）利用 Extractor的extract()函数 将计算结果填写到我们构造的输出mat中。</p><p>注意：extract()函数中的第一个字符串参数输入的是 blob的名称 而不是 layer的名称 哦！</p><p><strong>2.2.3.1 extract() 的递归流程图</strong></p><p>ncnn 在进行 extract() 的时候，使用了递归的方式，这边将其 宏观逻辑进行抽象。（描绘所有的代码细节会使图过于复杂，不易阅读）</p><p>递归实现.png</p><p><strong>2.2.3.2 extract() 最简递归展开流程</strong></p><p>之所以说最简，因为我们假设：</p><p>1 目标网络的每层都只有 一个输入(blob) 和 一个输出(blob)，</p><p>2 使用的extract是新创建的（即 无缓存数据）</p><p><img src="/2020/09/21/ncnn%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B%E6%B5%85%E6%9E%90/7.png" alt></p><p>如图：</p><p>1）每一层进行forward()的时候，需要一些输入参数，这些输入参数是 由上面的层的forward()运算输出的。</p><p>2）只有 输入层的输入参数是我们填写的（2.2.2 节），也正是因为它的存在，递归得以有了终结。</p><p>2.3 输出处理</p>  <pre><code class="hljs reasonml"><span class="hljs-comment">/* Step3.1 : 结果处理(获取检测概率最高的5种物品，认为存在) */</span>   NSArray *rstArray = ts<span class="hljs-constructor">_mat2array(<span class="hljs-params">mat_dst</span>)</span>;   NSArray *top5Array = ts<span class="hljs-constructor">_topN(<span class="hljs-params">rstArray</span>, 5)</span>;      <span class="hljs-comment">/* Step3.2 : 打印输出 */</span>   <span class="hljs-constructor">NSLog(@<span class="hljs-string">&quot;%@&quot;</span>, <span class="hljs-params">top5Array</span>)</span>;      <span class="hljs-comment">/* 说明：该Demo中发现输出的第一项是 index 为 673 的项目，</span><span class="hljs-comment">    * 在result_info.json中查找下 &quot;index&quot; : &quot;673&quot; 发现对应的描述是 鼠标</span><span class="hljs-comment">    * 也可以换其他图片进行检测，但要将图片规格化成 227 * 227 的大小才可以保证结果的准确性</span><span class="hljs-comment">    */</span></code></pre><p>​    输出处理是根据需求具体模型需求，很灵活的。比如我给予输出结果的每个数字以 概念（识别到某种物品的概率） ，并对输出结果进行排序后取其 概率最高的 五个值。</p><p><strong>2.4 封装</strong></p><p>​    玩一玩的话，12行代码足够了；但若真的要工程化的话，我们还是要将 面向过程 的思路 向 面向对象靠拢的。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_6</title>
    <link href="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-6/"/>
    <url>/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-6/</url>
    
    <content type="html"><![CDATA[<p>分析了几个比较常见的算子 forward 过程， 比如 abs， bias，argmax， conv，pool， bn </p><a id="more"></a><h4 id="1-abs"><a href="#1-abs" class="headerlink" title="1. abs"></a>1. abs</h4><pre><code class="hljs c"><span class="hljs-comment">// 绝对值层特性: 单输入，单输出，可直接对输入进行修改</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">AbsVal::forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> w = bottom_top_blob.w;    <span class="hljs-keyword">int</span> h = bottom_top_blob.h;    <span class="hljs-keyword">int</span> channels = bottom_top_blob.c;    <span class="hljs-keyword">int</span> size = w * h;    <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)  <span class="hljs-comment">// openmp </span></span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)    &#123;        <span class="hljs-keyword">float</span>* ptr = bottom_top_blob.channel(q);<span class="hljs-comment">// 当前通道数据的起始指针</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)        &#123;            <span class="hljs-keyword">if</span> (ptr[i] &lt; <span class="hljs-number">0</span>)                ptr[i] = - ptr[i]; <span class="hljs-comment">// 小于零取相反数，大于零保持原样</span>        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="2-bias"><a href="#2-bias" class="headerlink" title="2. bias"></a>2. bias</h4><pre><code class="hljs c"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Bias::forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> w = bottom_top_blob.w;    <span class="hljs-keyword">int</span> h = bottom_top_blob.h;    <span class="hljs-keyword">int</span> channels = bottom_top_blob.c;    <span class="hljs-keyword">int</span> size = w * h;    <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)    &#123;        <span class="hljs-keyword">float</span>* ptr = bottom_top_blob.channel(q);  <span class="hljs-comment">// 每个通道数据起始指针</span>        <span class="hljs-keyword">float</span> bias = bias_data[q];   <span class="hljs-comment">// 需要添加的偏置数据 前面从模型中载入的参数 每通道偏置参数一样</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)        &#123;            ptr[i] += bias;<span class="hljs-comment">// 加上偏置</span>        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="2-argmax"><a href="#2-argmax" class="headerlink" title="2.  argmax"></a>2.  argmax</h4><pre><code class="hljs c"><span class="hljs-comment">// 层参数包含两个参数，第一个是是否需要包含值对应在源blob中的位置，第二个是需要前多少个最大的数</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">ArgMax::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; pd)</span></span><span class="hljs-function"></span>&#123;    out_max_val = pd.get(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);<span class="hljs-comment">// 是否 需要存储位置</span>    topk = pd.get(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>);       <span class="hljs-comment">// 在前topk个最大的数</span>       <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">ArgMax::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> size = bottom_blob.total(); <span class="hljs-comment">// 输入blob参数总数量</span>        <span class="hljs-comment">// 创建一个新的输出 blob </span>    <span class="hljs-keyword">if</span> (out_max_val)        top_blob.create(topk, <span class="hljs-number">2</span>, <span class="hljs-number">4u</span>, opt.blob_allocator); <span class="hljs-comment">// topk个值 + topk个值对应的位置</span>    <span class="hljs-keyword">else</span>        top_blob.create(topk, <span class="hljs-number">1</span>, <span class="hljs-number">4u</span>, opt.blob_allocator); <span class="hljs-comment">// 只存  topk个值，不存位置</span>    <span class="hljs-keyword">if</span> (top_blob.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* ptr = bottom_blob;        <span class="hljs-comment">// partial sort topk with index, optional value</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt; <span class="hljs-built_in">std</span>::<span class="hljs-built_in">pair</span>&lt;<span class="hljs-keyword">float</span>, <span class="hljs-keyword">int</span>&gt; &gt; vec;    vec.resize(size);    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)    &#123;        vec[i] = <span class="hljs-built_in">std</span>::<span class="hljs-built_in">make_pair</span>(ptr[i], i);<span class="hljs-comment">// 源 输入blob 的参数的 值：位置id 键值对</span>    &#125;    <span class="hljs-built_in">std</span>::partial_sort(vec.begin(), vec.begin() + topk, vec.end(),                      <span class="hljs-built_in">std</span>::greater&lt; <span class="hljs-built_in">std</span>::<span class="hljs-built_in">pair</span>&lt;<span class="hljs-keyword">float</span>, <span class="hljs-keyword">int</span>&gt; &gt;());<span class="hljs-comment">// 按第一列排序，获取前 topk个</span>    <span class="hljs-comment">// 保存前面最大的 topk 个参数</span>    <span class="hljs-keyword">float</span>* outptr = top_blob;    <span class="hljs-keyword">if</span> (out_max_val)    &#123;        <span class="hljs-keyword">float</span>* valptr = outptr + topk; <span class="hljs-comment">// 前面topk的位置存值，后面存对应值在源输入 blob 中的位置ID</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;topk; i++)        &#123;            outptr[i] = vec[i].first; <span class="hljs-comment">// 存值</span>            valptr[i] = vec[i].second;<span class="hljs-comment">// 存位置</span>        &#125;    &#125;    <span class="hljs-keyword">else</span>    &#123;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;topk; i++)        &#123;            outptr[i] = vec[i].second;<span class="hljs-comment">// 只存值</span>        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="3-concat"><a href="#3-concat" class="headerlink" title="3.concat"></a>3.concat</h4><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Concat::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; bottom_blobs, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; top_blobs, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> dims = bottom_blobs[<span class="hljs-number">0</span>].dims;    <span class="hljs-keyword">size_t</span> elemsize = bottom_blobs[<span class="hljs-number">0</span>].elemsize;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">1</span>) <span class="hljs-comment">// axis == 0</span>    &#123;        <span class="hljs-keyword">int</span> top_w = <span class="hljs-number">0</span>; <span class="hljs-comment">// 输出长度</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> b = <span class="hljs-number">0</span>; b &lt; bottom_blobs.size(); b++)        &#123;            <span class="hljs-keyword">const</span> Mat&amp; bottom_blob = bottom_blobs[b];            top_w += bottom_blob.w;        &#125;        <span class="hljs-comment">// 创建输出 blob</span>        Mat&amp; top_blob = top_blobs[<span class="hljs-number">0</span>];        top_blob.create(top_w, elemsize, opt.blob_allocator);        <span class="hljs-keyword">if</span> (top_blob.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;        <span class="hljs-comment">// 将不同的输入复制到输出</span>        <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>* outptr = top_blob;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> b = <span class="hljs-number">0</span>; b &lt; bottom_blobs.size(); b++)        &#123;            <span class="hljs-keyword">const</span> Mat&amp; bottom_blob = bottom_blobs[b];            <span class="hljs-keyword">int</span> w = bottom_blob.w;            <span class="hljs-keyword">const</span> <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>* ptr = bottom_blob;            <span class="hljs-built_in">memcpy</span>(outptr, ptr, w * elemsize);            outptr += w * elemsize;        &#125;        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">2</span> &amp;&amp; axis == <span class="hljs-number">0</span>)&#123;        <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">2</span> &amp;&amp; axis == <span class="hljs-number">1</span>)&#123;        <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">3</span> &amp;&amp; axis == <span class="hljs-number">0</span>)&#123;        <span class="hljs-comment">//  ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">3</span> &amp;&amp; axis == <span class="hljs-number">1</span>)&#123;      <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">3</span> &amp;&amp; axis == <span class="hljs-number">2</span>)&#123;       <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="4-卷积层"><a href="#4-卷积层" class="headerlink" title="4. 卷积层"></a>4. 卷积层</h4><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Convolution::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// convolv with NxN kernel</span>    <span class="hljs-comment">// value = value + bias</span>    <span class="hljs-keyword">if</span> (opt.use_int8_inference &amp;&amp; weight_data.elemsize == (<span class="hljs-keyword">size_t</span>)<span class="hljs-number">1u</span>)    &#123;        <span class="hljs-keyword">return</span> forward_int8(bottom_blob, top_blob, opt);    &#125;    <span class="hljs-comment">// flattened blob, implement as InnerProduct</span>    <span class="hljs-comment">//...</span>    <span class="hljs-keyword">int</span> w = bottom_blob.w;    <span class="hljs-keyword">int</span> h = bottom_blob.h;    <span class="hljs-keyword">int</span> channels = bottom_blob.c;    <span class="hljs-keyword">size_t</span> elemsize = bottom_blob.elemsize;    <span class="hljs-comment">// NCNN_LOGE(&quot;Convolution input %d x %d  pad = %d %d  ksize=%d %d  stride=%d %d&quot;, w, h, pad_w, pad_h, kernel_w, kernel_h, stride_w, stride_h);</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> kernel_extent_w = dilation_w * (kernel_w - <span class="hljs-number">1</span>) + <span class="hljs-number">1</span>;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> kernel_extent_h = dilation_h * (kernel_h - <span class="hljs-number">1</span>) + <span class="hljs-number">1</span>;    Mat bottom_blob_bordered;    make_padding(bottom_blob, bottom_blob_bordered, opt);    <span class="hljs-keyword">if</span> (bottom_blob_bordered.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    w = bottom_blob_bordered.w;    h = bottom_blob_bordered.h;    <span class="hljs-keyword">int</span> outw = (w - kernel_extent_w) / stride_w + <span class="hljs-number">1</span>;    <span class="hljs-keyword">int</span> outh = (h - kernel_extent_h) / stride_h + <span class="hljs-number">1</span>;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> maxk = kernel_w * kernel_h;    <span class="hljs-comment">// kernel offsets</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; _space_ofs(maxk);    <span class="hljs-keyword">int</span>* space_ofs = &amp;_space_ofs[<span class="hljs-number">0</span>];    &#123;        <span class="hljs-keyword">int</span> p1 = <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> p2 = <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> gap = w * dilation_h - kernel_w * dilation_w;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; kernel_h; i++)        &#123;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; kernel_w; j++)            &#123;                space_ofs[p1] = p2;                p1++;                p2 += dilation_w;            &#125;            p2 += gap;        &#125;    &#125;    <span class="hljs-comment">// 申请输出</span>    top_blob.create(outw, outh, num_output, elemsize, opt.blob_allocator);    <span class="hljs-keyword">if</span> (top_blob.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    <span class="hljs-comment">// num_output</span>    <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> p = <span class="hljs-number">0</span>; p &lt; num_output; p++) <span class="hljs-comment">// 逐输出通道</span>    &#123;        <span class="hljs-keyword">float</span>* outptr = top_blob.channel(p);        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; outh; i++) <span class="hljs-comment">// 输出高度</span>        &#123;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; outw; j++) <span class="hljs-comment">// 输出宽度</span>            &#123;                <span class="hljs-keyword">float</span> sum = <span class="hljs-number">0.f</span>;                <span class="hljs-keyword">if</span> (bias_term)                    sum = bias_data[p];                <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* kptr = (<span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>*)weight_data + maxk * channels * p; <span class="hljs-comment">// 卷积单元对应的起始位置</span>                <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q = <span class="hljs-number">0</span>; q &lt; channels; q++) <span class="hljs-comment">// 输入</span>                &#123;                    <span class="hljs-keyword">const</span> Mat m = bottom_blob_bordered.channel(q);                    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* sptr = m.row(i * stride_h) + j * stride_w;                    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> k = <span class="hljs-number">0</span>; k &lt; maxk; k++) <span class="hljs-comment">// 29.23</span>                    &#123;                        <span class="hljs-keyword">float</span> val = sptr[space_ofs[k]]; <span class="hljs-comment">// 20.72</span>                        <span class="hljs-keyword">float</span> w = kptr[k];                        sum += val * w; <span class="hljs-comment">// 41.45</span>                    &#125;                    kptr += maxk;                &#125;                                <span class="hljs-comment">// 激活函数</span>                <span class="hljs-comment">// ......</span>                outptr[j] = sum;            &#125;            outptr += outw;        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="5-pooling"><a href="#5-pooling" class="headerlink" title="5. pooling"></a>5. pooling</h4><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Pooling::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// max value in NxN window</span>    <span class="hljs-keyword">int</span> w = bottom_blob.w;    <span class="hljs-keyword">int</span> h = bottom_blob.h;    <span class="hljs-keyword">int</span> channels = bottom_blob.c;    <span class="hljs-keyword">size_t</span> elemsize = bottom_blob.elemsize;    <span class="hljs-keyword">if</span> (global_pooling)    &#123;        <span class="hljs-comment">// ...</span>    &#125;    Mat bottom_blob_bordered;    make_padding(bottom_blob, bottom_blob_bordered, opt);    <span class="hljs-keyword">if</span> (bottom_blob_bordered.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    w = bottom_blob_bordered.w;    h = bottom_blob_bordered.h;    <span class="hljs-keyword">int</span> outw = (w - kernel_w) / stride_w + <span class="hljs-number">1</span>;    <span class="hljs-keyword">int</span> outh = (h - kernel_h) / stride_h + <span class="hljs-number">1</span>;、        <span class="hljs-comment">// 申请输出 blob</span>    top_blob.create(outw, outh, channels, elemsize, opt.blob_allocator);    <span class="hljs-keyword">if</span> (top_blob.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> maxk = kernel_w * kernel_h;    <span class="hljs-comment">// kernel offsets</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; _space_ofs(maxk); <span class="hljs-comment">// </span>    <span class="hljs-keyword">int</span>* space_ofs = &amp;_space_ofs[<span class="hljs-number">0</span>];    &#123;        <span class="hljs-keyword">int</span> p1 = <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> p2 = <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> gap = w - kernel_w;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; kernel_h; i++)        &#123;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; kernel_w; j++)            &#123;                space_ofs[p1] = p2;                p1++;                p2++;            &#125;            p2 += gap;        &#125;    &#125;    <span class="hljs-keyword">if</span> (pooling_type == PoolMethod_MAX)    &#123;        <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q = <span class="hljs-number">0</span>; q &lt; channels; q++) <span class="hljs-comment">// 通道</span>        &#123;            <span class="hljs-keyword">const</span> Mat m = bottom_blob_bordered.channel(q);            <span class="hljs-keyword">float</span>* outptr = top_blob.channel(q);            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; outh; i++) <span class="hljs-comment">// 输出层高度</span>            &#123;                <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; outw; j++) <span class="hljs-comment">// 输出层宽度</span>                &#123;                    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* sptr = m.row(i * stride_h) + j * stride_w; <span class="hljs-comment">// 池化单元对应的起始位置</span>                    <span class="hljs-keyword">float</span> max = sptr[<span class="hljs-number">0</span>];                    <span class="hljs-comment">// 求每个池化单元对应的最大值</span>                    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> k = <span class="hljs-number">0</span>; k &lt; maxk; k++)                    &#123;                        <span class="hljs-keyword">float</span> val = sptr[space_ofs[k]];                        max = <span class="hljs-built_in">std</span>::max(max, val);                    &#125;                    outptr[j] = max;                &#125;                outptr += outw;            &#125;        &#125;    &#125;    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (pooling_type == PoolMethod_AVE)    &#123;        <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="6-Batchnorm"><a href="#6-Batchnorm" class="headerlink" title="6. Batchnorm"></a>6. Batchnorm</h4><pre><code class="hljs c"><span class="hljs-comment">// 可以看出来， 这里的其实就是 value = b * value + a, 完全可以和 conv 进行合并</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">BatchNorm::forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// a = bias - slope * mean / sqrt(var)</span>    <span class="hljs-comment">// b = slope / sqrt(var)</span>    <span class="hljs-comment">// value = b * value + a</span>    <span class="hljs-keyword">int</span> dims = bottom_top_blob.dims;        <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">1</span>) <span class="hljs-comment">// 1维度====================</span>    &#123;        <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">2</span>) <span class="hljs-comment">// 2维度======================</span>    &#123;        <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">3</span>) <span class="hljs-comment">// 3维度================================</span>    &#123;        <span class="hljs-keyword">int</span> w = bottom_top_blob.w;        <span class="hljs-keyword">int</span> h = bottom_top_blob.h;        <span class="hljs-keyword">int</span> size = w * h;        <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)        &#123;            <span class="hljs-keyword">float</span>* ptr = bottom_top_blob.channel(q);            <span class="hljs-keyword">float</span> a = a_data[q];            <span class="hljs-keyword">float</span> b = b_data[q];            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)            &#123;                ptr[i] = b * ptr[i] + a;            &#125;        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_5</title>
    <link href="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-5/"/>
    <url>/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-5/</url>
    
    <content type="html"><![CDATA[<a id="more"></a><p>ncnn 提供了两种解析网络 layer 的方法：</p><h4 id="方法一-在推理中进行解析"><a href="#方法一-在推理中进行解析" class="headerlink" title="方法一: 在推理中进行解析"></a>方法一: 在推理中进行解析</h4><p>模型转换之后，某些 layer 没有转换成功：(在转换的过程中)。 如 shufflenetv2 中，param 文件只转换到了 fc 这一个 layer。这时需要添加一层softmax，可以先对param文件中转换成功的layer做推理，然后手动添加一个softmax层：</p><pre><code class="hljs cpp">ncnn::Extractor ex = shufflenetv2.create_extractor();ex.input(<span class="hljs-string">&quot;data&quot;</span>, in);ncnn::Mat out;ex.extract(<span class="hljs-string">&quot;fc&quot;</span>, out);<span class="hljs-comment">// manually call softmax on the fc output</span><span class="hljs-comment">// convert result into probability</span><span class="hljs-comment">// skip if your model already has softmax operation</span>&#123;    ncnn::Layer* softmax = ncnn::create_layer(<span class="hljs-string">&quot;Softmax&quot;</span>);    ncnn::ParamDict pd;    softmax-&gt;load_param(pd);    softmax-&gt;forward_inplace(out, shufflenetv2.opt);    <span class="hljs-keyword">delete</span> softmax;&#125;out = out.reshape(out.w * out.h * out.c);</code></pre><h4 id="方法二-自定义层"><a href="#方法二-自定义层" class="headerlink" title="方法二: 自定义层"></a>方法二: 自定义层</h4><h5 id="1-新建空的类"><a href="#1-新建空的类" class="headerlink" title="1. 新建空的类"></a>1. 新建空的类</h5><pre><code class="hljs c"><span class="hljs-comment">// 在 ncnn/src/layer/ 下新建两个文件 mylayer.h mylayer.cpp</span><span class="hljs-comment">// 1.mylayer.h</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer.h&quot;</span></span><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> ncnn;      <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyLayer</span> :</span> <span class="hljs-keyword">public</span> Layer&#123;&#125;;<span class="hljs-comment">// 2. mylayer.cpp</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;mylayer.h&quot;</span></span>DEFINE_LAYER_CREATOR(MyLayer)   <span class="hljs-comment">// 注册新定义的层</span></code></pre><h5 id="2-定义层参数-parameters-和-权重-weights-并实现载入函数"><a href="#2-定义层参数-parameters-和-权重-weights-并实现载入函数" class="headerlink" title="2. 定义层参数 parameters 和 权重 weights, 并实现载入函数"></a>2. 定义层参数 parameters 和 权重 weights, 并实现载入函数</h5><pre><code class="hljs c"><span class="hljs-comment">// mylayer.h</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer.h&quot;</span></span><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> ncnn;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyLayer</span> :</span> <span class="hljs-keyword">public</span> Layer&#123;<span class="hljs-keyword">public</span>:  <span class="hljs-comment">// 公有方法</span>      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDic&amp; pd)</span></span>;<span class="hljs-comment">// 虚函数，可以被子类覆盖</span>      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span>;   <span class="hljs-comment">// </span><span class="hljs-keyword">private</span>: <span class="hljs-comment">// 私有参数</span>      <span class="hljs-keyword">int</span> channels;   <span class="hljs-comment">// 参数1 通道数量</span>      <span class="hljs-keyword">float</span> eps;      <span class="hljs-comment">// 参数2 精度</span>      Mat gamma_data; <span class="hljs-comment">// 权重</span>&#125;；<span class="hljs-comment">// 2. mylayer.cpp</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;mylayer.h&quot;</span></span>DEFINE_LAYER_CREATOR(MyLayer) <span class="hljs-comment">// 实现 load_param() 载入网络层参数</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; pd)</span></span><span class="hljs-function"></span>&#123;      <span class="hljs-comment">// 使用pd.get(key,default_val); 从param文件中（key=val）获取参数</span>      channels = pd.get(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);      <span class="hljs-comment">// 解析 0=&lt;int value&gt;, 默认为0</span>      eps      = pd.get(<span class="hljs-number">1</span>, <span class="hljs-number">0.001f</span>); <span class="hljs-comment">// 解析 1=&lt;float value&gt;, 默认为0.001f</span>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>; <span class="hljs-comment">// 载入成功返回0</span>&#125;<span class="hljs-comment">// 实现 load_model() 载入模型权重</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span><span class="hljs-function"></span>&#123;      <span class="hljs-comment">// 读取二进制数据的长度为 channels * sizeof(float)</span>      <span class="hljs-comment">// 0 自动判断数据类型， float32 float16 int8</span>      <span class="hljs-comment">// 1 按 float32读取  2 按float16读取 3 按int8读取</span>      gamma_data = mb.load(channels, <span class="hljs-number">1</span>);<span class="hljs-comment">// 按 float32读取 </span>      <span class="hljs-keyword">if</span>(gamma_data.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>; <span class="hljs-comment">// 错误返回非0数，-100表示 out-of-memory </span>      <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>; <span class="hljs-comment">//  载入成功返回0</span>&#125;</code></pre><h5 id="3-定义类构造函数，确定前向传播行为"><a href="#3-定义类构造函数，确定前向传播行为" class="headerlink" title="3.  定义类构造函数，确定前向传播行为"></a>3.  定义类构造函数，确定前向传播行为</h5><pre><code class="hljs c"><span class="hljs-comment">// mylayer.h</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer.h&quot;</span></span><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> ncnn;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyLayer</span> :</span> <span class="hljs-keyword">public</span> Layer&#123;<span class="hljs-keyword">public</span>:        MyLayer();  <span class="hljs-comment">// 构造函数</span>      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDic&amp; pd)</span></span>;      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span>;  <span class="hljs-keyword">private</span>:      <span class="hljs-keyword">int</span> channels;         <span class="hljs-keyword">float</span> eps;       Mat gamma_data; &#125;；<span class="hljs-comment">// mylayer.cpp</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;mylayer.h&quot;</span></span>DEFINE_LAYER_CREATOR(MyLayer) <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; pd)</span></span><span class="hljs-function"></span>&#123;      channels = pd.get(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);           eps      = pd.get(<span class="hljs-number">1</span>, <span class="hljs-number">0.001f</span>);            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span><span class="hljs-function"></span>&#123;      gamma_data = mb.load(channels, <span class="hljs-number">1</span>);      <span class="hljs-keyword">if</span>(gamma_data.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;      <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;<span class="hljs-comment">// 构造函数</span>MyLayer::MyLayer()&#123;      <span class="hljs-comment">// 是否为 1输入1输出层</span>      <span class="hljs-comment">// 1输入1输出层： Convolution, Pooling, ReLU, Softmax ...</span>      <span class="hljs-comment">// 反例       ： Eltwise, Split, Concat, Slice ...</span>      one_blob_only = <span class="hljs-literal">true</span>;      <span class="hljs-comment">// 是否可以在 输入blob 上直接修改 后输出</span>      <span class="hljs-comment">// 支持在原位置上修改： Relu、BN、scale、Sigmod...</span>      <span class="hljs-comment">// 不支持： Convolution、Pooling ...</span>      support_inplace = <span class="hljs-literal">true</span>;&#125;</code></pre><h5 id="4-选择合适的-forward-函数接口，-并实现对应的-forward-函数"><a href="#4-选择合适的-forward-函数接口，-并实现对应的-forward-函数" class="headerlink" title="4.  选择合适的 forward()函数接口， 并实现对应的 forward 函数"></a>4.  选择合适的 forward()函数接口， 并实现对应的 forward 函数</h5><p>Layer类定义了四种 forward()函数：</p><ul><li><p>多输入多输出，const 不可直接对输入进行修改</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; bottom_blobs, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; top_blobs, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;</code></pre></li><li><p>单输入单输出，const 不可直接对输入进行修改</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;</code></pre></li><li><p>多输入多输出，可直接对输入进行修改</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward_inplace</span><span class="hljs-params">(<span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; bottom_top_blobs, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;</code></pre></li><li><p>单输入单输出，可直接对输入进行修改</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;</code></pre></li></ul><p>具体实现:</p><pre><code class="hljs c"><span class="hljs-comment">// mylayer.h</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer.h&quot;</span></span><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> ncnn;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyLayer</span> :</span> <span class="hljs-keyword">public</span> Layer&#123;<span class="hljs-keyword">public</span>:      MyLayer();      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDic&amp; pd)</span></span>;      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span>;            <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>; <span class="hljs-comment">// 单输入单输出</span>      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;          <span class="hljs-comment">// 单入单出本地修改 </span><span class="hljs-keyword">private</span>:       <span class="hljs-keyword">int</span> channels;        <span class="hljs-keyword">float</span> eps;       Mat gamma_data;&#125;；<span class="hljs-comment">// mylayer.cpp </span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;mylayer.h&quot;</span></span>DEFINE_LAYER_CREATOR(MyLayer)MyLayer::MyLayer()&#123;      one_blob_only = <span class="hljs-literal">true</span>;      support_inplace = <span class="hljs-literal">true</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; pd)</span></span><span class="hljs-function"></span>&#123;      channels = pd.get(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);      eps      = pd.get(<span class="hljs-number">1</span>, <span class="hljs-number">0.001f</span>);             <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span><span class="hljs-function"></span>&#123;      gamma_data = mb.load(channels, <span class="hljs-number">1</span>);      <span class="hljs-keyword">if</span>(gamma_data.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;      <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;<span class="hljs-comment">// 单入单出 前向传播网络 不可修改 非const</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;      <span class="hljs-keyword">if</span>(bottom_blob.c != channels)            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;            <span class="hljs-comment">// 实现运算 x = (x + eps) * gamma_per_channel</span>      <span class="hljs-keyword">int</span> w = bottom_blob.w;      <span class="hljs-keyword">int</span> h = bottom_blob.h;      <span class="hljs-keyword">size_t</span> elemsize = bottom_blob.elemsize;      <span class="hljs-keyword">int</span> size = w * h;            <span class="hljs-comment">// 输出需要新建，不可直接在输入blob上修改</span>      top_blob.create(w, h, channels, elemsize, opt.blob_allocator);      <span class="hljs-keyword">if</span>(top_blob.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;            <span class="hljs-meta">#pragam omp parallel for num_threads(opt.num_threads);</span>      <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)      &#123;            <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* in_ptr = bottom_blob.channel(q);            <span class="hljs-keyword">float</span>* out_ptr = top_blob.channel(q);            <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> gamma = gamma_data[q];                        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)            &#123;                  out_ptr[i] = (in_ptr[i] + eps)*gamma;            &#125;            &#125;            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>；&#125;<span class="hljs-comment">// 单入单出 前向传播网络  可在输入blob上修改</span><span class="hljs-keyword">int</span> MyLayer::forward_inplace(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt) <span class="hljs-keyword">const</span>&#123;      <span class="hljs-keyword">if</span>(bottom_blob.c != channels)            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;             <span class="hljs-comment">// 实现运算 x = (x + eps) * gamma_per_channel</span>      <span class="hljs-keyword">int</span> w = bottom_blob.w;      <span class="hljs-keyword">int</span> h = bottom_blob.h;      <span class="hljs-keyword">int</span> size = w * h;            <span class="hljs-comment">// 输出不需要新建，可直接在输入blob上修改</span>      <span class="hljs-meta">#pragam omp parallel for num_threads(opt.num_threads);</span>      <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)      &#123;            <span class="hljs-keyword">float</span>* in_out_ptr = bottom_top_blob.channel(q);            <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> gamma = gamma_data[q];            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)            &#123;                  in_out_ptr[i] = (in_out_ptr[i] + eps)*gamma;            &#125;            &#125;            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>；&#125;</code></pre><h5 id="5-集成进-ncnn-库"><a href="#5-集成进-ncnn-库" class="headerlink" title="5. 集成进 ncnn 库"></a>5. 集成进 ncnn 库</h5><pre><code class="hljs c"><span class="hljs-comment">// 层类型       层名称   输入数量  输出数量   输入层   输出层</span><span class="hljs-comment">// MyLayer     mylayer    1       1     conv2d   mylayer0  0=32 1=0.2 // 对应 param</span><span class="hljs-comment">// 层类型: 和对应的注册的名名称一致</span><span class="hljs-comment">// 注册新层</span>ncnn::Net net;net.register_custom_layer(<span class="hljs-string">&quot;MyLayer&quot;</span>, MyLayer_layer_creator); <span class="hljs-comment">// 注册新层</span>net.load_param(<span class="hljs-string">&quot;model.param&quot;</span>);net.load_model(<span class="hljs-string">&quot;model.bin&quot;</span>);</code></pre><p>​      </p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_4</title>
    <link href="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/"/>
    <url>/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/</url>
    
    <content type="html"><![CDATA[<p>ncnn源码分析-4 模型量化源码</p><a id="more"></a><p>ncnn量化分析代码对应的文件主要有两个:ncnn/tools/quantize/<strong>ncnn2table.cpp</strong> 和 ncnn/tools/quantize/<strong>ncnn2int8.cpp</strong>。</p><ul><li><strong>ncnn2table.cpp</strong> <strong>主要是分析生成相应的量化表， 其中存储了每个层的 scale 值。</strong></li><li><strong>ncnn2int8.cpp</strong> <strong>则是对网络进行 int8量化</strong></li></ul><h4 id="1-ncnn2int8"><a href="#1-ncnn2int8" class="headerlink" title="1. ncnn2int8"></a>1. ncnn2int8</h4><p>​    我们首先对 ncnn2int8.cpp 这个文件进行分析。 该文件完成的主要功能是将 float32 类型转化为 int8 类型。主要分为四个步骤:</p><p>(1) 读取生成的 table 文件，里面存储了对应的 scale。这本质上就是一个文本文件的读取和解析。</p><p>​    read<em>int8_scale_table 负责读取 table 文件, 将 scale 分别存储在 weight_int8scale_table (带 _param\</em>) 和 blob<em>int8scale_table(不带_param\</em>)。 这里只是一个读取文件并解析的过程。</p><p><strong>weight</strong>( 带_param_)格式示例： scale 个数和通道数相同，是主通道进行量化</p><p><img src="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/1.jpg" alt></p><p><strong>blob</strong> (不带_param_) 格式示例：</p><p><img src="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/2.jpg" alt></p><p>(2) <strong>载入原始的参数和模型文件。</strong>net 中的 .param 和 .bin 载入方式<strong>。</strong></p><p>(3) <strong>对卷积层、深度可分离卷积层和全连接层进行量化。quantize_convolution、quantize_convolutiondepthwise、quantize_innerproduct</strong> 分别对卷积层、可分离卷积层和全连接层进行量化<strong>。 这里其实是构建了一个量化层，然后将原始的fp32类型的权重和scale作为输入, 进行一次forward运算，将结果替换原来的fp32权重</strong></p><p>(4) <strong>保存量化后的权重文件</strong></p><p><img src="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/3.jpg" alt></p><p><strong>从主函数入手</strong>: 分别读取了5个参数, <strong>分别是输入模型文件、参数文件、输出模型文件、输出参数文件和量化表的路径</strong>。 然后执行如上所示的四个步骤。</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">int</span> argc, <span class="hljs-keyword">char</span>** argv)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// 读取相应的参数 </span>    <span class="hljs-keyword">if</span> (argc != <span class="hljs-number">6</span>)    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;usage: %s [inparam] [inbin] [outparam] [outbin] [calibration table]\n&quot;</span>, argv[<span class="hljs-number">0</span>]);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* inparam = argv[<span class="hljs-number">1</span>];  <span class="hljs-comment">// 输入模型文件</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* inbin = argv[<span class="hljs-number">2</span>];  <span class="hljs-comment">// 输入参数文件</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* outparam = argv[<span class="hljs-number">3</span>];  <span class="hljs-comment">// 输出模型文件</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* outbin = argv[<span class="hljs-number">4</span>];  <span class="hljs-comment">// 输出参数文件</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* int8scale_table_path = argv[<span class="hljs-number">5</span>]; <span class="hljs-comment">// 量化表的路径</span>    NetQuantize quantizer; <span class="hljs-comment">// 主要的量化类</span>    <span class="hljs-comment">// (1)  读取并解析 scale table 文件</span>    <span class="hljs-keyword">if</span> (int8scale_table_path)    &#123;        <span class="hljs-keyword">bool</span> s2 = read_int8scale_table(int8scale_table_path, quantizer.blob_int8scale_table, quantizer.weight_int8scale_table);        <span class="hljs-keyword">if</span> (!s2)         &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;read_int8scale_table failed\n&quot;</span>);            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;    &#125;    <span class="hljs-comment">// (2) 载入模型文件和参数</span>    quantizer.load_param(inparam);     quantizer.load_model(inbin);    <span class="hljs-comment">// (3) 量化: 主要量化三层: conv、convdw 和 fc</span>    quantizer.quantize_convolution();    quantizer.quantize_convolutiondepthwise();    quantizer.quantize_innerproduct();    <span class="hljs-comment">// (4) 存储模型和参数文件</span>    quantizer.save(outparam, outbin);    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>​    <strong>代码的最核心的三个函数:</strong> <strong>quantize_convolution、quantize_convolutiondepthwise、quantize_innerproduct</strong> 分别对卷积层、可分离卷积层和全连接层进行量化, </p><p>我们在此以 <strong>quantize_convolution</strong> 为例:</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">NetQuantize::quantize_convolution</span><span class="hljs-params">()</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> layer_count = <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">int</span>&gt;(layers.size());    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; layer_count; i++)    &#123;        <span class="hljs-comment">// 查找所有的卷积层</span>        <span class="hljs-keyword">if</span> (layers[i]-&gt;type != <span class="hljs-string">&quot;Convolution&quot;</span>)            <span class="hljs-keyword">continue</span>;                         <span class="hljs-comment">// 获取卷积层的名称           </span>        <span class="hljs-keyword">char</span> key[<span class="hljs-number">256</span>];        <span class="hljs-built_in">sprintf</span>(key, <span class="hljs-string">&quot;%s_param_0&quot;</span>, layers[i]-&gt;name.c_str());                            <span class="hljs-comment">// 在 blob_int8scale_table 找到该层  /* 其实这里的 blob_int8scale_table 下文并没有用到 */</span>        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">map</span>&lt;<span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span>, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; &gt;::iterator iter_data = blob_int8scale_table.find(layers[i]-&gt;name);        <span class="hljs-keyword">if</span> (iter_data == blob_int8scale_table.end())            <span class="hljs-keyword">continue</span>;        <span class="hljs-comment">// 在 weight_int8scale_table 找到该层</span>        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">map</span>&lt;<span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span>, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; &gt;::iterator iter = weight_int8scale_table.find(key);        <span class="hljs-keyword">if</span> (iter == weight_int8scale_table.end())        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;this layer need to be quantized, but no scale param!\n&quot;</span>);            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;                <span class="hljs-comment">// 卷积层量化 -&gt;  fp32 到 int8</span>        ncnn::Convolution* convolution = (ncnn::Convolution*)layers[i]; <span class="hljs-comment">// (1) 获取该卷积层</span>        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;quantize_convolution %s\n&quot;</span>, convolution-&gt;name.c_str());        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; weight_data_int8_scales = iter-&gt;second; <span class="hljs-comment">//  (2) 获取weight_data_int8_scales</span>        &#123;            <span class="hljs-function">ncnn::Mat <span class="hljs-title">int8_weight_data</span><span class="hljs-params">(convolution-&gt;weight_data_size, (<span class="hljs-keyword">size_t</span>)<span class="hljs-number">1u</span>)</span></span>; <span class="hljs-comment">// (3) 结果，和weight的大小一致</span>            <span class="hljs-keyword">if</span> (int8_weight_data.empty())                <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;            <span class="hljs-comment">// 这里所谓的量化，即进行了一次简单的前向传播，将原来的float32类型的权重替换为int类型结果</span>            <span class="hljs-comment">// 在此之前我们准备的东西有</span>            <span class="hljs-comment">// (1) 卷积层的 fp32 的数据 (2) scale 数据 (3) 声明了一个 int8_weight_data的数据</span>            <span class="hljs-comment">// 我们的目标就是 (1) + (2) -&gt; (3)</span>            <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> weight_data_size_output = convolution-&gt;weight_data_size / convolution-&gt;num_output;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> n = <span class="hljs-number">0</span>; n &lt; convolution-&gt;num_output; n++)   <span class="hljs-comment">// 逐卷积核进行量化</span>            &#123;                <span class="hljs-comment">// (4) 创建一个quantize op</span>                ncnn::Layer* op = ncnn::create_layer(ncnn::LayerType::Quantize);                 <span class="hljs-comment">// (5) 把量化表中的scale设置进op里去</span>                ncnn::ParamDict pd;                pd.<span class="hljs-built_in">set</span>(<span class="hljs-number">0</span>, weight_data_int8_scales[n]);                op-&gt;load_param(pd);                <span class="hljs-comment">// (6) blob_allocator</span>                ncnn::Option opt;                opt.blob_allocator = int8_weight_data.allocator;                <span class="hljs-comment">// (7) weight_data &lt;-&gt; weight_data_n , int8_weight_data &lt;-&gt;  int8_weight_data_n </span>                <span class="hljs-keyword">const</span> ncnn::Mat weight_data_n = convolution-&gt;weight_data.range(weight_data_size_output * n, weight_data_size_output);                ncnn::Mat int8_weight_data_n = int8_weight_data.range(weight_data_size_output * n, weight_data_size_output);                <span class="hljs-comment">// (8) quantitze op前传，计算量化权值 weight_data_n -&gt;  int8_weight_data_n</span>                op-&gt;forward(weight_data_n, int8_weight_data_n, opt);                                 <span class="hljs-keyword">delete</span> op;            &#125;            convolution-&gt;weight_data = int8_weight_data; <span class="hljs-comment">// (9) 用量化后的权值替换原来的权值</span>        &#125;        convolution-&gt;int8_scale_term = <span class="hljs-number">2</span>;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>可以来简单的看一下 <strong>quantize</strong> 层:</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">inline</span> <span class="hljs-keyword">signed</span> <span class="hljs-keyword">char</span> <span class="hljs-title">float2int8</span><span class="hljs-params">(<span class="hljs-keyword">float</span> v)</span></span>&#123;    <span class="hljs-keyword">int</span> int32 = <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">int</span>&gt;(round(v)); <span class="hljs-comment">// 取整数, 然后转化为 int32类型</span>    <span class="hljs-keyword">if</span> (int32 &gt; <span class="hljs-number">127</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">127</span>;  <span class="hljs-comment">// 如果大于127, 返回127</span>    <span class="hljs-keyword">if</span> (int32 &lt; <span class="hljs-number">-127</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">-127</span>; <span class="hljs-comment">// 如果小于 -127, 返回 -127</span>    <span class="hljs-keyword">return</span> (<span class="hljs-keyword">signed</span> <span class="hljs-keyword">char</span>)int32; <span class="hljs-comment">// 返回 int32 -&gt; int8</span>&#125;<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Quantize::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>&#123;    <span class="hljs-keyword">int</span> dims = bottom_blob.dims;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">1</span>)&#123;        <span class="hljs-keyword">int</span> w = bottom_blob.w;        top_blob.create(w, (<span class="hljs-keyword">size_t</span>)<span class="hljs-number">1u</span>, opt.blob_allocator);        <span class="hljs-keyword">if</span> (top_blob.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;        <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* ptr = bottom_blob;        <span class="hljs-keyword">signed</span> <span class="hljs-keyword">char</span>* outptr = top_blob;        <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;w; i++)        &#123;            <span class="hljs-comment">// ! 这一句是最核心的，也是整个量化部分代码的核心</span>            <span class="hljs-comment">// 将 float32 乘以 scale， 然后将其转化为 int8 类型</span>            outptr[i] = float2int8(ptr[i] * scale);         &#125;    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">2</span>)&#123;        ...    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">3</span>)&#123;        ...    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>2. ncnn2table.cpp</strong> </p><p>ncnn2table.cpp 主要用于<strong>量化表的计算</strong>，说白了就是使用我们之前说的算法来计算各个参数的 scale，在ncnn2table.cpp下，顶层代码核心就一句话：</p><pre><code class="hljs cpp"><span class="hljs-comment">/* ncnn2table.cpp */</span><span class="hljs-comment">// filenames: 用来calibration的图片list </span><span class="hljs-comment">// parampath: 参数文件路径</span><span class="hljs-comment">// binpath: bin 二进制文件路径</span><span class="hljs-comment">// tablepath: 生成的量化表的路径</span><span class="hljs-comment">// pre_param: 参数</span>post_training_quantize(filenames, parampath, binpath, tablepath, pre_param);</code></pre><p>我们接下来重点看post_training_quantize这个函数，该函数做了如下几件事：</p><p><strong>&gt;&gt;&gt;&gt;  (1) 初始化quantitize_datas    (2) 计算最大值    (3) 初始化直方图的间隔      (4) 计算直方图           (5) 计算Scale</strong></p><p><strong>(1) 初始化quantitize_datas</strong></p><p>​    没什么好说的，每一个层有一个<strong>QuantizeData</strong>对象，初始化 num_bins=2048，也就是原始的fp32分布Po，其统计直方图一共有<strong>2048个bins</strong></p><pre><code class="hljs cpp"><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;QuantizeData&gt; quantize_datas;<span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; net.conv_names.size(); i++)&#123;    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> layer_name = net.conv_names[i];    <span class="hljs-function">QuantizeData <span class="hljs-title">quantize_data</span><span class="hljs-params">(layer_name, <span class="hljs-number">2048</span>)</span></span>;    quantize_datas.push_back(quantize_data);&#125;</code></pre><p><strong>(2) 计算最大值</strong></p><p>​    遍历所有图片，计算每个blob的最大激活值，这里找的是绝对值最大的那个</p><pre><code class="hljs cpp">    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; image_list.size(); i++) <span class="hljs-comment">// 遍历calibration数据</span>    &#123;        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> img_name = image_list[i];        <span class="hljs-keyword">if</span> ((i + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>)        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;    %d/%d\n&quot;</span>, <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">int</span>&gt;(i + <span class="hljs-number">1</span>), <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">int</span>&gt;(size));        &#125;        cv::Mat bgr = cv::imread(img_name, cv::IMREAD_COLOR);        <span class="hljs-keyword">if</span> (bgr.empty())        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;cv::imread %s failed\n&quot;</span>, img_name.c_str());            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;        ncnn::Mat in = ncnn::Mat::from_pixels_resize(bgr.data, swapRB ? ncnn::Mat::PIXEL_BGR2RGB : ncnn::Mat::PIXEL_BGR, bgr.cols, bgr.rows, width, height);        in.substract_mean_normalize(mean_vals, norm_vals);        ncnn::Extractor ex = net.create_extractor();        ex.input(net.input_names[<span class="hljs-number">0</span>].c_str(), in);        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> j = <span class="hljs-number">0</span>; j &lt; net.conv_names.size(); j++)        &#123;            <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> layer_name = net.conv_names[j];            <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> blob_name = net.conv_bottom_blob_names[layer_name];            ncnn::Mat out;            ex.extract(blob_name.c_str(), out); <span class="hljs-comment">// 前传网络，相当于caffe的forwardTo，拿到blob数据</span>            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> k = <span class="hljs-number">0</span>; k &lt; quantize_datas.size(); k++)            &#123;                <span class="hljs-keyword">if</span> (quantize_datas[k].name == layer_name)                &#123;                    quantize_datas[k].initial_blob_max(out); <span class="hljs-comment">// 统计最大值</span>                    <span class="hljs-keyword">break</span>;                &#125;            &#125;        &#125;    &#125;    <span class="hljs-comment">// 被调函数:</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">QuantizeData::initial_blob_max</span><span class="hljs-params">(ncnn::Mat data)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> channel_num = data.c;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> size = data.w * data.h;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q = <span class="hljs-number">0</span>; q &lt; channel_num; q++)    &#123;        <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> *data_n = data.channel(q);        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; size; i++)        &#123;            max_value = <span class="hljs-built_in">std</span>::max(max_value, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">fabs</span>(data_n[i]));        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>(3) 初始化直方图间隔</strong></p><p>也很简单，遍历每个层，初始化直方图间隔=最大激活值/2048</p><pre><code class="hljs cpp">    <span class="hljs-comment">// step 2 histogram_interval</span>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;    ====&gt; step 2 : generate the histogram_interval.\n&quot;</span>);    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; net.conv_names.size(); i++)    &#123;        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> layer_name = net.conv_names[i];        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> k = <span class="hljs-number">0</span>; k &lt; quantize_datas.size(); k++)        &#123;            <span class="hljs-keyword">if</span> (quantize_datas[k].name == layer_name)            &#123;                quantize_datas[k].initial_histogram_interval();                <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;%-20s : max = %-15f interval = %-10f\n&quot;</span>, quantize_datas[k].name.c_str(), quantize_datas[k].max_value, quantize_datas[k].histogram_interval);                <span class="hljs-keyword">break</span>;            &#125;        &#125;    &#125;    <span class="hljs-comment">// 被调函数    </span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">QuantizeData::initial_histogram_interval</span><span class="hljs-params">()</span></span><span class="hljs-function"></span>&#123;    histogram_interval = max_value / <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">float</span>&gt;(num_bins);    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>(4) 计算直方图</strong></p><p>​    再前传一次，遍历每个blob，向每个bin中投票，计算出直方图，得到原始fp32分布</p><pre><code class="hljs cpp">    <span class="hljs-comment">// step 3 histogram</span>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;    ====&gt; step 3 : generate the histogram.\n&quot;</span>);    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; image_list.size(); i++)    &#123;        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> img_name = image_list[i];        <span class="hljs-keyword">if</span> ((i + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>)            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;          %d/%d\n&quot;</span>, (<span class="hljs-keyword">int</span>)(i + <span class="hljs-number">1</span>), (<span class="hljs-keyword">int</span>)size);                    cv::Mat bgr = cv::imread(img_name, cv::IMREAD_COLOR);        <span class="hljs-keyword">if</span> (bgr.empty())        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;cv::imread %s failed\n&quot;</span>, img_name.c_str());            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;        ncnn::Mat in = ncnn::Mat::from_pixels_resize(bgr.data, swapRB ? ncnn::Mat::PIXEL_BGR2RGB : ncnn::Mat::PIXEL_BGR, bgr.cols, bgr.rows, width, height);        in.substract_mean_normalize(mean_vals, norm_vals);        ncnn::Extractor ex = net.create_extractor();        ex.input(net.input_names[<span class="hljs-number">0</span>].c_str(), in);        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> j = <span class="hljs-number">0</span>; j &lt; net.conv_names.size(); j++)        &#123;            <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> layer_name = net.conv_names[j];            <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> blob_name = net.conv_bottom_blob_names[layer_name];            ncnn::Mat out;            ex.extract(blob_name.c_str(), out);            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> k = <span class="hljs-number">0</span>; k &lt; quantize_datas.size(); k++)            &#123;                <span class="hljs-keyword">if</span> (quantize_datas[k].name == layer_name)                &#123;                    quantize_datas[k].update_histogram(out);                    <span class="hljs-keyword">break</span>;                &#125;            &#125;        &#125;    &#125;    <span class="hljs-comment">// 被调函数      </span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">QuantizeData::update_histogram</span><span class="hljs-params">(ncnn::Mat data)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> channel_num = data.c;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> size = data.w * data.h;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q = <span class="hljs-number">0</span>; q &lt; channel_num; q++)    &#123;        <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> *data_n = data.channel(q);        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; size; i++)        &#123;            <span class="hljs-keyword">if</span> (data_n[i] == <span class="hljs-number">0</span>)                <span class="hljs-keyword">continue</span>;            <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> index = <span class="hljs-built_in">std</span>::min(<span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">int</span>&gt;(<span class="hljs-built_in">std</span>::<span class="hljs-built_in">abs</span>(data_n[i]) / histogram_interval), <span class="hljs-number">2047</span>);            histogram[index]++;        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>(5) 计算Scale</strong></p><pre><code class="hljs cpp">    <span class="hljs-comment">// step4 kld</span>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;    ====&gt; step 4 : using kld to find the best threshold value.\n&quot;</span>);    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; net.conv_names.size(); i++)    &#123;        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> layer_name = net.conv_names[i];        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> blob_name = net.conv_bottom_blob_names[layer_name];        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;%-20s &quot;</span>, layer_name.c_str());        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> k = <span class="hljs-number">0</span>; k &lt; quantize_datas.size(); k++)        &#123;            <span class="hljs-keyword">if</span> (quantize_datas[k].name == layer_name)            &#123;                quantize_datas[k].get_data_blob_scale();                <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;bin : %-8d threshold : %-15f interval : %-10f scale : %-10f\n&quot;</span>,                        quantize_datas[k].threshold_bin,                        quantize_datas[k].threshold,                        quantize_datas[k].histogram_interval,                        quantize_datas[k].scale);                <span class="hljs-built_in">fprintf</span>(fp, <span class="hljs-string">&quot;%s %f\n&quot;</span>, layer_name.c_str(), quantize_datas[k].scale);                <span class="hljs-keyword">break</span>;            &#125;        &#125;    &#125;<span class="hljs-comment">// 被调函数 </span><span class="hljs-function"><span class="hljs-keyword">float</span> <span class="hljs-title">QuantizeData::get_data_blob_scale</span><span class="hljs-params">()</span></span><span class="hljs-function"></span>&#123;       normalize_histogram();   <span class="hljs-comment">// 直方图归一化</span>    threshold_bin = threshold_distribution(histogram);   <span class="hljs-comment">// 计算最后有多少个bins</span>    threshold = (threshold_bin + <span class="hljs-number">0.5</span>) * histogram_interval;   <span class="hljs-comment">// 之后很容易就能找到Threshold</span>    scale = <span class="hljs-number">127</span> / threshold;   <span class="hljs-comment">// Scale也很简单就能z好到</span>    <span class="hljs-keyword">return</span> scale;&#125;</code></pre><p>其实说了半天，<strong>最核心的就在get_data_blob_scale这个函数里，函数分为3步</strong>：</p><p><strong>a. 直方图归一化</strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">QuantizeData::normalize_histogram</span><span class="hljs-params">()</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">size_t</span> length = histogram.size();    <span class="hljs-keyword">float</span> sum = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; length; i++)        sum += histogram[i];    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; length; i++)        histogram[i] /= sum;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>b. 使用KL散度计算最后用多少个bins比较合适</strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">QuantizeData::threshold_distribution</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; &amp;distribution, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> target_bin = <span class="hljs-number">128</span>)</span></span><span class="hljs-function"></span>&#123;    ...    <span class="hljs-comment">// 这里length就是原始分布Po的长度，NCNN默认2048。这里的threshold实际上是num_bins，可以换算成T</span>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> threshold = target_bin; threshold &lt; length; threshold++)   <span class="hljs-comment">// target_bin=128，length = 2048</span>    &#123;         <span class="hljs-comment">// ①. 计算截断的fp32分布P</span>         <span class="hljs-comment">// ②. 计算int8分布Q</span>         <span class="hljs-comment">// ③. 计算扩展分布Q_expand</span>        <span class="hljs-comment">// ④. 计算KL散度</span>        <span class="hljs-comment">// ⑤. 比大小</span>    &#125;&#125;</code></pre><p>①. 计算截断的fp32分布P也很简单：</p><pre><code class="hljs cpp"><span class="hljs-keyword">float</span> threshold_sum = <span class="hljs-number">0</span>;<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> threshold=target_bin; threshold&lt;length; threshold++) &#123;    threshold_sum += distribution[threshold];   <span class="hljs-comment">// 128以上的所有数据和</span>&#125;<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> threshold=target_bin; threshold&lt;length; threshold++) &#123;    <span class="hljs-function"><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; <span class="hljs-title">t_distribution</span><span class="hljs-params">(distribution.begin(), distribution.begin()+threshold)</span></span>;        t_distribution[threshold<span class="hljs-number">-1</span>] += threshold_sum;   <span class="hljs-comment">// P的最后一个bin加上被截断的所有概率，得到截断的fp32分布P</span>    threshold_sum -= distribution[threshold];       <span class="hljs-comment">// 是通过减法来保证数值正确性的，很巧秒</span>    ...</code></pre><p>②. 计算int8分布Q，注意Q是从Po得来的，而不是从P得来的，大于T的部分并不会加进最后一个bin内（存疑，不理解）；另外，当发生了4舍5入时，会有特殊处理</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; <span class="hljs-title">quantize_distribution</span><span class="hljs-params">(target_bin)</span></span>;  <span class="hljs-comment">// 量化后分布Q，长度是128</span> fill(quantize_distribution.begin(), quantize_distribution.end(), <span class="hljs-number">0</span>);<span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> num_per_bin = <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">float</span>&gt;(threshold) / target_bin;  <span class="hljs-comment">// 其实就是当前T下的Scale </span><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;target_bin; i++) &#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> start = i * num_per_bin;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> end = start + num_per_bin;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> left_upper = <span class="hljs-built_in">ceil</span>(start);    <span class="hljs-keyword">if</span> (left_upper &gt; start)     &#123;   <span class="hljs-comment">// 这里的意思是，如果发生了5入，则需要将舍掉的那个bin按比例加进来</span>        <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> left_scale = left_upper - start;        quantize_distribution[i] += left_scale * distribution[left_upper - <span class="hljs-number">1</span>];    &#125;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> right_lower = <span class="hljs-built_in">floor</span>(end);    <span class="hljs-keyword">if</span> (right_lower &lt; end)     &#123;        <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> right_scale = end - right_lower;        quantize_distribution[i] += right_scale * distribution[right_lower];    &#125;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j=left_upper; j&lt;right_lower; j++)     &#123;        quantize_distribution[i] += distribution[j];    &#125;&#125;</code></pre><p>③. 计算Q_expand，统计count时0不算在内，注意不管是统计数量还是上采样的过程中，都有4舍5入相关的问题</p><pre><code class="hljs cpp"><span class="hljs-comment">// get Q</span><span class="hljs-function"><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; <span class="hljs-title">expand_distribution</span><span class="hljs-params">(threshold, <span class="hljs-number">0</span>)</span></span>;<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;target_bin; i++) &#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> start = i * num_per_bin;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> end = start + num_per_bin;    <span class="hljs-keyword">float</span> count = <span class="hljs-number">0</span>;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> left_upper = <span class="hljs-built_in">ceil</span>(start);    <span class="hljs-keyword">float</span> left_scale = <span class="hljs-number">0</span>;    <span class="hljs-keyword">if</span> (left_upper &gt; start)     &#123;        left_scale = left_upper - start;        <span class="hljs-keyword">if</span> (distribution[left_upper - <span class="hljs-number">1</span>] != <span class="hljs-number">0</span>)         &#123;            count += left_scale;        &#125;    &#125;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> right_lower = <span class="hljs-built_in">floor</span>(end);    <span class="hljs-keyword">float</span> right_scale = <span class="hljs-number">0</span>;    <span class="hljs-keyword">if</span> (right_lower &lt; end)     &#123;        right_scale = end - right_lower;        <span class="hljs-keyword">if</span> (distribution[right_lower] != <span class="hljs-number">0</span>)         &#123;            count += right_scale;        &#125;    &#125;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j=left_upper; j&lt;right_lower; j++)     &#123;        <span class="hljs-keyword">if</span> (distribution[j] != <span class="hljs-number">0</span>)         &#123;            count++;        &#125;    &#125;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> expand_value = quantize_distribution[i] / count;    <span class="hljs-keyword">if</span> (left_upper &gt; start)     &#123;        <span class="hljs-keyword">if</span> (distribution[left_upper - <span class="hljs-number">1</span>] != <span class="hljs-number">0</span>)         &#123;            expand_distribution[left_upper - <span class="hljs-number">1</span>] += expand_value * left_scale;  <span class="hljs-comment">// 上采样过程中一样有四舍五入的问题</span>        &#125;    &#125;    <span class="hljs-keyword">if</span> (right_lower &lt; end)     &#123;        <span class="hljs-keyword">if</span> (distribution[right_lower] != <span class="hljs-number">0</span>)         &#123;            expand_distribution[right_lower] += expand_value * right_scale;        &#125;    &#125;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j=left_upper; j&lt;right_lower; j++)     &#123;        <span class="hljs-keyword">if</span> (distribution[j] != <span class="hljs-number">0</span>)         &#123;            expand_distribution[j] += expand_value;        &#125;    &#125;&#125;</code></pre><p>④. 计算KL散度，注意当Q为0时，KL散度只加一（存疑，不理解）</p><pre><code class="hljs cpp"><span class="hljs-keyword">float</span> kl_divergence = compute_kl_divergence(t_distribution, expand_distribution);<span class="hljs-function"><span class="hljs-keyword">float</span> <span class="hljs-title">QuantizeData::compute_kl_divergence</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; &amp;dist_a, <span class="hljs-keyword">const</span> <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; &amp;dist_b)</span> </span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> length = dist_a.size();    assert(dist_b.size() == length);    <span class="hljs-keyword">float</span> result = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;length; i++)     &#123;        <span class="hljs-keyword">if</span> (dist_a[i] != <span class="hljs-number">0</span>)         &#123;            <span class="hljs-keyword">if</span> (dist_b[i] == <span class="hljs-number">0</span>)             &#123;                result += <span class="hljs-number">1</span>;   <span class="hljs-comment">// Q为0时，KL散度只加一</span>            &#125;             <span class="hljs-keyword">else</span>             &#123;                result += dist_a[i] * <span class="hljs-built_in">log</span>(dist_a[i] / dist_b[i]);            &#125;        &#125;    &#125;    <span class="hljs-keyword">return</span> result;&#125;</code></pre><p>⑤. 轻松愉悦的找最大值</p><pre><code class="hljs cpp"><span class="hljs-keyword">if</span> (kl_divergence &lt; min_kl_divergence) &#123;    min_kl_divergence = kl_divergence;       target_threshold = threshold;   <span class="hljs-comment">// 实际上是num_bins</span>&#125;</code></pre><p><strong>c. 计算 Threshold 和 bins</strong></p><p>至此，我们得到了KL散度最小的桶数num_bins，可以通过下述公式得到T和Scale，就三行：</p><pre><code class="hljs cpp">threshold = (<span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">float</span>&gt;(threshold_bin) + <span class="hljs-number">0.5f</span>) * histogram_interval;scale = <span class="hljs-number">127</span> / threshold;<span class="hljs-keyword">return</span> scale;</code></pre><p>最后就是将Scale数据存下来，得到量化表了。</p><p><strong>总结一下以上的代码逻辑:</strong></p><p><img src="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/4.png" alt></p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_3</title>
    <link href="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/"/>
    <url>/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/</url>
    
    <content type="html"><![CDATA[<p>ncnn 源码分析 模型量化原理</p><a id="more"></a><h4 id="1-FP32-vs-int8"><a href="#1-FP32-vs-int8" class="headerlink" title="1. FP32 vs int8"></a>1. FP32 vs int8</h4><p>​     来看一下 <strong>FP32、FP16和int8</strong>之间的动态范围和精度的对比， 可以看到float32 的取值范围几乎是无穷的， 而int8只有<strong>-128~127</strong>. 因此需要建立映射关系将float32类型的浮点数映射到指定范围的int8类型。 </p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/ncnn1.png" alt></p><h4 id="2-TensorRT-int8-量化方案"><a href="#2-TensorRT-int8-量化方案" class="headerlink" title="2. TensorRT int8 量化方案"></a>2. TensorRT int8 量化方案</h4><p>Nvidia 的 TensorRT提供了一种量化方案，但是它仅仅提供相应的SDK和解决方案， 没有公布对应的源代码， 诸多第三方厂家则根据该解决方案自己造轮子，产生了对应的解决方案。该量化方案的最重要的两份参考资料如下所示:</p><p>- TensorRT Develop guide: <a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#work-with-qat-networks">https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#work-with-qat-networks</a></p><p>- PDF 链接： <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf</a></p><p><strong>(1)</strong> <strong>max-max 映射</strong>： 最简单粗暴的方式如下左图所示</p><p>​    首先求出一个laye 的激活值范围， 然后按照绝对值的最大值作为阈值， 然后把这个范围按照比例映射到-127到128的范围内, 其fp32和int8的转换公式为:</p><p><strong>FP32 Tensor (T) = scale_factor(sf) * 8-bit Tensor(t) + FP32_bias (b)</strong> </p><p>通过实验得知，bias值去掉对精度的影响不是很大，因此我们直接去掉, 所以该公式可以简化为:</p><p><strong>T = sf * t</strong></p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/ncnn2.png" alt></p><p><strong>(2) 饱和映射</strong></p><p>​    如上方法会有一个问题：<strong>不饱和，即通常在正负上会有一些量化值未被利用，且会产生的精度损失较大。</strong>针对 max-max 映射存在的问题， TensorRT提出了如上右图的饱和映射。 <strong>选取一个阈值T，然后将 -|T|~|T| 之间的值映射到 -127 到 128 这个范围内。这样确定了阈值T之后，其实也能确定Scale，一个简单的线性公式是: Scale = T/127。 所以要计算Scale，只要找到合适的阈值T就可以了。那么问题来了，T应该取何值? 其基本流程如下:</strong></p><p>​    <strong>(a) 选取不同的 T 阈值进行量化, 将 P(fp32) 映射到 Q(int8)。</strong></p><p>​    <strong>(b) 将 Q(int8) 反量化到 P(fp32) 一样长度，得到分布 Q_expand；</strong></p><p>​    <strong>(c) 计算P和Q_expand 的相对熵(KL散度)，然后选择相对熵最少的一个，也就是跟原分布最像的一个,</strong> <strong>从而确定Scale**</strong>。**</p><p><strong>(3) KL 散度</strong></p><p>​    KL散度可以用来<strong>描述P、Q两个分布的差异</strong>。<strong>散度越小，两个分布的差异越小，概率密度函数形状和数值越接近</strong>。这里的所有分布、计算，都是离散形式的。分布是以统计直方图的方式存在，KL散度公式也是离散公式：</p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/ncnn5.png" alt></p><p>从上式中我们还发现一个问题：KL散度计算公式要求P、Q两个统计直方图长度一样（也就是bins的数量一样）。Q一直都是-127～127；可是P的数量会随着T的变化而变化。那这怎么做KL散度呢？</p><p>ncnn 的做法是将 Q扩展到和P一样的长度，下面举个例子(NVIDIA PPT中的例子)：</p><pre><code class="hljs cpp">P = [<span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">5</span> <span class="hljs-number">3</span> <span class="hljs-number">1</span> <span class="hljs-number">7</span>]     <span class="hljs-comment">// fp32的统计直方图，T=8</span><span class="hljs-comment">// 假设只量化到两个bins，即量化后的值只有-1/0/+1三种</span>Q=[<span class="hljs-number">1</span>+<span class="hljs-number">0</span>+<span class="hljs-number">2</span>+<span class="hljs-number">3</span>, <span class="hljs-number">5</span>+<span class="hljs-number">3</span>+<span class="hljs-number">1</span>+<span class="hljs-number">7</span>] = [<span class="hljs-number">6</span>, <span class="hljs-number">16</span>]<span class="hljs-comment">// P和Q现在没法做KL散度，所以要将Q扩展到和P一样的长度</span>Q_expand = [<span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>] = [<span class="hljs-number">2</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span>]  <span class="hljs-comment">// P中有0时，不算在内</span>D = KL(P||Q_expand)  <span class="hljs-comment">// 这样就可以做KL散度计算了</span></code></pre><p>​    这个扩展的操作，就像图像的上采样一样，将低精度的统计直方图(Q)，上采样的高精度的统计直方图上去(Q_expand)。由于Q中一个bin对应P中的4个bin，因此在Q上采样的Q_expand的过程中，所有的数据要除以4。另外，在计算fp32的分布P时，被T截断的数据，是要算在最后一个bin里面的。</p><h4 id="3-ncnn的conv量化计算流程"><a href="#3-ncnn的conv量化计算流程" class="headerlink" title="3. ncnn的conv量化计算流程"></a>3. ncnn的conv量化计算流程</h4><p>正常的 fp32 计算中， 一个conv 的计算流程如下所示， 所有的数据均是 fp32， 没什么特殊的</p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/ncnn3.png" alt="fp32 conv计算流程"></p><p>在 ncnn conv 进行Int8计算时， 计算流程如下所示，ncnn首先<strong>将输入(bottom_blob)和权重量化成Int8，在Int8下计算卷积，然后反量化到 fp32，再和未量化的bias相加，得到输出 top_blob</strong>(ncnn并没有对bias做量化)</p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/ncnn4.png" alt="int8 conv 计算流程， 在conv前， 对input和weight做量化， 计算完后反量化到 fp32, 再加 bias"></p><p>输入和权重的<strong>量化公式</strong>为:</p><pre><code class="hljs cpp">bottom_blob(int8) = bottom_blob_in8t_scale * bottom(fp32)weight_blob(int8) = weight_data_int8_scale * weight(fp32)</code></pre><p><strong>反量化</strong>的目的是将int8映射回到原来的fp32,范围保持要一致, 由于 weight_blob(int8) 和 bottom_blob(int8) 相乘， 所以此处的量化反量化的 scale 应该为:</p><pre><code class="hljs cpp">dequantize_scale = <span class="hljs-number">1</span>/(bottom_blob_int8_scale * weight_data_int8_scale)innner_blob(fp32) = dequantize_scale * inner_blob</code></pre><p><strong>! 值得注意的是， 权重是在网络初始化时候就进行量化了， 而输入则是在前向推导时进行量化。</strong></p><h4 id="4-ncnn-量化工具的使用"><a href="#4-ncnn-量化工具的使用" class="headerlink" title="4. ncnn 量化工具的使用"></a>4. <strong>ncnn 量化工具的使用</strong></h4><p>(1) <strong>Optimization graphic 图优化: 最明显的变化是将conv层和bn层进行合并</strong></p><pre><code class="hljs shell">./ncnnoptimize mobilenet-fp32.param mobilenet-fp32.bin mobilenet-nobn-fp32.param mobilenet-nobn-fp32.bin</code></pre><p><strong>(2) Create the calibration table file(建议使用超过5000张图片的验证集进行对齐): 计算产生对应的 scale</strong></p><pre><code class="hljs shell">./ncnn2table --param mobilenet-nobn-fp32.param --bin mobilenet-nobn-fp32.bin --images images/ --output mobilenet-nobn.table --mean 104,117,123 --norm 0.017,0.017,0.017 --size 224,224 --thread 2</code></pre><p><strong>(3) Quantization：量化</strong></p><pre><code class="hljs shell">./ncnn2int8 mobilenet-nobn-fp32.param mobilenet-nobn-fp32.bin mobilenet-int8.param mobilenet-int8.bin mobilenet-nobn.table</code></pre><h4 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h4><p>[1] <a href="https://me.csdn.net/sinat_31425585">https://me.csdn.net/sinat_31425585</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/c_1064124187198705664">https://zhuanlan.zhihu.com/c_1064124187198705664</a></p><p>[3] <a href="https://github.com/BUG1989/caffe-int8-convert-tools">https://github.com/BUG1989/caffe-int8-convert-tools</a></p><p>[4] <a href="https://github.com/Tencent/ncnn/wiki/quantized-int8-inference">Tencent/ncnn</a></p><p>[5] QNNPACK</p><p>[6] Nvidia solution： Szymon Migacz. 8-bit Inference with TensorRT</p><p>[7] Google solution：Quantizing deep convolutional networks for efficient inference: A whitepaper</p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_2</title>
    <link href="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-2/"/>
    <url>/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-2/</url>
    
    <content type="html"><![CDATA[<p>ncnn 源码分析（二） Extractor</p><a id="more"></a><p>前面大致总结了一下ncnn模型载入的流程，模型载入之后，就是新建一个<strong>Extractor</strong>，然后设置输入，获取输出：</p><pre><code class="hljs cpp">ncnn::Extractor ex = net.create_extractor();ex.set_num_threads(<span class="hljs-number">4</span>); ex.input(<span class="hljs-string">&quot;data&quot;</span>, in); ncnn::Mat out;ex.extract(<span class="hljs-string">&quot;detection_out&quot;</span>, out);</code></pre><p>现在可以看一下Extractor的定义了：</p><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Extractor</span></span><span class="hljs-class">&#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-comment">// enable light mode, intermediate blob will be recycled when enabled</span>    <span class="hljs-comment">// enabled by default</span>    <span class="hljs-comment">// 设置light模式，启用时中间 blob 将会循环利用</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">set_light_mode</span><span class="hljs-params">(<span class="hljs-keyword">bool</span> enable)</span></span>;     <span class="hljs-comment">// set thread count for this extractor, 设置线程数</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">set_num_threads</span><span class="hljs-params">(<span class="hljs-keyword">int</span> num_threads)</span></span>;     <span class="hljs-comment">// set blob memory allocator, 设置blob的内存分配器</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">set_blob_allocator</span><span class="hljs-params">(Allocator* allocator)</span></span>;     <span class="hljs-comment">// set workspace memory allocator, 设置工作空间的内存分配器</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">set_workspace_allocator</span><span class="hljs-params">(Allocator* allocator)</span></span>;  <span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>    <span class="hljs-comment">// set input by blob name</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 设置网络输入：字符串layer名</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">input</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* blob_name, <span class="hljs-keyword">const</span> Mat&amp; in)</span></span>;     <span class="hljs-comment">// get result by blob name</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 设置提取器的输入：得到对应输出</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">extract</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* blob_name, Mat&amp; feat)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>     <span class="hljs-comment">// set input by blob index</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 设置int类型blob索引及输入</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">input</span><span class="hljs-params">(<span class="hljs-keyword">int</span> blob_index, <span class="hljs-keyword">const</span> Mat&amp; in)</span></span>;     <span class="hljs-comment">// get result by blob index</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 设置int类型blob索引及输出</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">extract</span><span class="hljs-params">(<span class="hljs-keyword">int</span> blob_index, Mat&amp; feat)</span></span>; <span class="hljs-keyword">protected</span>:    <span class="hljs-comment">// 对外提供create_extractor接口</span>    <span class="hljs-function"><span class="hljs-keyword">friend</span> Extractor <span class="hljs-title">Net::create_extractor</span><span class="hljs-params">()</span> <span class="hljs-keyword">const</span></span>;    Extractor(<span class="hljs-keyword">const</span> Net* net, <span class="hljs-keyword">int</span> blob_count); <span class="hljs-keyword">private</span>:    <span class="hljs-comment">// 网络 </span>    <span class="hljs-keyword">const</span> Net* net;    <span class="hljs-comment">// blob的 mat</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt; blob_mats;    <span class="hljs-comment">// 选项</span>    Option opt; &#125;;</code></pre><p>除了设置option的接口之外，就只剩下我们需要使用的几个接口函数了：</p><p><strong>(1) Extractor</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// 创建Extractor</span><span class="hljs-function">Extractor <span class="hljs-title">Net::create_extractor</span><span class="hljs-params">()</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">return</span> Extractor(<span class="hljs-keyword">this</span>, blobs.size());&#125;</code></pre><p>内部调用了接口为，就是将blob_mat数组resize到网络的blob数目大小，然后设置了一下选项：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 执行器</span>Extractor::Extractor(<span class="hljs-keyword">const</span> Net* _net, <span class="hljs-keyword">int</span> blob_count) : net(_net)&#123;    blob_mats.resize(blob_count);    opt = net-&gt;opt;&#125;</code></pre><p><strong>(2) input接口：</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// 设置输入</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Extractor::input</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* blob_name, <span class="hljs-keyword">const</span> Mat&amp; in)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// 获取输入模块对应index</span>    <span class="hljs-keyword">int</span> blob_index = net-&gt;find_blob_index_by_name(blob_name);    <span class="hljs-keyword">if</span> (blob_index == <span class="hljs-number">-1</span>)        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;     <span class="hljs-comment">// 调用直接用index的设置input方法</span>    <span class="hljs-keyword">return</span> input(blob_index, in);&#125;</code></pre><p>内部调用的接口为：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 输入为index的输入接口</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Extractor::input</span><span class="hljs-params">(<span class="hljs-keyword">int</span> blob_index, <span class="hljs-keyword">const</span> Mat&amp; in)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">if</span> (blob_index &lt; <span class="hljs-number">0</span> || blob_index &gt;= (<span class="hljs-keyword">int</span>)blob_mats.size())        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;     <span class="hljs-comment">// 设置blob_index对应 Mat</span>    blob_mats[blob_index] = in;     <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>(3) extract接口：</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// 将输入string类型name转换成对应的索引</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Extractor::extract</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* blob_name, VkMat&amp; feat, VkCompute&amp; cmd)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> blob_index = net-&gt;find_blob_index_by_name(blob_name);    <span class="hljs-keyword">if</span> (blob_index == <span class="hljs-number">-1</span>)        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;     <span class="hljs-keyword">return</span> extract(blob_index, feat, cmd);&#125;</code></pre><p>这里调用的接口为：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 提取特征</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Extractor::extract</span><span class="hljs-params">(<span class="hljs-keyword">int</span> blob_index, Mat&amp; feat)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">if</span> (blob_index &lt; <span class="hljs-number">0</span> || blob_index &gt;= (<span class="hljs-keyword">int</span>)blob_mats.size())        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;     <span class="hljs-keyword">int</span> ret = <span class="hljs-number">0</span>;        <span class="hljs-keyword">if</span> (blob_mats[blob_index].dims == <span class="hljs-number">0</span>)     <span class="hljs-comment">// 如果输出blob为空</span>    &#123;        <span class="hljs-keyword">int</span> layer_index = net-&gt;blobs[blob_index].producer;  <span class="hljs-comment">// 查找输出blob对应的生产者</span>        ret = net-&gt;forward_layer(layer_index, blob_mats, opt);   <span class="hljs-comment">// 前向推理</span>    &#125;    feat = blob_mats[blob_index];   <span class="hljs-comment">// 输出特征</span>     <span class="hljs-keyword">if</span> (opt.use_packing_layout)   <span class="hljs-comment">// 对特征进行unpack</span>    &#123;        Mat bottom_blob_unpacked;        convert_packing(feat, bottom_blob_unpacked, <span class="hljs-number">1</span>, opt);        feat = bottom_blob_unpacked;    &#125;     <span class="hljs-keyword">return</span> ret;&#125;</code></pre><p>​    这里就是调用各层前向推理forward_layer方法来进行推理的，这个对应于特定层的推理过程，后面总结各个层的时候再说。</p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_1</title>
    <link href="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1/"/>
    <url>/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1/</url>
    
    <content type="html"><![CDATA[<p>ncnn 源码分析 — 参数与模型载入</p><a id="more"></a><h4 id="1-实例代码"><a href="#1-实例代码" class="headerlink" title="1. 实例代码"></a>1. 实例代码</h4><p>使用ncnn进行前向计算的步骤很简单，就如下几行代码即可完成。</p><pre><code class="hljs cpp"> <span class="hljs-comment">// 代码来自 ncnn/examples/shufflenetv2.cpp</span> <span class="hljs-comment">/* Step 1.1 : 加载.parma 文件 和 .bin 文件 */</span> ncnn::Net shufflenetv2; shufflenetv2.load_param(<span class="hljs-string">&quot;shufflenet_v2_x0.5.param&quot;</span>); shufflenetv2.load_model(<span class="hljs-string">&quot;shufflenet_v2_x0.5.bin&quot;</span>); <span class="hljs-comment">/* Step 1.2 : 构建并配置 提取器 */</span> ncnn::Extractor ex = shufflenetv2.create_extractor(); <span class="hljs-comment">/* Step 1.3 : 设置输入（将图片转换成ncnn::Mat结构作为输入） */</span>    ncnn::Mat in = ncnn::Mat::from_pixels_resize(bgr.data, ncnn::Mat::PIXEL_BGR, bgr.cols, bgr.rows, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>);<span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> norm_vals[<span class="hljs-number">3</span>] = &#123;<span class="hljs-number">1</span>/<span class="hljs-number">255.f</span>, <span class="hljs-number">1</span>/<span class="hljs-number">255.f</span>, <span class="hljs-number">1</span>/<span class="hljs-number">255.f</span>&#125;;in.substract_mean_normalize(<span class="hljs-number">0</span>, norm_vals);ex.input(<span class="hljs-string">&quot;data&quot;</span>, in);        <span class="hljs-comment">/* Step 1.4 : 提取输出 */</span> ncnn::Mat out;ex.extract(<span class="hljs-string">&quot;fc&quot;</span>, out);</code></pre><h4 id="2-代码分析"><a href="#2-代码分析" class="headerlink" title="2. 代码分析"></a>2. 代码分析</h4><p><strong>我姑且将其分为：加载模型</strong>、<strong>构建并配置提取器</strong>、<strong>设置输入</strong>、<strong>输出处理</strong>、<strong>模型封装</strong>五个部分来加以分析</p><p><strong>模型载入</strong>:</p><p><strong>相关代码:</strong></p><p><strong>net.h/cpp</strong>   <strong>blob.h/cpp</strong>   <strong>layer.h/.cpp</strong>  </p><p>​    <strong>paramdict.cpp/h</strong>  </p><p>​    <strong>modelbin.h/.cpp</strong></p><p><strong>相关文件:</strong></p><p>xx.bin  xx.param</p><h4 id="3-加载模型"><a href="#3-加载模型" class="headerlink" title="3. 加载模型"></a>3. 加载模型</h4><p>ncnn 在使用 <strong>.param</strong> 和 <strong>.bin</strong> 两个文件来描述一个神经网络模型。 模型加载的根本目的是将 .param 和 .bin 文件的信息加载到目标神经网络（一个ncnn::Net结构）中</p><p>其中：</p><p><strong>.param</strong>：描述神经网络的结构，包括层名称，层输入输出信息，层参数信息（如卷积层的kernal大小等）等。</p><p><strong>.bin</strong> 文件则记录神经网络运算所需要的数据信息（比如卷积层的权重、偏置信息等）</p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1/ncnn1.png" alt="ncnn官方demo中的模型文件"></p><h6 id="3-1-param-文件"><a href="#3-1-param-文件" class="headerlink" title="3.1 param 文件"></a>3.1 param 文件</h6><p><strong>一个.param文件由以下几部分组成：</strong></p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1/ncnn2.png" alt></p><p><strong>1）MagicNum</strong></p><p>固定位7767517，可以通过这个数来判定版本 -&gt; 为什么这个数字，不知道问倪神去吧 </p><p>2）<strong>layer、blob个数</strong></p><p>上图示例的文件两个数字分别为：75、83</p><p>​    <strong>layer：我们知道神经网络是一层一层向前推进计算的，每一层我们用一个layer表示；</strong></p><p>​    <strong>blob：每一个layer都可能会有输入、输出，在ncnn中，它们统一用一个多维（3维）向量表示，我们称每一个输入、输出的原子为一个blob，并为它起名</strong></p><p>2.1.1.2 layer的描述</p><p>layer 在 .param 中是一个相对复杂的元素（从第3行起的每一行描述一个layer），所以我们把它单独抽出来进行说明。</p><p><strong>1）层类型</strong>     比如<strong>Input、Convolution、ReLU</strong></p><p><strong>2）层名</strong>       模型训练者为该层起得名字（毕竟相同类型的层可能多次使用，我们要区分它们）</p><p><strong>3）层输入输出  包含：层输入blob数量，层输出blob数量，层输入、输出blob的名称</strong></p><p><strong>4）层配置参数</strong></p><p>比如 <strong>卷积层（Convolution Layer）的 卷积核大小、步长信息</strong> 等</p><p>在 具体层里面都有一个函数: load_param, 从里面可以查询到相关信息。</p><p><strong>data层： 0=长 1=宽 3=通道</strong></p><p><strong>Convolution层 0=输出单元 1=卷积核大小  2=核膨胀[见膨胀卷积]    3=stride</strong>    </p><p>​                                 <strong>4=padding    5=是否存在偏置    6=权重数量</strong></p><p><strong>pooling 层    0=池化类型   1=卷积核大小   2=步长stride   3=padding   4=全局池化  5=padding类型</strong></p><p><strong>ReLU 层 0=0.000000 无参数</strong></p><p><strong>softmax 层 0=0 无参数</strong></p><p><strong>Concat Split Dropout 无参数</strong></p><p><strong>ConvolutionDepthWise    7=group 数目</strong></p><h6 id="3-2-读取"><a href="#3-2-读取" class="headerlink" title="3.2 读取"></a>3.2 读取</h6><p><strong>下面我们具体从代码的角度来看看如何读取这个文件的:(文件为 net.cpp/h 和 paramdict.cpp/h) 为了方便， 我们将 Vulkan 相关代码剔除掉。</strong></p><p><strong>net.h 主要是 Net 类的接口， 其中最重要的功能是实现 load_param(载入模型参数) 和 load_model(载入模型的数据) 功能</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// net.h</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span></span><span class="hljs-class">&#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-comment">// empty init</span>    Net();    <span class="hljs-comment">// clear and destroy</span>    ~Net();<span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>    <span class="hljs-comment">// register custom layer by layer type name</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 注册自定义类型层， 通过string类型名</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">register_custom_layer</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* type, layer_creator_func creator)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>    <span class="hljs-comment">// register custom layer by layer type</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 注册自定义层， 通过int类型的 layer 索引</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">register_custom_layer</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index, layer_creator_func creator)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STDIO</span><span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>    <span class="hljs-comment">// load network structure from plain param file</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 从文件指针中载入参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(FILE* fp)</span></span>;    <span class="hljs-comment">// 从 param 文件中载入参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* protopath)</span></span>;    <span class="hljs-comment">// 从 mem 中载入参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param_mem</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* mem)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>    <span class="hljs-comment">// load network structure from binary param file</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 从二进制文件指针中载入 param 参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param_bin</span><span class="hljs-params">(FILE* fp)</span></span>;    <span class="hljs-comment">// 从二进制文件中载入参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param_bin</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* protopath)</span></span>;    <span class="hljs-comment">// load network weight data from model file</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 从 file 指针中传入模型</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(FILE* fp)</span></span>;    <span class="hljs-comment">// 从二进制文件中载入模型</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* modelpath)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STDIO</span></span>    <span class="hljs-comment">// load network structure from external memory</span>    <span class="hljs-comment">// memory pointer must be 32-bit aligned</span>    <span class="hljs-comment">// return bytes consumed</span>    <span class="hljs-comment">// 从外部内存中载入参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>* mem)</span></span>;    <span class="hljs-comment">// reference network weight data from external memory</span>    <span class="hljs-comment">// weight data is not copied but referenced</span>    <span class="hljs-comment">// so external memory should be retained when used</span>    <span class="hljs-comment">// memory pointer must be 32-bit aligned</span>    <span class="hljs-comment">// return bytes consumed</span>    <span class="hljs-comment">// 从外部内存中载入网络权重</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>* mem)</span></span>;    <span class="hljs-comment">// unload network structure and weight data</span>    <span class="hljs-comment">// 清空网络结构</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">clear</span><span class="hljs-params">()</span></span>;    <span class="hljs-comment">// construct an Extractor from network</span>    <span class="hljs-comment">// 从网络构建一个执行器</span>    <span class="hljs-function">Extractor <span class="hljs-title">create_extractor</span><span class="hljs-params">()</span> <span class="hljs-keyword">const</span></span>;<span class="hljs-keyword">protected</span>:    <span class="hljs-keyword">friend</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Extractor</span>;</span> <span class="hljs-comment">// 外部 Extractor 接口</span><span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRIN</span>    <span class="hljs-comment">// 通过name查找blob对应的索引</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">find_blob_index_by_name</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* name)</span> <span class="hljs-keyword">const</span></span>;     <span class="hljs-comment">// 通过name查找对应的 layer 索引</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">find_layer_index_by_name</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* name)</span> <span class="hljs-keyword">const</span></span>;    <span class="hljs-comment">// 通过类型查找对应的 layer索引</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">custom_layer_to_index</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* type)</span></span>;    <span class="hljs-comment">// 通过类型创建layer</span>    <span class="hljs-function">Layer* <span class="hljs-title">create_custom_layer</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* type)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>    <span class="hljs-comment">// 通过 index 穿件 layer</span>    <span class="hljs-function">Layer* <span class="hljs-title">create_custom_layer</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index)</span></span>;    <span class="hljs-comment">// 前向推理层</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">forward_layer</span><span class="hljs-params">(<span class="hljs-keyword">int</span> layer_index, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; blob_mats, Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;<span class="hljs-keyword">protected</span>:    <span class="hljs-comment">// blobs &amp; layers</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Blob&gt; blobs;    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Layer*&gt; layers;    <span class="hljs-comment">// layers</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;layer_registry_entry&gt; custom_layer_registry;&#125;;</code></pre><p>在此我们可以先来看一下 blob类 ! 着重看一下，对应的生产者、消费者模型</p><pre><code class="hljs cpp"><span class="hljs-comment">// Blob 用于记录数据传输过程， producer 记录当前blob从那一层产生的，</span><span class="hljs-comment">// consumer 记录当前blob被哪些层调用:</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Blob</span></span><span class="hljs-class">&#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-comment">// empty</span>    Blob();<span class="hljs-keyword">public</span>:<span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>    <span class="hljs-comment">// blob name</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> name;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>    <span class="hljs-comment">// layer index which produce this blob as output</span>    <span class="hljs-comment">// 生产者</span>    <span class="hljs-keyword">int</span> producer;    <span class="hljs-comment">// layer index which need this blob as input</span>    <span class="hljs-comment">// 消费者</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; consumers;&#125;;</code></pre><p>然后我们打开 net.cpp 文件，来看一下 load_param 的具体实现:</p><pre><code class="hljs cpp"><span class="hljs-comment">// 从文件中载入 net 参数</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Net::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* protopath)</span></span><span class="hljs-function"></span>&#123;    FILE* fp = fopen(protopath, <span class="hljs-string">&quot;rb&quot;</span>);    <span class="hljs-keyword">if</span> (!fp)    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;fopen %s failed\n&quot;</span>, protopath);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;    <span class="hljs-comment">// 从文件指针中载入 param</span>    <span class="hljs-keyword">int</span> ret = load_param(fp);    fclose(fp);    <span class="hljs-keyword">return</span> ret;&#125;</code></pre><p>参数载入接口中， 调用了另外一个参数载入接口: load_param(FILE * fp)</p><p>(1) 读取 magic number, 通过判断 magic number 是否等于 7767517, 就可以判断当前param文件是否是最新的 param 文件 </p><pre><code class="hljs cpp"><span class="hljs-keyword">int</span> magic = <span class="hljs-number">0</span>;<span class="hljs-comment">// 读取 magic number</span><span class="hljs-keyword">int</span> nbr = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%d&quot;</span>, &amp;magic);<span class="hljs-comment">// 读取失败</span><span class="hljs-keyword">if</span> (nbr != <span class="hljs-number">1</span>)&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;issue with param file\n&quot;</span>);    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;&#125;<span class="hljs-comment">// 判断是否是最新的 magic number</span><span class="hljs-keyword">if</span> (magic != <span class="hljs-number">7767517</span>)&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;param is too old, please regenerate\n&quot;</span>);    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;&#125;</code></pre><p>(2) 解析出网络的 layer 层数和 blob 数目 </p><pre><code class="hljs cpp"><span class="hljs-comment">// 对 layer 和 blob 进行解析</span><span class="hljs-keyword">int</span> layer_count = <span class="hljs-number">0</span>;<span class="hljs-keyword">int</span> blob_count = <span class="hljs-number">0</span>;<span class="hljs-comment">// 层数 &amp;&amp; blob 数目</span>nbr = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%d %d&quot;</span>, &amp;layer_count, &amp;blob_count);<span class="hljs-comment">// 层数和 blob数读取失败</span><span class="hljs-keyword">if</span> (nbr != <span class="hljs-number">2</span> || layer_count &lt;= <span class="hljs-number">0</span> || blob_count &lt;= <span class="hljs-number">0</span>)&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;issue with param file\n&quot;</span>);    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;&#125;<span class="hljs-comment">// resize 网络的层数和blob数目</span>layers.resize((<span class="hljs-keyword">size_t</span>)layer_count);blobs.resize((<span class="hljs-keyword">size_t</span>)blob_count);</code></pre><p>(3) 遍历所有的 layer, 解析每层 layer 层的类型(layer type)、名称(layer name)、输入数目(bottom_count) 和 输出数目(top_count)</p><pre><code class="hljs cpp"><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;layer_count; i++)&#123;    <span class="hljs-keyword">int</span> nscan = <span class="hljs-number">0</span>;    <span class="hljs-comment">// layer 的类型和名称</span>    <span class="hljs-keyword">char</span> layer_type[<span class="hljs-number">257</span>];    <span class="hljs-keyword">char</span> layer_name[<span class="hljs-number">257</span>];    <span class="hljs-keyword">int</span> bottom_count = <span class="hljs-number">0</span>;    <span class="hljs-keyword">int</span> top_count = <span class="hljs-number">0</span>;    <span class="hljs-comment">// 读取层类型、名称。输入bottom数目和输出top数目</span>    nscan = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%256s %256s %d %d&quot;</span>, layer_type, layer_name, &amp;bottom_count, &amp;top_count);    <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">4</span>) <span class="hljs-comment">// 解析失败</span>    &#123;        <span class="hljs-keyword">continue</span>;    &#125;</code></pre><p>(4) 根据layer的类型， 创建 layer</p><pre><code class="hljs cpp"><span class="hljs-comment">// 创建 layer</span>Layer* layer = create_layer(layer_type);<span class="hljs-comment">// layer_type 不是默认类型</span><span class="hljs-keyword">if</span> (!layer)&#123;    <span class="hljs-comment">// 从自定义 layer 读取</span>    layer = create_custom_layer(layer_type);&#125;<span class="hljs-keyword">if</span> (!layer) <span class="hljs-comment">// 如果自定义 layer 中不存在当前类型的 layer </span>&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;layer %s not exists or registered\n&quot;</span>, layer_type);    clear();    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;&#125;<span class="hljs-comment">// 设置 layer 参数: layer的类型、名称、输入和输出      </span>layer-&gt;type = <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span>(layer_type);layer-&gt;name = <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span>(layer_name);</code></pre><p>(5) 在设置输入时，如果当前blob名不存在，就将当前blob名添加到net的blobs数组里面</p><pre><code class="hljs cpp">layer-&gt;bottoms.resize(bottom_count); <span class="hljs-comment">// layer的输入</span><span class="hljs-comment">// 解析 layer 的输入</span><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j=<span class="hljs-number">0</span>; j&lt;bottom_count; j++)&#123;    <span class="hljs-keyword">char</span> bottom_name[<span class="hljs-number">257</span>];    <span class="hljs-comment">// 解析 bottom的name</span>    nscan = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%256s&quot;</span>, bottom_name);    <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>)    &#123;        <span class="hljs-keyword">continue</span>;    &#125;    <span class="hljs-comment">// 按照 bottom 的name 查找对应 blob 的index</span>    <span class="hljs-keyword">int</span> bottom_blob_index = find_blob_index_by_name(bottom_name);    <span class="hljs-comment">// 如果没有找到 bottom_name 对应的 blob</span>    <span class="hljs-comment">// 将向 blobs 数组中插入一个名为 bottom_name 的 blob</span>    <span class="hljs-keyword">if</span> (bottom_blob_index == <span class="hljs-number">-1</span>)    &#123;        <span class="hljs-comment">// 设置第blob_index个blob 的参数</span>        Blob&amp; blob = blobs[blob_index];        <span class="hljs-comment">// blob的索引</span>        bottom_blob_index = blob_index;        <span class="hljs-comment">// 设置blob的name</span>        blob.name = <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span>(bottom_name);        <span class="hljs-comment">// 更新全局的 blob 索引</span>        blob_index++;    &#125;    <span class="hljs-comment">// 设置当前的blob的参数</span>    Blob&amp; blob = blobs[bottom_blob_index];    <span class="hljs-comment">// 使用当前的blob记录传输关系， 第i层以当前blob为输入</span>    blob.consumers.push_back(i);    <span class="hljs-comment">// 第i层layer的第j个输入</span>    layer-&gt;bottoms[j] = bottom_blob_index;&#125;</code></pre><p> (6) 设置输出的过程和这个类似，在此不再赘述，最后就是<strong>参数载入了</strong>:</p><p>例如: <strong>conv1的参数: 0=64 1=3 11=3 5=1 6=1728</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">//解析 blob后面跟随的特定参数字典 pd</span><span class="hljs-keyword">int</span> pdlr = pd.load_param(fp);<span class="hljs-keyword">if</span> (pdlr != <span class="hljs-number">0</span>)&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict load_param failed\n&quot;</span>);    <span class="hljs-keyword">continue</span>;&#125;<span class="hljs-comment">// layer 载入 param</span><span class="hljs-keyword">int</span> lr = layer-&gt;load_param(pd);<span class="hljs-keyword">if</span> (lr != <span class="hljs-number">0</span>)&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;layer load_param failed\n&quot;</span>);    <span class="hljs-keyword">continue</span>;&#125;layers[i] = layer;</code></pre><p>这其中有两个重要的函数:</p><p>pd.load_param(fp) 和  layer_param(pd)。前者负责解析 .param 文件中特定的参数， 后者则是用解析的参数来构建对应的layer</p><p>(7) 用参数字典来解析layer相关参数！ 看一下这个自己构造layer， 以及解析的过程 </p><p>   在使用load_param接口载入参数时，需要用参数字典ParamDict来解析.param文件中的特定参数，那么参数字典具体如何进行解析的？我们首先看一下paramdict.h文件中定义的数据成员变量：</p><pre><code class="hljs cpp"><span class="hljs-comment">// parameters</span><span class="hljs-class"><span class="hljs-keyword">struct</span></span><span class="hljs-class">&#123;</span>    <span class="hljs-comment">// 是否已经被载入：1表示已载入</span>    <span class="hljs-keyword">int</span> loaded;    <span class="hljs-comment">// 单个值可能为整形也有可能为浮点型</span>    <span class="hljs-keyword">union</span> &#123; <span class="hljs-keyword">int</span> i; <span class="hljs-keyword">float</span> f; &#125;;    <span class="hljs-comment">// 还有可能是数组</span>    Mat v;&#125; params[NCNN_MAX_PARAM_COUNT];</code></pre><p>​    这里，NCNN_MAX_PARAM_COUNT大小为20，params是一个大小为32的结构体数组，即一行中特定参数数量不能超过20，当然，一般情况下也不会超过20。</p><pre><code class="hljs cpp"><span class="hljs-comment">// at most 20 parameters</span><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> NCNN_MAX_PARAM_COUNT 20</span></code></pre><p>​    然后，我们看一下paramdict.cpp源码，可以看到，这里会解析出当前行的index（id），也即是等号左边部分： </p><pre><code class="hljs cpp"><span class="hljs-comment">// 解析之后的 key=value 对</span><span class="hljs-keyword">int</span> id = <span class="hljs-number">0</span>;<span class="hljs-keyword">while</span> (<span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%d=&quot;</span>, &amp;id) == <span class="hljs-number">1</span>)&#123;    ...&#125;</code></pre><p> 在这里可以结合 <a href="https://github.com/Tencent/ncnn/wiki/param-and-model-file-structure">https://github.com/Tencent/ncnn/wiki/param-and-model-file-structure</a> 阅读源码：</p><p><strong>index-value 的规则为:</strong></p><ul><li>index为0~19: 对应整形或浮点型数据</li><li><p>index小于 -23000： 对应整形或浮点型数组, 等号右边第一个参数就是数组长度，后面顺序就是数组内容，[array size],int,int,…,int或[array size],float,float,…,float，例如：</p><p><strong>0=1 1=2.5 -23303=2,2.0,3.0</strong></p><p>index为 -23303，表明当前参数为数组，等号右边第一个参数为2，表明数组长度为2，后面2.0,3.0就是数组的内容</p></li></ul><pre><code class="hljs cpp"><span class="hljs-keyword">bool</span> is_array = id &lt;= <span class="hljs-number">-23300</span>; <span class="hljs-comment">// index &lt;= -23300：数组</span><span class="hljs-keyword">if</span> (is_array) <span class="hljs-comment">// 如果是数组</span>&#123;  <span class="hljs-comment">// 计算id</span>    id = -id - <span class="hljs-number">23300</span>;&#125;<span class="hljs-keyword">if</span> (is_array)  <span class="hljs-comment">// 如果当前参数是数组类型</span>&#123;    <span class="hljs-keyword">int</span> len = <span class="hljs-number">0</span>;  <span class="hljs-comment">// 数组长度</span>    <span class="hljs-keyword">int</span> nscan = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%d&quot;</span>, &amp;len);    <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>)             <span class="hljs-comment">// 等于1才表示读取成功</span>    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict read array length failed\n&quot;</span>);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;        params[id].v.create(len);  <span class="hljs-comment">// 创建数组：就是一个Mat</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; len; j++)    &#123;        <span class="hljs-keyword">char</span> vstr[<span class="hljs-number">16</span>]; <span class="hljs-comment">// 从二值文件中读取string</span>        nscan = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;,%15[^,\n ]&quot;</span>, vstr);        <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>) <span class="hljs-comment">// 如果读取失败</span>        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict read array element failed\n&quot;</span>);            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;         <span class="hljs-comment">// 是否为浮点型：看解析的字符串中是否存在&#x27;.&#x27;或&#x27;e&#x27;</span>        <span class="hljs-comment">// 小数点计数法和科学计数法</span>        <span class="hljs-keyword">bool</span> is_float = vstr_is_float(vstr);        <span class="hljs-comment">// 如果是浮点数</span>        <span class="hljs-keyword">if</span> (is_float)  <span class="hljs-comment">// vstr赋值给params[id].v[j]</span>        &#123;            <span class="hljs-keyword">float</span>* ptr = params[id].v;            nscan = <span class="hljs-built_in">sscanf</span>(vstr, <span class="hljs-string">&quot;%f&quot;</span>, &amp;ptr[j]);        &#125;        <span class="hljs-keyword">else</span>  <span class="hljs-comment">// vstr赋值给params[id].v[j]</span>        &#123;            <span class="hljs-keyword">int</span>* ptr = params[id].v;            nscan = <span class="hljs-built_in">sscanf</span>(vstr, <span class="hljs-string">&quot;%d&quot;</span>, &amp;ptr[j]);        &#125;        <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>)  <span class="hljs-comment">// 赋值失败</span>        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict parse array element failed\n&quot;</span>);            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;    &#125;&#125;</code></pre><p>​    这里有个vstr_is_float函数，原理很简单，就是判断数字对应字符串中是否存在小数点’.’或字母’e’，对应小数的两种写法，一种正常的小数点表示法，一种是科学计数法。</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">bool</span> <span class="hljs-title">vstr_is_float</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span> vstr[<span class="hljs-number">16</span>])</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// look ahead for determine isfloat</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j=<span class="hljs-number">0</span>; j&lt;<span class="hljs-number">16</span>; j++)    &#123;        <span class="hljs-keyword">if</span> (vstr[j] == <span class="hljs-string">&#x27;\0&#x27;</span>)            <span class="hljs-keyword">break</span>;        <span class="hljs-keyword">if</span> (vstr[j] == <span class="hljs-string">&#x27;.&#x27;</span> || <span class="hljs-built_in">tolower</span>(vstr[j]) == <span class="hljs-string">&#x27;e&#x27;</span>)            <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;&#125;</code></pre><p>​    如果不是数组，直接读取即可：</p><pre><code class="hljs cpp"><span class="hljs-keyword">else</span>   <span class="hljs-comment">// 不是数组</span>&#123;    <span class="hljs-keyword">char</span> vstr[<span class="hljs-number">16</span>];    <span class="hljs-keyword">int</span> nscan = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%15s&quot;</span>, vstr); <span class="hljs-comment">// 直接将字符串赋值给vstr</span>    <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>)  <span class="hljs-comment">// 赋值失败</span>    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict read value failed\n&quot;</span>);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;    <span class="hljs-keyword">bool</span> is_float = vstr_is_float(vstr);  <span class="hljs-comment">// 判断是否为浮点数</span>    <span class="hljs-keyword">if</span> (is_float)  <span class="hljs-comment">// 将字符串中的值赋给参数字典</span>        nscan = <span class="hljs-built_in">sscanf</span>(vstr, <span class="hljs-string">&quot;%f&quot;</span>, &amp;params[id].f);    <span class="hljs-keyword">else</span>        nscan = <span class="hljs-built_in">sscanf</span>(vstr, <span class="hljs-string">&quot;%d&quot;</span>, &amp;params[id].i);    <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>)  <span class="hljs-comment">// 赋值失败</span>    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict parse value failed\n&quot;</span>);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;&#125;params[id].loaded = <span class="hljs-number">1</span>;         <span class="hljs-comment">// 载入成功</span></code></pre><p>(8) 用参数字典来解析layer相关参数</p><p>如前所示, layer会根据参数字典来构造 layer</p><pre><code class="hljs cpp"> <span class="hljs-comment">// layer载入param</span><span class="hljs-keyword">int</span> lr = layer-&gt;load_param(pd);</code></pre><p>转到 layer层的 load_param 接口可以看到：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 载入参数：参数列表</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Layer::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; <span class="hljs-comment">/*pd*/</span>)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>这里并没有实现，在layer.h头文件中有：</p><pre><code class="hljs cpp"><span class="hljs-comment">// load layer specific parameter from parsed dict</span><span class="hljs-comment">// return 0 if success</span><span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; pd)</span></span>;</code></pre><p> ! load_param实际上是一个虚函数，熟悉C++的同学应该知道，调用虚函数时，实际调用的是继承类的版本，那么到底如何调用的？，我们可以往回看，有这样一段代码：</p><pre><code class="hljs cpp">Layer* layer = create_layer(layer_type)  <span class="hljs-comment">// 创建layer</span><span class="hljs-keyword">if</span> (!layer) <span class="hljs-comment">// layer_type不是默认类型</span>&#123;       layer = create_custom_layer(layer_type);   <span class="hljs-comment">// 从自定义layer读取</span>&#125;<span class="hljs-keyword">if</span> (!layer)   <span class="hljs-comment">// 如果自定义layer中也不存在当前类型layer</span>&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;layer %s not exists or registered\n&quot;</span>, layer_type);    clear();    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;&#125;</code></pre><p>回到layer.cpp文件中，可以看到，代码中先找到当前层layer类型对应层注册器中类型的索引index。</p><p><strong>layer_type -&gt; index -&gt; create_layer</strong></p><pre><code class="hljs cpp"> <span class="hljs-comment">// 将string对应layer类型转换成对应index</span> <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">layer_to_index</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* type)</span></span><span class="hljs-function"> </span>&#123;     <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;layer_registry_entry_count; i++)     &#123;         <span class="hljs-keyword">if</span> (<span class="hljs-built_in">strcmp</span>(type, layer_registry[i].name) == <span class="hljs-number">0</span>)             <span class="hljs-keyword">return</span> i;     &#125;     <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>; &#125;   <span class="hljs-comment">// 根据index创建layer：</span><span class="hljs-function">Layer* <span class="hljs-title">create_layer</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">if</span> (index &lt; <span class="hljs-number">0</span> || index &gt;= layer_registry_entry_count)     <span class="hljs-comment">// index不能超过索引范围</span>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;     layer_creator_func layer_creator = layer_registry[index].creator;     <span class="hljs-comment">// 创建layer构造器</span>    <span class="hljs-keyword">if</span> (!layer_creator)     <span class="hljs-comment">// layer构造器创建失败</span>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;     Layer* layer = layer_creator();     <span class="hljs-comment">// 构造layer</span>    layer-&gt;typeindex = index;    <span class="hljs-comment">// 设置layer的类型index</span>    <span class="hljs-keyword">return</span> layer;&#125;<span class="hljs-comment">// 根据字符串layer类型创建layer -&gt; 调用上面两个函数， 创建layer</span><span class="hljs-function">Layer* <span class="hljs-title">create_layer</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* type)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> index = layer_to_index(type);    <span class="hljs-keyword">if</span> (index == <span class="hljs-number">-1</span>)        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;     <span class="hljs-keyword">return</span> create_layer(index);&#125;</code></pre><p>line 18 有个layer_registry，其定义为：</p><pre><code class="hljs cpp"><span class="hljs-keyword">static</span> <span class="hljs-keyword">const</span> layer_registry_entry layer_registry[] =&#123;    #include <span class="hljs-string">&quot;layer_registry.h&quot;</span>&#125;;</code></pre><p>而这个”layer_registry.h”文件是在build项目的时候自动产生的，部分内容如下：</p><pre><code class="hljs cpp"> <span class="hljs-comment">// Layer Registry header</span> <span class="hljs-comment">//</span> <span class="hljs-comment">// This file is auto-generated by cmake, don&#x27;t edit it.</span>  <span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span> &#123;<span class="hljs-string">&quot;AbsVal&quot;</span>,AbsVal_final_layer_creator&#125;, <span class="hljs-meta">#<span class="hljs-meta-keyword">else</span></span> &#123;AbsVal_final_layer_creator&#125;, <span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span> <span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>&#123;<span class="hljs-string">&quot;ArgMax&quot;</span>,<span class="hljs-number">0</span>&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">else</span></span>&#123;<span class="hljs-number">0</span>&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span><span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>&#123;<span class="hljs-string">&quot;BatchNorm&quot;</span>,BatchNorm_final_layer_creator&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">else</span></span>&#123;BatchNorm_final_layer_creator&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span><span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>&#123;<span class="hljs-string">&quot;Bias&quot;</span>,Bias_final_layer_creator&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">else</span></span>&#123;Bias_final_layer_creator&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span></code></pre><p>而 layer_registry_entry 的结构为：</p><pre><code class="hljs cpp"><span class="hljs-comment">// layer factory function</span><span class="hljs-keyword">typedef</span> Layer* (*layer_creator_func)(); <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">layer_registry_entry</span></span><span class="hljs-class">&#123;</span><span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>    <span class="hljs-comment">// layer type name</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* name;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>    <span class="hljs-comment">// layer factory entry</span>    layer_creator_func creator;&#125;;</code></pre><p>我们代入一组参数进去就是：</p><pre><code class="hljs cpp">name = <span class="hljs-string">&quot;AbsVal&quot;</span>;layer_creator_func = AbsVal_final_layer_creator;</code></pre><p>这里layer_creator_func定义为：</p><pre><code class="hljs cpp"><span class="hljs-keyword">typedef</span> Layer* (*layer_creator_func)();</code></pre><p>那么，layer_creator_func AbsVal_final_layer_creator转换过去就是： </p><pre><code class="hljs isbl"><span class="hljs-variable">Layer</span>* <span class="hljs-function"><span class="hljs-title">AbsVal_final_layer_creator</span>()</span></code></pre><p>在layer.h文件最下面还有一个定义：</p><pre><code class="hljs cpp"><span class="hljs-comment">// ## 字符串连接</span><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> DEFINE_LAYER_CREATOR(name) \</span>    ::ncnn::Layer* name##_layer_creator() &#123; <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> name; &#125;</code></pre><p>我们在absval.cpp文件中可以看到 DEFINE_LAYER_CREATOR(AbsVal)，相当于就是声明了一个函数：</p><pre><code class="hljs cpp"><span class="hljs-comment">// #define DEFINE_LAYER_CREATOR(name) \</span><span class="hljs-comment">//    ::ncnn::Layer* name##_layer_creator() &#123; return new name; &#125;</span><span class="hljs-comment">// 由上面这段代码可知，DEFINE_LAYER_CREATOR(AbsVal)等价于：</span>::<span class="hljs-function">ncnn::Layer* <span class="hljs-title">AbsVal_layer_creator</span><span class="hljs-params">()</span> </span>&#123; <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> AbsVal; &#125;</code></pre><p>​    上面那句话相当于就是new了一个AbsVal层，但是这里还是对应不起来，上面的是 AbsVal_final_layer_creator()，这里声明的是AbsVal_layer_creator()，这里就涉及到ncnn还有一层继承，使用cmake编译ncnn项目后，除了生成了layer_registry.h文件之外，还生成了一个layer_declaration.h文件，打开这个文件，一切就清楚了：</p><pre><code class="hljs cpp"><span class="hljs-comment">// Layer Declaration header</span><span class="hljs-comment">//</span><span class="hljs-comment">// This file is auto-generated by cmake, don&#x27;t edit it.</span> <span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer/absval.h&quot;</span></span><span class="hljs-keyword">namespace</span> ncnn &#123;    <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AbsVal_final</span> :</span> <span class="hljs-keyword">virtual</span> <span class="hljs-keyword">public</span> AbsVal&#123;<span class="hljs-keyword">public</span>:    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">create_pipeline</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Option&amp; opt)</span> </span>&#123;        &#123; <span class="hljs-keyword">int</span> ret = AbsVal::create_pipeline(opt); <span class="hljs-keyword">if</span> (ret) <span class="hljs-keyword">return</span> ret; &#125;        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    &#125;    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">destroy_pipeline</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Option&amp; opt)</span> </span>&#123;        &#123; <span class="hljs-keyword">int</span> ret = AbsVal::destroy_pipeline(opt); <span class="hljs-keyword">if</span> (ret) <span class="hljs-keyword">return</span> ret; &#125;        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    &#125;&#125;;DEFINE_LAYER_CREATOR(AbsVal_final)&#125; <span class="hljs-comment">// namespace ncnn</span> <span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer/batchnorm.h&quot;</span></span><span class="hljs-keyword">namespace</span> ncnn &#123;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BatchNorm_final</span> :</span> <span class="hljs-keyword">virtual</span> <span class="hljs-keyword">public</span> BatchNorm&#123;<span class="hljs-keyword">public</span>:    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">create_pipeline</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Option&amp; opt)</span> </span>&#123;        &#123; <span class="hljs-keyword">int</span> ret = BatchNorm::create_pipeline(opt); <span class="hljs-keyword">if</span> (ret) <span class="hljs-keyword">return</span> ret; &#125;        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    &#125;    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">destroy_pipeline</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Option&amp; opt)</span> </span>&#123;        &#123; <span class="hljs-keyword">int</span> ret = BatchNorm::destroy_pipeline(opt); <span class="hljs-keyword">if</span> (ret) <span class="hljs-keyword">return</span> ret; &#125;        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    &#125;&#125;;DEFINE_LAYER_CREATOR(BatchNorm_final)&#125; <span class="hljs-comment">// namespace ncnn</span></code></pre><p>​    AbsVal_final层继承了AbsVal层，如果当前操作系统不是linux系统，就会将create_pipeline()和destroy_pipeline()抽象出来，具体调用时，就调用对应优化了的代码。那么layer载入ParamDict具体实现就对应于各个layer的载入流程了。</p><h6 id="3-3-bin文件"><a href="#3-3-bin文件" class="headerlink" title="3.3 bin文件"></a>3.3 bin文件</h6><p>​    前面已经大致总结了ncnn的param文件载入，根据param文件创建网络结构，然后通过bin文件载入每一层对应的网络参数。这里就总结一下，如何载入每一层的参数：</p><p>​        我们常用的网络参数载入的接口为：</p><pre><code class="hljs reasonml"><span class="hljs-comment">// 从二进制文件中载入模型</span><span class="hljs-built_in">int</span> load<span class="hljs-constructor">_model(<span class="hljs-params">const</span> <span class="hljs-params">char</span><span class="hljs-operator">*</span> <span class="hljs-params">modelpath</span>)</span>;</code></pre><p>​    找到对应net.cpp文件实现部分有：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 从二进制文件中载入模型</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Net::load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* modelpath)</span></span><span class="hljs-function"></span>&#123;    FILE* fp = fopen(modelpath, <span class="hljs-string">&quot;rb&quot;</span>);    <span class="hljs-keyword">if</span> (!fp)    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;fopen %s failed\n&quot;</span>, modelpath);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;     <span class="hljs-keyword">int</span> ret = load_model(fp);     fclose(fp);     <span class="hljs-keyword">return</span> ret;&#125;</code></pre><p>和载入模型参数一样，ncnn模型载入这里调用了另外一个接口，从文件指针载入权重参数：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 从文件指针载入模型</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Net::load_model</span><span class="hljs-params">(FILE* fp)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">if</span> (layers.empty()) <span class="hljs-comment">// 判断当前layer是否为空</span>    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;network graph not ready\n&quot;</span>);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;        <span class="hljs-keyword">int</span> ret = <span class="hljs-number">0</span>;     <span class="hljs-comment">// load file</span>    <span class="hljs-function">ModelBinFromStdio <span class="hljs-title">mb</span><span class="hljs-params">(fp)</span></span>;     <span class="hljs-comment">// 从二进制文件读取</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i=<span class="hljs-number">0</span>; i&lt;layers.size(); i++)     <span class="hljs-comment">// 遍历所有的层</span>    &#123;        Layer* layer = layers[i]; <span class="hljs-comment">// 读取第i层</span>        <span class="hljs-comment">//Here we found inconsistent content in the parameter file.</span>        <span class="hljs-keyword">if</span> (!layer)&#123;    <span class="hljs-comment">// 如果第i层不存在</span>            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;load_model error at layer %d, parameter file has inconsistent content.\n&quot;</span>, (<span class="hljs-keyword">int</span>)i);            ret = <span class="hljs-number">-1</span>;            <span class="hljs-keyword">break</span>;        &#125;         <span class="hljs-comment">// 载入模型参数</span>        <span class="hljs-keyword">int</span> lret = layer-&gt;load_model(mb);        <span class="hljs-keyword">if</span> (lret != <span class="hljs-number">0</span>)        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;layer load_model %d failed\n&quot;</span>, (<span class="hljs-keyword">int</span>)i);            ret = <span class="hljs-number">-1</span>;            <span class="hljs-keyword">break</span>;        &#125;         <span class="hljs-keyword">int</span> cret = layer-&gt;create_pipeline(opt);  <span class="hljs-comment">// 从opt处创建网络的pipline</span>        <span class="hljs-keyword">if</span> (cret != <span class="hljs-number">0</span>) <span class="hljs-comment">// 如果创建第i层的pipline失败</span>        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;layer create_pipeline %d failed\n&quot;</span>, (<span class="hljs-keyword">int</span>)i);            ret = <span class="hljs-number">-1</span>;            <span class="hljs-keyword">break</span>;        &#125;    &#125;     <span class="hljs-comment">// 网络复用</span>    fuse_network();    <span class="hljs-keyword">return</span> ret;&#125;</code></pre><p>​    按照代码注释，应该还是比较好懂得，这里需要解析两个部分，第一个部分为<strong>ModelBinFromStdio</strong>，对应于二进制模型文件解析，另外一部分为 <strong>layer-&gt;load_model(mb)</strong>，对应于具体某个层的参数载入：</p><p>​    （1）二进制模型文件解析</p><p>​    这里对应于modelbin.h和modelbin.cpp文件，首先看一下modelbin.h文件：</p><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ModelBin</span></span><span class="hljs-class">&#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-keyword">virtual</span> ~ModelBin();    <span class="hljs-comment">// element type</span>    <span class="hljs-comment">// 0 = auto</span>    <span class="hljs-comment">// 1 = float32</span>    <span class="hljs-comment">// 2 = float16</span>    <span class="hljs-comment">// 3 = int8</span>    <span class="hljs-comment">// load vec</span>    <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span> </span>= <span class="hljs-number">0</span>;    <span class="hljs-comment">// load image</span>    <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> h, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span>;    <span class="hljs-comment">// load dim</span>    <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> h, <span class="hljs-keyword">int</span> c, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span>;&#125;; <span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STDIO</span><span class="hljs-comment">// 载入模型参数到一个Mat中</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ModelBinFromStdio</span> :</span> <span class="hljs-keyword">public</span> ModelBin&#123;<span class="hljs-keyword">public</span>:    <span class="hljs-comment">// construct from file</span>    ModelBinFromStdio(FILE* binfp);     <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span>; <span class="hljs-keyword">protected</span>:    FILE* binfp;&#125;;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STDIO</span></span> <span class="hljs-comment">// 载入模型参数到一个Mat中</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ModelBinFromMemory</span> :</span> <span class="hljs-keyword">public</span> ModelBin&#123;<span class="hljs-keyword">public</span>:    <span class="hljs-comment">// construct from external memory</span>    ModelBinFromMemory(<span class="hljs-keyword">const</span> <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>*&amp; mem);     <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span>; <span class="hljs-keyword">protected</span>:    <span class="hljs-keyword">const</span> <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>*&amp; mem;&#125;; <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ModelBinFromMatArray</span> :</span> <span class="hljs-keyword">public</span> ModelBin&#123;<span class="hljs-keyword">public</span>:    <span class="hljs-comment">// construct from weight blob array</span>    ModelBinFromMatArray(<span class="hljs-keyword">const</span> Mat* weights);     <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span>; <span class="hljs-keyword">protected</span>:    <span class="hljs-keyword">mutable</span> <span class="hljs-keyword">const</span> Mat* weights;&#125;;</code></pre><p>   找到对应实现部分，就是modelbin.cpp，可以看到，ModelBinFromStdio mb(fp);就是将文件指针传给binfp对象</p><pre><code class="hljs cpp">ModelBinFromStdio::ModelBinFromStdio(FILE* _binfp) : binfp(_binfp)&#123;&#125;</code></pre><p>​    下面再看一下layer载入参数，layer具体操作对应于具体类型的层操作，例如batchnorm，可以看到：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 载入模型</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">BatchNorm::load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// slope数据</span>    slope_data = mb.load(channels, <span class="hljs-number">1</span>);    <span class="hljs-comment">// 载入失败：返还-100</span>    <span class="hljs-keyword">if</span> (slope_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;     <span class="hljs-comment">// mean数据</span>    mean_data = mb.load(channels, <span class="hljs-number">1</span>);    <span class="hljs-comment">// 载入数据失败，返还-100</span>    <span class="hljs-keyword">if</span> (mean_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;     <span class="hljs-comment">// variance数据</span>    var_data = mb.load(channels, <span class="hljs-number">1</span>);    <span class="hljs-comment">// 载入数据失败，返还-100</span>    <span class="hljs-keyword">if</span> (var_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;     <span class="hljs-comment">// bias数据</span>    bias_data = mb.load(channels, <span class="hljs-number">1</span>);    <span class="hljs-comment">// 载入数据失败，返还-100</span>    <span class="hljs-keyword">if</span> (bias_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;     <span class="hljs-comment">// 创建矩阵</span>    a_data.create(channels);    <span class="hljs-keyword">if</span> (a_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    <span class="hljs-comment">// 创建矩阵</span>    b_data.create(channels);    <span class="hljs-keyword">if</span> (b_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;     <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;channels; i++)    &#123;        <span class="hljs-comment">// sqrt variance</span>        <span class="hljs-keyword">float</span> sqrt_var = <span class="hljs-built_in">sqrt</span>(var_data[i] + eps);        a_data[i] = bias_data[i] - slope_data[i] * mean_data[i] / sqrt_var;        b_data[i] = slope_data[i] / sqrt_var;    &#125;     <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>实际上调用的是ModelBinFromStdio 的load接口：</p><pre><code class="hljs cpp"><span class="hljs-function">Mat <span class="hljs-title">ModelBinFromStdio::load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span></code></pre><p>后面type对应有四种类型：auto，float32，float16和int8</p><pre><code class="hljs cpp"><span class="hljs-comment">// 0 = auto</span><span class="hljs-comment">// 1 = float32</span><span class="hljs-comment">// 2 = float16</span><span class="hljs-comment">// 3 = int8</span></code></pre><p> 然后，根据这四种类型进行模型参数载入，感觉没什么好说的，主要是里面有个alignSize函数需要做个笔记：</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">inline</span> <span class="hljs-keyword">size_t</span> <span class="hljs-title">alignSize</span><span class="hljs-params">(<span class="hljs-keyword">size_t</span> sz, <span class="hljs-keyword">int</span> n)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">return</span> (sz + n<span class="hljs-number">-1</span>) &amp; -n;&#125;</code></pre><p>alignSize就是申请sz大小的内存，实际申请内存是 y =(sz+n-1)&amp;-n 大小的内存，y &gt;= sz，且y是n的整数倍，然后对(sz+n-1)&amp; -n的解释是：</p><p>​    假设n为16，-n就是0xfffffff0，(sz+n-1)，加这个n-1一是为了保证sz刚好是16的倍数不会多算，二十为了防止不是16的倍数会少算，如，sz=3, 就是从二进制角度舍弃19小于16部分。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_0</title>
    <link href="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-0/"/>
    <url>/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-0/</url>
    
    <content type="html"><![CDATA[<p>ncnn 源码分析第一篇</p><a id="more"></a><p>ncnn 是一个架构比较简单的深度学习推导框架。 其项目目录如下所示:</p><pre><code class="hljs ncnn">.├── CMakeLists.txt├── CONTRIBUTING.md├── Info.plist├── LICENSE.txt├── README.md├── benchmark├── build.sh├── cmake├── docs    &#x2F;&#x2F; 文档├── examples    &#x2F;&#x2F; 实例├── images   &#x2F;&#x2F; 图片├── package.sh├── src      &#x2F;&#x2F; 主要的源代码│   ├── arm &#x2F;&#x2F; 针对于 arm 平台的优化│   ├── mips &#x2F;&#x2F; 针对于 mips 平台的优化│   ├── vulkan &#x2F;&#x2F; GPU 优化代码│   ├── x86 &#x2F;&#x2F; 针对于 x86 平台的优化├── ... &#x2F;&#x2F; 通用平台代码└── xxx.h├── tests  &#x2F;&#x2F; 测试├── toolchains└── tools   &#x2F;&#x2F; 工具: (1) 其他模型 -&gt; ncnn  (2) 模型优化和量化</code></pre><p>本系列旨在于去深入了解 ncnn 的内部机理， 主要分为如下几个部分：</p><p>ncnn 源码分析_1  参数与模型载入</p><p>ncnn 源码分析_2 Extractor</p><p>ncnn 源码分析_3 模型量化原理</p><p>ncnn 源码分析_4 模型量化源码</p><p>ncnn 源码分析_5 添加 layer</p><p>ncnn 源码分析_6 层分析</p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>mxnet2ncnn</title>
    <link href="/2020/09/17/mxnet2ncnn/"/>
    <url>/2020/09/17/mxnet2ncnn/</url>
    
    <content type="html"><![CDATA[<p>mxnet2ncnn 部署方案</p><a id="more"></a><h4 id="1-下载并编译-ncnn-框架"><a href="#1-下载并编译-ncnn-框架" class="headerlink" title="1. 下载并编译 ncnn 框架"></a>1. 下载并编译 ncnn 框架</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> install xcode and protobuf</span><span class="hljs-meta">#</span><span class="bash"> install protobuf</span><span class="hljs-meta">$</span><span class="bash"> brew install protobuf</span><span class="hljs-meta">#</span><span class="bash"> build &amp; install ncnn</span><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> &lt;ncnn-root-dir&gt;</span><span class="hljs-meta">$</span><span class="bash"> mkdir -p build</span><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> build</span><span class="hljs-meta">$</span><span class="bash"> cmake -DNCNN_VULKAN=OFF ..</span><span class="hljs-meta">$</span><span class="bash"> make -j4</span><span class="hljs-meta">$</span><span class="bash"> make install</span></code></pre><h4 id="2-model-slim"><a href="#2-model-slim" class="headerlink" title="2. model slim"></a>2. model slim</h4><p>从如下地址下载 mobileface 的文件：</p><p><a href="https://pan.baidu.com/s/1If28BkHde4fiuweJrbicVA"><strong>https://pan.baidu.com/s/1If28BkHde4fiuweJrbicVA</strong></a></p><p>解压可以得到三个文件：  log、model-0000.params 和 model-symbol.json</p><p>执行：</p><pre><code class="hljs shell">python model_slim.py  --model  ../models/model-y1-test2/model,0</code></pre><p>此时 会生成两个文件：../models/model-y1-test2/models-</p><h4 id="3-使用-mxnet2ncnn-工具来转化模型"><a href="#3-使用-mxnet2ncnn-工具来转化模型" class="headerlink" title="3. 使用 mxnet2ncnn 工具来转化模型"></a>3. 使用 mxnet2ncnn 工具来转化模型</h4><pre><code class="hljs shell">cd ncnn/build/tools/mxnet./mxnet2ncnn models-symbol.json models-0000.params mobilefacenet.param mobilefacenet.bin</code></pre><p>生成两个新的文件 mobilefacenet.param ，mobilefacenet.bin，这就是我们部署 ncnn 需要用到的文件。</p><h4 id="4-项目构建"><a href="#4-项目构建" class="headerlink" title="4.  项目构建"></a>4.  项目构建</h4><pre><code class="hljs crystal">arcface 我们的主项目文件image  存储测试图像文件models  存储我们的转换完成的 人脸检测、人脸识别模型ncnn  存放 ncnn 的 <span class="hljs-class"><span class="hljs-keyword">lib</span> 和 <span class="hljs-title">include</span> 文件</span>Makefile 文件</code></pre><p>其中 arcface 存放相关文件和 Makefile 文件。进入主项目文件 make 然后执行 ./main 即可。</p><p>需要重点研究的文件：（1） model_slim.py   （2）arcface 下的 四个 文件 arcface/mtcnn/base/Makefile 文件。</p><p>​    ncnn只是一个前向推断的库，利用它可以快速完成前向传播。在项目中，我们将图片传给MTCNN， 分别经过三个网络Pnet、RNet、ONet，然后产生待检测的人脸， 然后再将待检测的人脸送给facent， 最后比对产生的128D向量的距离。这样就可以完成人脸的识别。具体结合代码来说：</p><p>下面这个函数是MTCNN的主要的接口文件：</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;FaceInfo&gt; <span class="hljs-title">MtcnnDetector::Detect</span><span class="hljs-params">(ncnn::Mat img)</span></span>&#123;    <span class="hljs-keyword">int</span> img_w = img.w;  <span class="hljs-comment">// 获取 宽度</span>    <span class="hljs-keyword">int</span> img_h = img.h; <span class="hljs-comment">// 获取高度</span>    <span class="hljs-built_in">vector</span>&lt;FaceInfo&gt; pnet_results = Pnet_Detect(img);  <span class="hljs-comment">// 让图片经过PNet</span>    doNms(pnet_results, <span class="hljs-number">0.7</span>, <span class="hljs-string">&quot;union&quot;</span>);  <span class="hljs-comment">// 对输出的结果进行nms</span>    refine(pnet_results, img_h, img_w, <span class="hljs-literal">true</span>);  <span class="hljs-comment">// refine，把结果放在 pnet_results 中</span>    <span class="hljs-built_in">vector</span>&lt;FaceInfo&gt; rnet_results = Rnet_Detect(img, pnet_results);  <span class="hljs-comment">// 让图片经过 RNet</span>    doNms(rnet_results, <span class="hljs-number">0.7</span>, <span class="hljs-string">&quot;union&quot;</span>); <span class="hljs-comment">// 对输出结果进行 nms</span>    refine(rnet_results, img_h, img_w, <span class="hljs-literal">true</span>); <span class="hljs-comment">// refine， 把结果放在 Rnet_results中</span>    <span class="hljs-built_in">vector</span>&lt;FaceInfo&gt; onet_results = Onet_Detect(img, rnet_results);  <span class="hljs-comment">// 把图片 经过 Onet</span>    refine(onet_results, img_h, img_w, <span class="hljs-literal">false</span>);  <span class="hljs-comment">// refine</span>    doNms(onet_results, <span class="hljs-number">0.7</span>, <span class="hljs-string">&quot;min&quot;</span>);  <span class="hljs-comment">//  对最后的结果进行refine</span>    Lnet_Detect(img, onet_results);  <span class="hljs-comment">// 后处理</span>    <span class="hljs-keyword">return</span> onet_results;  <span class="hljs-comment">// 返回检测结果</span>&#125;</code></pre><p>这个文件是 facenet 的前向推断框架：</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; <span class="hljs-title">Arcface::getFeature</span><span class="hljs-params">(ncnn::Mat img)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; feature;    ncnn::Mat in = resize(img, <span class="hljs-number">112</span>, <span class="hljs-number">112</span>); <span class="hljs-comment">// 将人脸调整到 112x112</span>    in = bgr2rgb(in); <span class="hljs-comment">// 通道变幻</span>    ncnn::Extractor ex = net.create_extractor();  <span class="hljs-comment">// 设置提取器</span>    ex.set_light_mode(<span class="hljs-literal">true</span>);      ex.input(<span class="hljs-string">&quot;data&quot;</span>, in);  <span class="hljs-comment">// 让图片进入网络</span>    ncnn::Mat out;    ex.extract(<span class="hljs-string">&quot;fc1&quot;</span>, out);  <span class="hljs-comment">// 提取出 fc1 层的结果</span>    feature.resize(<span class="hljs-keyword">this</span>-&gt;feature_dim);  <span class="hljs-comment">// 结果resize</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-keyword">this</span>-&gt;feature_dim; i++)         feature[i] = out[i];      normalize(feature); <span class="hljs-comment">// 正则化特征</span>    <span class="hljs-keyword">return</span> feature; <span class="hljs-comment">// 返回特征</span>&#125;</code></pre><p>下面这个文件是 main.cpp， 现在看来非常简单了</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">int</span> argc, <span class="hljs-keyword">char</span>* argv[])</span></span><span class="hljs-function"></span>&#123;    Mat img1;    Mat img2;    img1 = imread(<span class="hljs-string">&quot;../image/gyy1.jpeg&quot;</span>);  <span class="hljs-comment">// 读取图片</span>    img2 = imread(<span class="hljs-string">&quot;../image/gyy2.jpeg&quot;</span>);    ncnn::Mat ncnn_img1 = ncnn::Mat::from_pixels(img1.data, ncnn::Mat::PIXEL_BGR, img1.cols, img1.rows);  <span class="hljs-comment">// 转换为 ncnn 格式</span>    ncnn::Mat ncnn_img2 = ncnn::Mat::from_pixels(img2.data, ncnn::Mat::PIXEL_BGR, img2.cols, img2.rows);    <span class="hljs-comment">// 检测</span>    <span class="hljs-function">MtcnnDetector <span class="hljs-title">detector</span><span class="hljs-params">(<span class="hljs-string">&quot;../../models&quot;</span>)</span></span>;     <span class="hljs-built_in">vector</span>&lt;FaceInfo&gt; results1 = detector.Detect(ncnn_img1);    <span class="hljs-built_in">vector</span>&lt;FaceInfo&gt; results2 = detector.Detect(ncnn_img2);    <span class="hljs-comment">// 处理检测结果，其实就是提取图片</span>    ncnn::Mat det1 = preprocess(ncnn_img1, results1[<span class="hljs-number">0</span>]);    ncnn::Mat det2 = preprocess(ncnn_img2, results2[<span class="hljs-number">0</span>]);     <span class="hljs-function">Arcface <span class="hljs-title">arc</span><span class="hljs-params">(<span class="hljs-string">&quot;../../models&quot;</span>)</span></span>;    <span class="hljs-comment">// 提取特征</span>    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; feature1 = arc.getFeature(det1);    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; feature2 = arc.getFeature(det2);    <span class="hljs-comment">// 特征比较</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">cout</span> &lt;&lt; <span class="hljs-string">&quot;Similarity: &quot;</span> &lt;&lt; calcSimilar(feature1, feature2) &lt;&lt; <span class="hljs-built_in">std</span>::<span class="hljs-built_in">endl</span>;;    imshow(<span class="hljs-string">&quot;det1&quot;</span>, ncnn2cv(det1));    imshow(<span class="hljs-string">&quot;det2&quot;</span>, ncnn2cv(det2));    waitKey(<span class="hljs-number">0</span>);    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>移动端部署要点</p><p>主要的解决方案：将MTCNN 和 facenet 的接口组织成为一个人脸 face_reg 的接口，然后使用 objective-c 调用该C++ 代码，然后在使用swift 调用 objective-c 的代码。之所以这样做是因为，swift 不能调用 C++ 呀。</p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>rk3399系统烧写</title>
    <link href="/2020/08/18/rk3399%E7%B3%BB%E7%BB%9F%E7%83%A7%E5%86%99/"/>
    <url>/2020/08/18/rk3399%E7%B3%BB%E7%BB%9F%E7%83%A7%E5%86%99/</url>
    
    <content type="html"><![CDATA[<h4 id="一-相关工具的准备"><a href="#一-相关工具的准备" class="headerlink" title="一. 相关工具的准备"></a>一. 相关工具的准备</h4><ol><li>下载 烧写工具:</li></ol><pre><code class="hljs shell">git clone https://github.com/rockchip-linux/rkbin.git</code></pre><ol><li>下载相关的 ubuntu 镜像。下载地址  <a href="https://github.com/lanseyujie/tn3399_v3/release">https://github.com/lanseyujie/tn3399_v3/release</a></li></ol><h4 id="二-刷机"><a href="#二-刷机" class="headerlink" title="二. 刷机"></a>二. 刷机</h4><ol><li>板卡进入刷机模式</li></ol><p>连接 usb-c 和 PC 主机。断开电源， 按住 recovey 键， 然后插上电源， 让板卡进入刷机模式。</p><ol><li><p>短接板卡反面的两个触点， 进入 maskrom 模式。</p></li><li><p>执行如下命令， 将相应的镜像刷入 板卡</p><pre><code class="hljs apache"><span class="hljs-attribute">sudo</span> rkdeveloptool db rk<span class="hljs-number">3399</span>_loader_v<span class="hljs-number">1</span>.<span class="hljs-number">24</span>.<span class="hljs-number">126</span>.bin<span class="hljs-attribute">sudo</span> rkdeveloptool ef<span class="hljs-attribute">sudo</span> rkdeveloptool wl <span class="hljs-number">0</span>x<span class="hljs-number">0</span> system.img</code></pre></li></ol><h4 id="三-登录"><a href="#三-登录" class="headerlink" title="三. 登录"></a>三. 登录</h4><ol><li><p>重新启动开电脑， 就可以进入 ubuntu 系统， 默认用户名是 root  密码是 1234</p></li><li><p>分区扩容</p><pre><code class="hljs shell">sudo apt install -y partedsudo parted /dev/mmcblk2unit sprintresizepart 5 100%printQsudo resize2fs /dev/mmcblk2p5</code></pre></li><li><p>可以安装 相关的桌面系统， ubuntu 新增账号循环登陆桌面。只是在添加新用户的时候需要使用</p></li></ol><pre><code class="hljs shell">useradd -m usrname # 加 m 参数会创建一个同名文件夹， 如果没有则会导致循环登录</code></pre><h4 id="参考网址"><a href="#参考网址" class="headerlink" title="参考网址:"></a>参考网址:</h4><p><a href="https://github.com/lanseyujie/tn3399_v3">https://github.com/lanseyujie/tn3399_v3</a>     # 很齐全的资料包</p><p><a href="https://naivekun.tk/">https://naivekun.tk/</a></p>]]></content>
    
    
    <categories>
      
      <category>嵌入式相关</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Classification_neural_network</title>
    <link href="/2019/04/13/Classification-neural-network/"/>
    <url>/2019/04/13/Classification-neural-network/</url>
    
    <content type="html"><![CDATA[<p>本文主要梳理了四种主要常见的分类网络 alexnet、vgg、inception、resnet。</p><a id="more"></a><h3 id="一-较为基础的分类网络"><a href="#一-较为基础的分类网络" class="headerlink" title="一. 较为基础的分类网络"></a>一. 较为基础的分类网络</h3><h4 id="1-Alexnet"><a href="#1-Alexnet" class="headerlink" title="1. Alexnet"></a>1. Alexnet</h4><p>Alexnet 将 LeNet 的思想发扬光大，把CNN 的基本原理应用到了很深很宽的网络中。</p><p>AlexNet 主要用到的新技术点如下：</p><p>（1） 成功<strong>使用ReLU作为CNN 的激活函数</strong>，并验证其效果在较深的网络超过 Sigmoid， 成功解决了Sigmoid在网络较深时的梯度弥散问题。</p><p>（2） 训练时<strong>使用Dropout随机忽略一部分神经元，以避免过拟合</strong>。</p><p>（3）在CNN中<strong>使用重叠的最大池化</strong>。此前CNN 中普遍采用平均池化,AlexNet 全部使用最大池化，避免平均池化的模糊化效果。并且 AlexNet 中提出让步长比池化核的尺寸小，这样池化层的输出之间有重叠和覆盖，提高了特征的丰富性。</p><p>（4）提出<strong>LRN层，对局部神经元的活动创建竞争机制</strong>，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强模型的泛化能力。</p><p>（5） <strong>使用CUDA加速深度卷积网络的训练</strong>，利用GPU强大的并行能力，处理神经网络训练时大量的矩阵运算。</p><p>（6）<strong>数据增强</strong>，随机地从 256 x 256 的原始图像中截取 224 x 224 大小的区域（以及水平翻转的镜像）相当于增加了(256-224)^2 * 2 = 2048 倍的数据量。大大减轻了模型过拟合，提升泛化能力。同时 AlexNet 论文中提到了会对图像的RGB 数据进行PCA 处理，并对主成分做一个标准差为0.1的高斯扰动， 增加一些噪声。</p><p><img src="/2019/04/13/Classification-neural-network/Alexnet.png" style="zoom:35%;"></p><h4 id="2-VGGNet"><a href="#2-VGGNet" class="headerlink" title="2. VGGNet"></a>2. VGGNet</h4><p><strong>VGGNet探索了卷积神经网络的深度与其性能之间的关系，通过反复堆叠3x3的小型卷积核和2x2 的最大池化层， VGGnet成功构筑了16~19层深的卷积神经网络</strong>。</p><p>（1）<strong>通过将多个卷积层堆叠在一起，可以减少参数数目的同时增加卷积层的非线性变换，使得CNN 对特征的学习能力更强</strong>。</p><p>（2）VGGNet 在训练时有个小技巧，先训练级别A 的简单网络，再复用A网络的权重来初始化后面几个复杂模型，这样训练收敛的速度更快。</p><p>（3）在测试，VGG 采用了 Multi-Scale 的方法，将 图像scale到一个尺寸Q， 并将图片输入卷积网络运算。再将不同尺寸Q的结果平均得到最后结果，这样提高图片数据的利用率并提升准确率。同时在训练中还是用了Multi-Scale 的方法做数据增强。</p><p><img src="/2019/04/13/Classification-neural-network/vgg.jpg" style="zoom:50%;"></p><h4 id="3-Google-Inception-Net"><a href="#3-Google-Inception-Net" class="headerlink" title="3. Google Inception Net"></a>3. Google Inception Net</h4><p>（1）<strong>精心设计了 Inception Module提高参数的利用效率，其结构如下所示，Inception Module中包含3种不同尺寸的卷积核1个最大池化，增加了网络对不同尺度的适应性</strong>。</p><p>第一个分支对输入进行 1x1卷积，<strong>1x1卷积可以跨通道组织信息，提高网络的表达能力，同时可以对输出通道升维和降维</strong>。</p><p>第二个分支先使用了 1x1 卷积，然后连接 3x3 卷积，相当于进行两次特征变换。</p><p>第三个分支和第二个分支类似，先是使用了1x1 的卷积，然后连接 5x5 的卷积。</p><p>最后一个分支则是3x3 最大池化后直接使用1x1卷积。</p><p>Inception Module 的4个分支在最后通过一个聚合操作合并（再输出通道这个维度上聚合）</p><p><img src="/2019/04/13/Classification-neural-network/inception.png" alt="img" style="zoom:50%;"></p><p>（2） <strong>去除了最后的全连接层，用全局平均池化层来取代它</strong>。</p><h4 id="4-ResNet"><a href="#4-ResNet" class="headerlink" title="4. ResNet"></a>4. ResNet</h4><p>​    <strong>ResNet 通过调整网络结构来解决梯度消失问题</strong>（反向传播时，梯度将涉及多层参数的交叉相乘，可能会在离输入近的网络层中产生梯度消失的现象）。首先考虑两层神经网络的简单叠加，这时  x 经过两个网络层的变换得到H(x),  激活函数采用 ReLU， 如下图所示。既然离输入近的神经网络层较难训练，那么我们可以将它短接到更靠近输出的层，如下图所示。输入  经过两个神经网络变换得到F(x) , 同时也短接到两层之后，最后这个包含两层的神经网络模块输出H(x) = F(x) + x 。这样一来，F(x) 被设计为只需要拟合输入x与目标输入H(x)的残差H(x)-x ， 残差网络的名称也因此而来。如果某一层的输出已经较好的拟合了期望结果，那么多加入一层也不会使得模型变得更差，因为该层的输出将直接短接到两层之后，相当于直接学习了一个恒等映射，而跳过的两层只需要拟合上层输出和目标之间的残差即可。</p><p><img src="/2019/04/13/Classification-neural-network/resnet.png" alt="img" style="zoom:70%;"></p><pre><code class="hljs python"><span class="hljs-comment"># resnet basicblock</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BasicBlock</span>(<span class="hljs-params">nn.Module</span>):</span>    expansion = <span class="hljs-number">1</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, inplanes, planes, stride=<span class="hljs-number">1</span>, downsample=None, groups=<span class="hljs-number">1</span>,</span></span><span class="hljs-function"><span class="hljs-params">                 base_width=<span class="hljs-number">64</span>, norm_layer=None</span>):</span>        super(BasicBlock, self).__init__()        <span class="hljs-keyword">if</span> norm_layer <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:            norm_layer = nn.BatchNorm2d        <span class="hljs-keyword">if</span> groups != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> base_width != <span class="hljs-number">64</span>:            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;BasicBlock only supports groups=1 and base_width=64&#x27;</span>)        <span class="hljs-comment"># Both self.conv1 and self.downsample layers downsample the input when stride != 1</span>        self.conv1 = conv3x3(inplanes, planes, stride)        self.bn1 = norm_layer(planes)        self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)        self.conv2 = conv3x3(planes, planes)        self.bn2 = norm_layer(planes)        self.downsample = downsample        self.stride = stride    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        identity = x        out = self.conv1(x)        out = self.bn1(out)        out = self.relu(out)        out = self.conv2(out)        out = self.bn2(out)        <span class="hljs-keyword">if</span> self.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:            identity = self.downsample(x)        out += identity        out = self.relu(out)        <span class="hljs-keyword">return</span> out</code></pre><h4 id="二、简述-Inception-V1-V4-网络-🌟"><a href="#二、简述-Inception-V1-V4-网络-🌟" class="headerlink" title="二、简述 Inception V1 - V4 网络? 🌟"></a>二、简述 Inception V1 - V4 网络? <strong>🌟</strong></h4><p><strong>Inception V1(GoogLeNet)</strong> <strong>精心设计了 Inception Module 来提高参数的利用效率</strong>，该模块包含3种不同尺寸的卷积核1个最大池化，增加了网络对不同尺度的适应性。(<strong>1x1conv、1x1conv+3x3conv、1x1conv+5x5conv、3x3pool+1x1conv</strong>)</p><p><strong>Inception V2</strong> 提出来 <strong>Batch Normalization</strong>,  用来加速网络收敛。</p><p><strong>Inception v3</strong> 改进了 Inception Module， <strong>将大卷积分解为对称的堆叠小卷积(VGG)</strong>, <strong>把n*n的卷积核替换成1*n和n*1的堆叠卷积核</strong>，在降低运算量的同时增加网络的非线性，减少过拟合。</p><p><strong>Inception v4</strong> 实际上是 <strong>inception + resnet</strong>。</p><pre><code class="hljs python"><span class="hljs-comment"># Inception </span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Inception</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj</span>):</span>        super(Inception, self).__init__()        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=<span class="hljs-number">1</span>)        self.branch2 = nn.Sequential(            BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="hljs-number">1</span>),            BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)        )        self.branch3 = nn.Sequential(            BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="hljs-number">1</span>),            BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)        )        self.branch4 = nn.Sequential(            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">True</span>),            BasicConv2d(in_channels, pool_proj, kernel_size=<span class="hljs-number">1</span>)        )    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        branch1 = self.branch1(x)        branch2 = self.branch2(x)        branch3 = self.branch3(x)        branch4 = self.branch4(x)        outputs = [branch1, branch2, branch3, branch4]        <span class="hljs-keyword">return</span> torch.cat(outputs, <span class="hljs-number">1</span>)</code></pre><h3 id="三-具体阐述一下你知道的resnet的相关变种"><a href="#三-具体阐述一下你知道的resnet的相关变种" class="headerlink" title="三. 具体阐述一下你知道的resnet的相关变种 ?"></a>三. 具体阐述一下你知道的resnet的相关变种 ?</h3><h5 id="1-resnetv2"><a href="#1-resnetv2" class="headerlink" title="1. resnetv2"></a>1. resnetv2</h5><ul><li><p>将激活函数放置在旁路中, short-cut 构建 clean information path</p></li><li><p>旁路中的结构从 <strong>conv-bn-relu</strong> 转换为 <strong>bn-relu-conv</strong></p></li></ul><p><img src="/2019/04/13/Classification-neural-network/18.png" style="zoom:30%;"></p><h5 id="2-resnext"><a href="#2-resnext" class="headerlink" title="2. resnext"></a>2. resnext</h5><p>​        借鉴了 Inception split-transform-merge 的模式， 将单路卷积变成多个支路的多路卷积，不过分组结构一致，进行分组卷积。</p><p><img src="/2019/04/13/Classification-neural-network/20.png" style="zoom:30%;"></p><h5 id="3-SENet"><a href="#3-SENet" class="headerlink" title="3. SENet"></a>3. SENet</h5><p>​        采用了一种全新的<strong>「特征重标定」</strong>的策略。具体来说，<strong>就是通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。</strong></p><p><img src="/2019/04/13/Classification-neural-network/6.png" style="zoom:70%;"></p><h5 id="4-Densenet"><a href="#4-Densenet" class="headerlink" title="4. Densenet"></a>4. Densenet</h5><p>​    DenseNet 的目标是提升网络层级间信息流与梯度流的效率，并提高参数效率。它也如同 ResNet那样连接前层特征图与后层特征图，但 DenseNet 并不会像 ResNet 那样对两个特征图求和，而是直接将特征图按深度相互拼接在一起。DenseNet最大的特点即<strong>每一层的输出都会作为后面所有层的输入，这样最后一层将拼接前面所有层级的输出特征图。这种结构确保了每一层能从损失函数直接访问到梯度，因此可以训练非常深的网络</strong>。</p><h5 id="5-res2net"><a href="#5-res2net" class="headerlink" title="5. res2net"></a>5. res2net</h5><h5 id="Paper-Res2Net-A-New-Multi-scale-Backbone-Architecture"><a href="#Paper-Res2Net-A-New-Multi-scale-Backbone-Architecture" class="headerlink" title="Paper:  Res2Net: A New Multi-scale Backbone Architecture"></a>Paper:  Res2Net: A New Multi-scale Backbone Architecture</h5><h5 id="Methods"><a href="#Methods" class="headerlink" title="Methods:"></a>Methods:</h5><p>作者提出了一种在更加细粒度 (卷积层) 的层面提升多尺度表达能力。其基本结构如下图(b) 所示：</p><p><img src="/Users/zhichaozhao/Documents/hexo-blog/source/_posts/recent-convolutions/3.png" alt></p><p>传统的resnet结构如上图所示，作者在其基础上进行改进，在不增加计算量的同时，使其具备更强的多尺度提取能力。如上图（b）所示，作者采用了更小的卷积组来替代 bottleneck block 里面的 3x3 卷积。具体操作为：</p><ul><li>首先将 1x1 卷积后的特征图均分为 s 个特征图子集。每个特征图子集的大小相同，但是通道数是输入特征图的 1/s。</li><li>对每一个特征图子集 $X<em>i$，有一个对应的 3x3 卷积 $K_i$ , 假设 $K_i$的输出是 $y_i$。接下来每个特征图子集 $X_i $会加上 $K</em>{i-1}$ 的输出，然后一起输入进 $K_i$。为了在增大 s 的值时减少参数量，作者省去了 $X_1$ 的 3x3 网络。因此，输出 $y_i$ 可以用如下公式表示：</li></ul><script type="math/tex; mode=display">y_i = \begin{equation}\begin{cases}x_i && i=1;\\K_i(x_i+y_{i-1}) && 1 < i \leq s.\end{cases}\end{equation}</script><p>根据图(b)，可以发现每一个 $X_j (j&lt;=i)$ 下的 3x3 卷积可以利用之前所有的特性信息，它的输出会有比 $X_j$ 更大的感受野。因此这样的组合可以使 Res2Net 的输出有更多样的感受野信息。为了更好的融合不同尺度的信息，作者将它们的输出拼接起来，然后再送入 1x1 卷积，如上图（b）所示。</p><h3 id="四-一些问题"><a href="#四-一些问题" class="headerlink" title="四. 一些问题"></a>四. 一些问题</h3><ul><li>关于通道的求和与拼接 ?</li></ul><p>​    (1) 常见的 add 操作见于 resnet 和 FPN、CPN。 而 concat 操作见于 Unet 和 Dense net。</p><p>​    (2) add等价于concat之后对应通道共享同一个卷积核。当两路输入可以具有“对应通道的特征图语义类似” 的性质的时候，可以用add来替代concat，这样更节省参数和计算量（concat是add的2倍）。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>mobile-neural-network-engine-design</title>
    <link href="/2019/04/13/mobile-neural-network-engine-design/"/>
    <url>/2019/04/13/mobile-neural-network-engine-design/</url>
    
    <content type="html"><![CDATA[<h4 id="1-基于C-C-的基本优化"><a href="#1-基于C-C-的基本优化" class="headerlink" title="1. 基于C/C++的基本优化"></a>1. 基于C/C++的基本优化</h4><ol><li><p>编译器很牛逼，GCC/CLANG 都有运行速度的优化选项，打开这些选项能帮助程序显著提升速度，虽然这还远远不够，但聊胜于无吧。</p><p>下面是 ncnn 示例项目中的一段代码:</p><pre><code class="hljs cmake"><span class="hljs-comment"># ncnn/examples/squeezencnn/jni/Android.mk</span>LOCAL_CFLAGS := -O2 -fvisibility=hidden -fomit-frame-pointer -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-<span class="hljs-keyword">math</span>LOCAL_CPPFLAGS := -O2 -fvisibility=hidden -fvisibility-inlines-hidden -fomit-frame-pointer -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-<span class="hljs-keyword">math</span>LOCAL_LDFLAGS += -Wl,--gc-sections</code></pre><p>以前工作时候写的CMakeLists 中的代码：</p><pre><code class="hljs cmake"><span class="hljs-comment"># set(CMAKE_BUILD_TYPE Release)</span><span class="hljs-keyword">if</span> (CMAKE_BUILD_TYPE <span class="hljs-keyword">STREQUAL</span> <span class="hljs-string">&quot;Debug&quot;</span>)    <span class="hljs-keyword">set</span>(CMAKE_CXX_FLAGS_DEBUG <span class="hljs-string">&quot;$ENV&#123;CXXFLAGS&#125; -O0 -Wall -g2 -ggdb&quot;</span>)    <span class="hljs-keyword">message</span>(STATUS <span class="hljs-string">&quot;CMAKE_BUILD_TYPE = Debug&quot;</span>)<span class="hljs-keyword">else</span>()    <span class="hljs-keyword">set</span>(CMAKE_CXX_FLAGS_RELEASE <span class="hljs-string">&quot;$ENV&#123;CXXFLAGS&#125; -O3 -Wall&quot;</span>)    <span class="hljs-keyword">message</span>(STATUS <span class="hljs-string">&quot;CMAKE_BUILD_TYPE = Release&quot;</span>)<span class="hljs-keyword">endif</span>()</code></pre></li><li><p>书写高效的C代码。循环展开、内联、分支优化，避免除法，查表等优化小技巧要滚瓜烂熟，信手拈来。</p></li><li><p>必须看得懂汇编，即使你不写，也要知道编译器编译出来的汇编代码效率如何。这样你可以通过调整C/C++代码，让编译器生成你需要的代码。</p></li></ol><h4 id="2-缓存友好"><a href="#2-缓存友好" class="headerlink" title="2. 缓存友好"></a>2. 缓存友好</h4><h5 id="（1）少用内存"><a href="#（1）少用内存" class="headerlink" title="（1）少用内存"></a>（1）少用内存</h5><h5 id="（2）连续访问、对齐访问、合并访问、显示对齐数据加载、缓存预取"><a href="#（2）连续访问、对齐访问、合并访问、显示对齐数据加载、缓存预取" class="headerlink" title="（2）连续访问、对齐访问、合并访问、显示对齐数据加载、缓存预取"></a>（2）连续访问、对齐访问、合并访问、显示对齐数据加载、缓存预取</h5><h4 id="3-多线程"><a href="#3-多线程" class="headerlink" title="3. 多线程"></a>3. 多线程</h4><h5 id="1-OpenMP"><a href="#1-OpenMP" class="headerlink" title="1. OpenMP"></a>1. OpenMP</h5><p>OpenMP会自动为循环分配线程，使用OpenMP加速只需要在串行代码中添加编译指令以及少量API即可。理想情况下，加速比大约能达到0.75*cores。</p><pre><code class="hljs cpp"><span class="hljs-comment">// ncnn/src/layer/relu.cpp</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">ReLU::forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">if</span> (bottom_top_blob.elemsize == <span class="hljs-number">1u</span>)        <span class="hljs-keyword">return</span> ReLU::forward_inplace_int8(bottom_top_blob, opt);    <span class="hljs-keyword">int</span> w = bottom_top_blob.w;    <span class="hljs-keyword">int</span> h = bottom_top_blob.h;    <span class="hljs-keyword">int</span> channels = bottom_top_blob.c;    <span class="hljs-keyword">int</span> size = w * h;    <span class="hljs-keyword">if</span> (slope == <span class="hljs-number">0.f</span>)    &#123;        <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)        &#123;            <span class="hljs-keyword">float</span>* ptr = bottom_top_blob.channel(q);            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)            &#123;                <span class="hljs-keyword">if</span> (ptr[i] &lt; <span class="hljs-number">0</span>)                    ptr[i] = <span class="hljs-number">0</span>;            &#125;        &#125;    &#125;    <span class="hljs-keyword">else</span>    &#123;        #pragma omp parallel <span class="hljs-keyword">for</span> num_threads(opt.num_threads)        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)        &#123;            <span class="hljs-keyword">float</span>* ptr = bottom_top_blob.channel(q);            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)            &#123;                <span class="hljs-keyword">if</span> (ptr[i] &lt; <span class="hljs-number">0</span>)                    ptr[i] *= slope;            &#125;        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>​    但并非所有循环都适合做多线程优化，如果每次循环只做了非常少的事情，那么使用多线程会得不偿失。实际运用中，可以通过 <code>#pragma omp parallel for if (cond)</code> 语句来判断runtime过程中是否要启用多线程。</p><h5 id="动态调度"><a href="#动态调度" class="headerlink" title="动态调度"></a>动态调度</h5><h5 id="稀疏化"><a href="#稀疏化" class="headerlink" title="稀疏化"></a>稀疏化</h5><h5 id="定点化"><a href="#定点化" class="headerlink" title="定点化"></a>定点化</h5><h5 id="NEON-汇编"><a href="#NEON-汇编" class="headerlink" title="NEON 汇编"></a>NEON 汇编</h5><h4 id="4-内存精简"><a href="#4-内存精简" class="headerlink" title="4. 内存精简"></a>4. 内存精简</h4><p>每个layer都会产生blob，除了最后的结果和多分支中间结果，大部分blob都可以不保留，开启ncnn的轻模式可以在运算后自动回收，省下内存。  如下图所示：</p><p><img src="/2019/04/13/mobile-neural-network-engine-design/1.png" alt>某网络结构为 <code>Conv</code> -&gt; <code>Bias</code> -&gt; <code>ReLU</code> -&gt; <code>Concat</code>，在轻模式下，向ncnn索要Concat结果时，Conv结果会在运算Bias时自动回收，而Bais结果会在运算ReLU时自动回收，而ReLU结果会在运算Concat时自动回收, 最后只保留Concat结果，后面再需要C结果会直接获得，满足绝大部分深度网络的使用方式。ncnn 开启轻模式仅需要一行代码:</p><pre><code class="hljs cpp">set_light_mode(<span class="hljs-literal">true</span>)</code></pre><p>相关参考资料：</p><p><a href="https://m.facebook.com/notes/facebook-engineering/three-optimization-tips-for-c/10151361643253920">Three-optimization-tips-for-c</a></p><p><a href="https://www.ibm.com/developerworks/cn/aix/library/au-aix-openmp-framework/index.html">通过GCC学习OpenMP框架</a></p><p><a href="https://www.openmp.org/">OPEN MP官网</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
