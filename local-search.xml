<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ncnn源码分析_6</title>
    <link href="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-6/"/>
    <url>/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-6/</url>
    
    <content type="html"><![CDATA[<p>分析了几个比较常见的算子 forward 过程， 比如 abs， bias，argmax， conv，pool， bn </p><a id="more"></a><h4 id="1-abs"><a href="#1-abs" class="headerlink" title="1. abs"></a>1. abs</h4><pre><code class="hljs c"><span class="hljs-comment">// 绝对值层特性: 单输入，单输出，可直接对输入进行修改</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">AbsVal::forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> w = bottom_top_blob.w;    <span class="hljs-keyword">int</span> h = bottom_top_blob.h;    <span class="hljs-keyword">int</span> channels = bottom_top_blob.c;    <span class="hljs-keyword">int</span> size = w * h;    <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)  <span class="hljs-comment">// openmp </span></span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)    &#123;        <span class="hljs-keyword">float</span>* ptr = bottom_top_blob.channel(q);<span class="hljs-comment">// 当前通道数据的起始指针</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)        &#123;            <span class="hljs-keyword">if</span> (ptr[i] &lt; <span class="hljs-number">0</span>)                ptr[i] = - ptr[i]; <span class="hljs-comment">// 小于零取相反数，大于零保持原样</span>        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="2-bias"><a href="#2-bias" class="headerlink" title="2. bias"></a>2. bias</h4><pre><code class="hljs c"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Bias::forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> w = bottom_top_blob.w;    <span class="hljs-keyword">int</span> h = bottom_top_blob.h;    <span class="hljs-keyword">int</span> channels = bottom_top_blob.c;    <span class="hljs-keyword">int</span> size = w * h;    <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)    &#123;        <span class="hljs-keyword">float</span>* ptr = bottom_top_blob.channel(q);  <span class="hljs-comment">// 每个通道数据起始指针</span>        <span class="hljs-keyword">float</span> bias = bias_data[q];   <span class="hljs-comment">// 需要添加的偏置数据 前面从模型中载入的参数 每通道偏置参数一样</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)        &#123;            ptr[i] += bias;<span class="hljs-comment">// 加上偏置</span>        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="2-argmax"><a href="#2-argmax" class="headerlink" title="2.  argmax"></a>2.  argmax</h4><pre><code class="hljs c"><span class="hljs-comment">// 层参数包含两个参数，第一个是是否需要包含值对应在源blob中的位置，第二个是需要前多少个最大的数</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">ArgMax::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; pd)</span></span><span class="hljs-function"></span>&#123;    out_max_val = pd.get(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);<span class="hljs-comment">// 是否 需要存储位置</span>    topk = pd.get(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>);       <span class="hljs-comment">// 在前topk个最大的数</span>       <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">ArgMax::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> size = bottom_blob.total(); <span class="hljs-comment">// 输入blob参数总数量</span>        <span class="hljs-comment">// 创建一个新的输出 blob </span>    <span class="hljs-keyword">if</span> (out_max_val)        top_blob.create(topk, <span class="hljs-number">2</span>, <span class="hljs-number">4u</span>, opt.blob_allocator); <span class="hljs-comment">// topk个值 + topk个值对应的位置</span>    <span class="hljs-keyword">else</span>        top_blob.create(topk, <span class="hljs-number">1</span>, <span class="hljs-number">4u</span>, opt.blob_allocator); <span class="hljs-comment">// 只存  topk个值，不存位置</span>    <span class="hljs-keyword">if</span> (top_blob.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* ptr = bottom_blob;        <span class="hljs-comment">// partial sort topk with index, optional value</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt; <span class="hljs-built_in">std</span>::<span class="hljs-built_in">pair</span>&lt;<span class="hljs-keyword">float</span>, <span class="hljs-keyword">int</span>&gt; &gt; vec;    vec.resize(size);    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)    &#123;        vec[i] = <span class="hljs-built_in">std</span>::<span class="hljs-built_in">make_pair</span>(ptr[i], i);<span class="hljs-comment">// 源 输入blob 的参数的 值：位置id 键值对</span>    &#125;    <span class="hljs-built_in">std</span>::partial_sort(vec.begin(), vec.begin() + topk, vec.end(),                      <span class="hljs-built_in">std</span>::greater&lt; <span class="hljs-built_in">std</span>::<span class="hljs-built_in">pair</span>&lt;<span class="hljs-keyword">float</span>, <span class="hljs-keyword">int</span>&gt; &gt;());<span class="hljs-comment">// 按第一列排序，获取前 topk个</span>    <span class="hljs-comment">// 保存前面最大的 topk 个参数</span>    <span class="hljs-keyword">float</span>* outptr = top_blob;    <span class="hljs-keyword">if</span> (out_max_val)    &#123;        <span class="hljs-keyword">float</span>* valptr = outptr + topk; <span class="hljs-comment">// 前面topk的位置存值，后面存对应值在源输入 blob 中的位置ID</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;topk; i++)        &#123;            outptr[i] = vec[i].first; <span class="hljs-comment">// 存值</span>            valptr[i] = vec[i].second;<span class="hljs-comment">// 存位置</span>        &#125;    &#125;    <span class="hljs-keyword">else</span>    &#123;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;topk; i++)        &#123;            outptr[i] = vec[i].second;<span class="hljs-comment">// 只存值</span>        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="3-concat"><a href="#3-concat" class="headerlink" title="3.concat"></a>3.concat</h4><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Concat::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; bottom_blobs, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; top_blobs, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> dims = bottom_blobs[<span class="hljs-number">0</span>].dims;    <span class="hljs-keyword">size_t</span> elemsize = bottom_blobs[<span class="hljs-number">0</span>].elemsize;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">1</span>) <span class="hljs-comment">// axis == 0</span>    &#123;        <span class="hljs-keyword">int</span> top_w = <span class="hljs-number">0</span>; <span class="hljs-comment">// 输出长度</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> b = <span class="hljs-number">0</span>; b &lt; bottom_blobs.size(); b++)        &#123;            <span class="hljs-keyword">const</span> Mat&amp; bottom_blob = bottom_blobs[b];            top_w += bottom_blob.w;        &#125;        <span class="hljs-comment">// 创建输出 blob</span>        Mat&amp; top_blob = top_blobs[<span class="hljs-number">0</span>];        top_blob.create(top_w, elemsize, opt.blob_allocator);        <span class="hljs-keyword">if</span> (top_blob.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;        <span class="hljs-comment">// 将不同的输入复制到输出</span>        <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>* outptr = top_blob;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> b = <span class="hljs-number">0</span>; b &lt; bottom_blobs.size(); b++)        &#123;            <span class="hljs-keyword">const</span> Mat&amp; bottom_blob = bottom_blobs[b];            <span class="hljs-keyword">int</span> w = bottom_blob.w;            <span class="hljs-keyword">const</span> <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>* ptr = bottom_blob;            <span class="hljs-built_in">memcpy</span>(outptr, ptr, w * elemsize);            outptr += w * elemsize;        &#125;        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">2</span> &amp;&amp; axis == <span class="hljs-number">0</span>)&#123;        <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">2</span> &amp;&amp; axis == <span class="hljs-number">1</span>)&#123;        <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">3</span> &amp;&amp; axis == <span class="hljs-number">0</span>)&#123;        <span class="hljs-comment">//  ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">3</span> &amp;&amp; axis == <span class="hljs-number">1</span>)&#123;      <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">3</span> &amp;&amp; axis == <span class="hljs-number">2</span>)&#123;       <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="4-卷积层"><a href="#4-卷积层" class="headerlink" title="4. 卷积层"></a>4. 卷积层</h4><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Convolution::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// convolv with NxN kernel</span>    <span class="hljs-comment">// value = value + bias</span>    <span class="hljs-keyword">if</span> (opt.use_int8_inference &amp;&amp; weight_data.elemsize == (<span class="hljs-keyword">size_t</span>)<span class="hljs-number">1u</span>)    &#123;        <span class="hljs-keyword">return</span> forward_int8(bottom_blob, top_blob, opt);    &#125;    <span class="hljs-comment">// flattened blob, implement as InnerProduct</span>    <span class="hljs-comment">//...</span>    <span class="hljs-keyword">int</span> w = bottom_blob.w;    <span class="hljs-keyword">int</span> h = bottom_blob.h;    <span class="hljs-keyword">int</span> channels = bottom_blob.c;    <span class="hljs-keyword">size_t</span> elemsize = bottom_blob.elemsize;    <span class="hljs-comment">// NCNN_LOGE(&quot;Convolution input %d x %d  pad = %d %d  ksize=%d %d  stride=%d %d&quot;, w, h, pad_w, pad_h, kernel_w, kernel_h, stride_w, stride_h);</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> kernel_extent_w = dilation_w * (kernel_w - <span class="hljs-number">1</span>) + <span class="hljs-number">1</span>;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> kernel_extent_h = dilation_h * (kernel_h - <span class="hljs-number">1</span>) + <span class="hljs-number">1</span>;    Mat bottom_blob_bordered;    make_padding(bottom_blob, bottom_blob_bordered, opt);    <span class="hljs-keyword">if</span> (bottom_blob_bordered.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    w = bottom_blob_bordered.w;    h = bottom_blob_bordered.h;    <span class="hljs-keyword">int</span> outw = (w - kernel_extent_w) / stride_w + <span class="hljs-number">1</span>;    <span class="hljs-keyword">int</span> outh = (h - kernel_extent_h) / stride_h + <span class="hljs-number">1</span>;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> maxk = kernel_w * kernel_h;    <span class="hljs-comment">// kernel offsets</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; _space_ofs(maxk);    <span class="hljs-keyword">int</span>* space_ofs = &amp;_space_ofs[<span class="hljs-number">0</span>];    &#123;        <span class="hljs-keyword">int</span> p1 = <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> p2 = <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> gap = w * dilation_h - kernel_w * dilation_w;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; kernel_h; i++)        &#123;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; kernel_w; j++)            &#123;                space_ofs[p1] = p2;                p1++;                p2 += dilation_w;            &#125;            p2 += gap;        &#125;    &#125;    <span class="hljs-comment">// 申请输出</span>    top_blob.create(outw, outh, num_output, elemsize, opt.blob_allocator);    <span class="hljs-keyword">if</span> (top_blob.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    <span class="hljs-comment">// num_output</span>    <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> p = <span class="hljs-number">0</span>; p &lt; num_output; p++) <span class="hljs-comment">// 逐输出通道</span>    &#123;        <span class="hljs-keyword">float</span>* outptr = top_blob.channel(p);        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; outh; i++) <span class="hljs-comment">// 输出高度</span>        &#123;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; outw; j++) <span class="hljs-comment">// 输出宽度</span>            &#123;                <span class="hljs-keyword">float</span> sum = <span class="hljs-number">0.f</span>;                <span class="hljs-keyword">if</span> (bias_term)                    sum = bias_data[p];                <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* kptr = (<span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>*)weight_data + maxk * channels * p; <span class="hljs-comment">// 卷积单元对应的起始位置</span>                <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q = <span class="hljs-number">0</span>; q &lt; channels; q++) <span class="hljs-comment">// 输入</span>                &#123;                    <span class="hljs-keyword">const</span> Mat m = bottom_blob_bordered.channel(q);                    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* sptr = m.row(i * stride_h) + j * stride_w;                    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> k = <span class="hljs-number">0</span>; k &lt; maxk; k++) <span class="hljs-comment">// 29.23</span>                    &#123;                        <span class="hljs-keyword">float</span> val = sptr[space_ofs[k]]; <span class="hljs-comment">// 20.72</span>                        <span class="hljs-keyword">float</span> w = kptr[k];                        sum += val * w; <span class="hljs-comment">// 41.45</span>                    &#125;                    kptr += maxk;                &#125;                                <span class="hljs-comment">// 激活函数</span>                <span class="hljs-comment">// ......</span>                outptr[j] = sum;            &#125;            outptr += outw;        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="5-pooling"><a href="#5-pooling" class="headerlink" title="5. pooling"></a>5. pooling</h4><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Pooling::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// max value in NxN window</span>    <span class="hljs-keyword">int</span> w = bottom_blob.w;    <span class="hljs-keyword">int</span> h = bottom_blob.h;    <span class="hljs-keyword">int</span> channels = bottom_blob.c;    <span class="hljs-keyword">size_t</span> elemsize = bottom_blob.elemsize;    <span class="hljs-keyword">if</span> (global_pooling)    &#123;        <span class="hljs-comment">// ...</span>    &#125;    Mat bottom_blob_bordered;    make_padding(bottom_blob, bottom_blob_bordered, opt);    <span class="hljs-keyword">if</span> (bottom_blob_bordered.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    w = bottom_blob_bordered.w;    h = bottom_blob_bordered.h;    <span class="hljs-keyword">int</span> outw = (w - kernel_w) / stride_w + <span class="hljs-number">1</span>;    <span class="hljs-keyword">int</span> outh = (h - kernel_h) / stride_h + <span class="hljs-number">1</span>;、        <span class="hljs-comment">// 申请输出 blob</span>    top_blob.create(outw, outh, channels, elemsize, opt.blob_allocator);    <span class="hljs-keyword">if</span> (top_blob.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> maxk = kernel_w * kernel_h;    <span class="hljs-comment">// kernel offsets</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; _space_ofs(maxk); <span class="hljs-comment">// </span>    <span class="hljs-keyword">int</span>* space_ofs = &amp;_space_ofs[<span class="hljs-number">0</span>];    &#123;        <span class="hljs-keyword">int</span> p1 = <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> p2 = <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> gap = w - kernel_w;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; kernel_h; i++)        &#123;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; kernel_w; j++)            &#123;                space_ofs[p1] = p2;                p1++;                p2++;            &#125;            p2 += gap;        &#125;    &#125;    <span class="hljs-keyword">if</span> (pooling_type == PoolMethod_MAX)    &#123;        <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q = <span class="hljs-number">0</span>; q &lt; channels; q++) <span class="hljs-comment">// 通道</span>        &#123;            <span class="hljs-keyword">const</span> Mat m = bottom_blob_bordered.channel(q);            <span class="hljs-keyword">float</span>* outptr = top_blob.channel(q);            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; outh; i++) <span class="hljs-comment">// 输出层高度</span>            &#123;                <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; outw; j++) <span class="hljs-comment">// 输出层宽度</span>                &#123;                    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* sptr = m.row(i * stride_h) + j * stride_w; <span class="hljs-comment">// 池化单元对应的起始位置</span>                    <span class="hljs-keyword">float</span> max = sptr[<span class="hljs-number">0</span>];                    <span class="hljs-comment">// 求每个池化单元对应的最大值</span>                    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> k = <span class="hljs-number">0</span>; k &lt; maxk; k++)                    &#123;                        <span class="hljs-keyword">float</span> val = sptr[space_ofs[k]];                        max = <span class="hljs-built_in">std</span>::max(max, val);                    &#125;                    outptr[j] = max;                &#125;                outptr += outw;            &#125;        &#125;    &#125;    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (pooling_type == PoolMethod_AVE)    &#123;        <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><h4 id="6-Batchnorm"><a href="#6-Batchnorm" class="headerlink" title="6. Batchnorm"></a>6. Batchnorm</h4><pre><code class="hljs c"><span class="hljs-comment">// 可以看出来， 这里的其实就是 value = b * value + a, 完全可以和 conv 进行合并</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">BatchNorm::forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// a = bias - slope * mean / sqrt(var)</span>    <span class="hljs-comment">// b = slope / sqrt(var)</span>    <span class="hljs-comment">// value = b * value + a</span>    <span class="hljs-keyword">int</span> dims = bottom_top_blob.dims;        <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">1</span>) <span class="hljs-comment">// 1维度====================</span>    &#123;        <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">2</span>) <span class="hljs-comment">// 2维度======================</span>    &#123;        <span class="hljs-comment">// ...</span>    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">3</span>) <span class="hljs-comment">// 3维度================================</span>    &#123;        <span class="hljs-keyword">int</span> w = bottom_top_blob.w;        <span class="hljs-keyword">int</span> h = bottom_top_blob.h;        <span class="hljs-keyword">int</span> size = w * h;        <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)        &#123;            <span class="hljs-keyword">float</span>* ptr = bottom_top_blob.channel(q);            <span class="hljs-keyword">float</span> a = a_data[q];            <span class="hljs-keyword">float</span> b = b_data[q];            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)            &#123;                ptr[i] = b * ptr[i] + a;            &#125;        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_5</title>
    <link href="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-5/"/>
    <url>/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-5/</url>
    
    <content type="html"><![CDATA[<a id="more"></a><p>ncnn 提供了两种解析网络 layer 的方法：</p><h4 id="方法一-在推理中进行解析"><a href="#方法一-在推理中进行解析" class="headerlink" title="方法一: 在推理中进行解析"></a>方法一: 在推理中进行解析</h4><p>模型转换之后，某些 layer 没有转换成功：(在转换的过程中)。 如 shufflenetv2 中，param 文件只转换到了 fc 这一个 layer。这时需要添加一层softmax，可以先对param文件中转换成功的layer做推理，然后手动添加一个softmax层：</p><pre><code class="hljs cpp">ncnn::Extractor ex = shufflenetv2.create_extractor();ex.input(<span class="hljs-string">&quot;data&quot;</span>, in);ncnn::Mat out;ex.extract(<span class="hljs-string">&quot;fc&quot;</span>, out);<span class="hljs-comment">// manually call softmax on the fc output</span><span class="hljs-comment">// convert result into probability</span><span class="hljs-comment">// skip if your model already has softmax operation</span>&#123;    ncnn::Layer* softmax = ncnn::create_layer(<span class="hljs-string">&quot;Softmax&quot;</span>);    ncnn::ParamDict pd;    softmax-&gt;load_param(pd);    softmax-&gt;forward_inplace(out, shufflenetv2.opt);    <span class="hljs-keyword">delete</span> softmax;&#125;out = out.reshape(out.w * out.h * out.c);</code></pre><h4 id="方法二-自定义层"><a href="#方法二-自定义层" class="headerlink" title="方法二: 自定义层"></a>方法二: 自定义层</h4><h5 id="1-新建空的类"><a href="#1-新建空的类" class="headerlink" title="1. 新建空的类"></a>1. 新建空的类</h5><pre><code class="hljs c"><span class="hljs-comment">// 在 ncnn/src/layer/ 下新建两个文件 mylayer.h mylayer.cpp</span><span class="hljs-comment">// 1.mylayer.h</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer.h&quot;</span></span><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> ncnn;      <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyLayer</span> :</span> <span class="hljs-keyword">public</span> Layer&#123;&#125;;<span class="hljs-comment">// 2. mylayer.cpp</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;mylayer.h&quot;</span></span>DEFINE_LAYER_CREATOR(MyLayer)   <span class="hljs-comment">// 注册新定义的层</span></code></pre><h5 id="2-定义层参数-parameters-和-权重-weights-并实现载入函数"><a href="#2-定义层参数-parameters-和-权重-weights-并实现载入函数" class="headerlink" title="2. 定义层参数 parameters 和 权重 weights, 并实现载入函数"></a>2. 定义层参数 parameters 和 权重 weights, 并实现载入函数</h5><pre><code class="hljs c"><span class="hljs-comment">// mylayer.h</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer.h&quot;</span></span><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> ncnn;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyLayer</span> :</span> <span class="hljs-keyword">public</span> Layer&#123;<span class="hljs-keyword">public</span>:  <span class="hljs-comment">// 公有方法</span>      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDic&amp; pd)</span></span>;<span class="hljs-comment">// 虚函数，可以被子类覆盖</span>      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span>;   <span class="hljs-comment">// </span><span class="hljs-keyword">private</span>: <span class="hljs-comment">// 私有参数</span>      <span class="hljs-keyword">int</span> channels;   <span class="hljs-comment">// 参数1 通道数量</span>      <span class="hljs-keyword">float</span> eps;      <span class="hljs-comment">// 参数2 精度</span>      Mat gamma_data; <span class="hljs-comment">// 权重</span>&#125;；<span class="hljs-comment">// 2. mylayer.cpp</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;mylayer.h&quot;</span></span>DEFINE_LAYER_CREATOR(MyLayer) <span class="hljs-comment">// 实现 load_param() 载入网络层参数</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; pd)</span></span><span class="hljs-function"></span>&#123;      <span class="hljs-comment">// 使用pd.get(key,default_val); 从param文件中（key=val）获取参数</span>      channels = pd.get(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);      <span class="hljs-comment">// 解析 0=&lt;int value&gt;, 默认为0</span>      eps      = pd.get(<span class="hljs-number">1</span>, <span class="hljs-number">0.001f</span>); <span class="hljs-comment">// 解析 1=&lt;float value&gt;, 默认为0.001f</span>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>; <span class="hljs-comment">// 载入成功返回0</span>&#125;<span class="hljs-comment">// 实现 load_model() 载入模型权重</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span><span class="hljs-function"></span>&#123;      <span class="hljs-comment">// 读取二进制数据的长度为 channels * sizeof(float)</span>      <span class="hljs-comment">// 0 自动判断数据类型， float32 float16 int8</span>      <span class="hljs-comment">// 1 按 float32读取  2 按float16读取 3 按int8读取</span>      gamma_data = mb.load(channels, <span class="hljs-number">1</span>);<span class="hljs-comment">// 按 float32读取 </span>      <span class="hljs-keyword">if</span>(gamma_data.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>; <span class="hljs-comment">// 错误返回非0数，-100表示 out-of-memory </span>      <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>; <span class="hljs-comment">//  载入成功返回0</span>&#125;</code></pre><h5 id="3-定义类构造函数，确定前向传播行为"><a href="#3-定义类构造函数，确定前向传播行为" class="headerlink" title="3.  定义类构造函数，确定前向传播行为"></a>3.  定义类构造函数，确定前向传播行为</h5><pre><code class="hljs c"><span class="hljs-comment">// mylayer.h</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer.h&quot;</span></span><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> ncnn;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyLayer</span> :</span> <span class="hljs-keyword">public</span> Layer&#123;<span class="hljs-keyword">public</span>:        MyLayer();  <span class="hljs-comment">// 构造函数</span>      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDic&amp; pd)</span></span>;      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span>;  <span class="hljs-keyword">private</span>:      <span class="hljs-keyword">int</span> channels;         <span class="hljs-keyword">float</span> eps;       Mat gamma_data; &#125;；<span class="hljs-comment">// mylayer.cpp</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;mylayer.h&quot;</span></span>DEFINE_LAYER_CREATOR(MyLayer) <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; pd)</span></span><span class="hljs-function"></span>&#123;      channels = pd.get(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);           eps      = pd.get(<span class="hljs-number">1</span>, <span class="hljs-number">0.001f</span>);            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span><span class="hljs-function"></span>&#123;      gamma_data = mb.load(channels, <span class="hljs-number">1</span>);      <span class="hljs-keyword">if</span>(gamma_data.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;      <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;<span class="hljs-comment">// 构造函数</span>MyLayer::MyLayer()&#123;      <span class="hljs-comment">// 是否为 1输入1输出层</span>      <span class="hljs-comment">// 1输入1输出层： Convolution, Pooling, ReLU, Softmax ...</span>      <span class="hljs-comment">// 反例       ： Eltwise, Split, Concat, Slice ...</span>      one_blob_only = <span class="hljs-literal">true</span>;      <span class="hljs-comment">// 是否可以在 输入blob 上直接修改 后输出</span>      <span class="hljs-comment">// 支持在原位置上修改： Relu、BN、scale、Sigmod...</span>      <span class="hljs-comment">// 不支持： Convolution、Pooling ...</span>      support_inplace = <span class="hljs-literal">true</span>;&#125;</code></pre><h5 id="4-选择合适的-forward-函数接口，-并实现对应的-forward-函数"><a href="#4-选择合适的-forward-函数接口，-并实现对应的-forward-函数" class="headerlink" title="4.  选择合适的 forward()函数接口， 并实现对应的 forward 函数"></a>4.  选择合适的 forward()函数接口， 并实现对应的 forward 函数</h5><p>Layer类定义了四种 forward()函数：</p><ul><li><p>多输入多输出，const 不可直接对输入进行修改</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; bottom_blobs, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; top_blobs, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;</code></pre></li><li><p>单输入单输出，const 不可直接对输入进行修改</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;</code></pre></li><li><p>多输入多输出，可直接对输入进行修改</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward_inplace</span><span class="hljs-params">(<span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; bottom_top_blobs, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;</code></pre></li><li><p>单输入单输出，可直接对输入进行修改</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;</code></pre></li></ul><p>具体实现:</p><pre><code class="hljs c"><span class="hljs-comment">// mylayer.h</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer.h&quot;</span></span><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> ncnn;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyLayer</span> :</span> <span class="hljs-keyword">public</span> Layer&#123;<span class="hljs-keyword">public</span>:      MyLayer();      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDic&amp; pd)</span></span>;      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span>;            <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>; <span class="hljs-comment">// 单输入单输出</span>      <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;          <span class="hljs-comment">// 单入单出本地修改 </span><span class="hljs-keyword">private</span>:       <span class="hljs-keyword">int</span> channels;        <span class="hljs-keyword">float</span> eps;       Mat gamma_data;&#125;；<span class="hljs-comment">// mylayer.cpp </span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;mylayer.h&quot;</span></span>DEFINE_LAYER_CREATOR(MyLayer)MyLayer::MyLayer()&#123;      one_blob_only = <span class="hljs-literal">true</span>;      support_inplace = <span class="hljs-literal">true</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; pd)</span></span><span class="hljs-function"></span>&#123;      channels = pd.get(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);      eps      = pd.get(<span class="hljs-number">1</span>, <span class="hljs-number">0.001f</span>);             <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span><span class="hljs-function"></span>&#123;      gamma_data = mb.load(channels, <span class="hljs-number">1</span>);      <span class="hljs-keyword">if</span>(gamma_data.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;      <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;<span class="hljs-comment">// 单入单出 前向传播网络 不可修改 非const</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">MyLayer::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;      <span class="hljs-keyword">if</span>(bottom_blob.c != channels)            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;            <span class="hljs-comment">// 实现运算 x = (x + eps) * gamma_per_channel</span>      <span class="hljs-keyword">int</span> w = bottom_blob.w;      <span class="hljs-keyword">int</span> h = bottom_blob.h;      <span class="hljs-keyword">size_t</span> elemsize = bottom_blob.elemsize;      <span class="hljs-keyword">int</span> size = w * h;            <span class="hljs-comment">// 输出需要新建，不可直接在输入blob上修改</span>      top_blob.create(w, h, channels, elemsize, opt.blob_allocator);      <span class="hljs-keyword">if</span>(top_blob.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;            <span class="hljs-meta">#pragam omp parallel for num_threads(opt.num_threads);</span>      <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)      &#123;            <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* in_ptr = bottom_blob.channel(q);            <span class="hljs-keyword">float</span>* out_ptr = top_blob.channel(q);            <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> gamma = gamma_data[q];                        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)            &#123;                  out_ptr[i] = (in_ptr[i] + eps)*gamma;            &#125;            &#125;            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>；&#125;<span class="hljs-comment">// 单入单出 前向传播网络  可在输入blob上修改</span><span class="hljs-keyword">int</span> MyLayer::forward_inplace(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt) <span class="hljs-keyword">const</span>&#123;      <span class="hljs-keyword">if</span>(bottom_blob.c != channels)            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;             <span class="hljs-comment">// 实现运算 x = (x + eps) * gamma_per_channel</span>      <span class="hljs-keyword">int</span> w = bottom_blob.w;      <span class="hljs-keyword">int</span> h = bottom_blob.h;      <span class="hljs-keyword">int</span> size = w * h;            <span class="hljs-comment">// 输出不需要新建，可直接在输入blob上修改</span>      <span class="hljs-meta">#pragam omp parallel for num_threads(opt.num_threads);</span>      <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)      &#123;            <span class="hljs-keyword">float</span>* in_out_ptr = bottom_top_blob.channel(q);            <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> gamma = gamma_data[q];            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)            &#123;                  in_out_ptr[i] = (in_out_ptr[i] + eps)*gamma;            &#125;            &#125;            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>；&#125;</code></pre><h5 id="5-集成进-ncnn-库"><a href="#5-集成进-ncnn-库" class="headerlink" title="5. 集成进 ncnn 库"></a>5. 集成进 ncnn 库</h5><pre><code class="hljs c"><span class="hljs-comment">// 层类型       层名称   输入数量  输出数量   输入层   输出层</span><span class="hljs-comment">// MyLayer     mylayer    1       1     conv2d   mylayer0  0=32 1=0.2 // 对应 param</span><span class="hljs-comment">// 层类型: 和对应的注册的名名称一致</span><span class="hljs-comment">// 注册新层</span>ncnn::Net net;net.register_custom_layer(<span class="hljs-string">&quot;MyLayer&quot;</span>, MyLayer_layer_creator); <span class="hljs-comment">// 注册新层</span>net.load_param(<span class="hljs-string">&quot;model.param&quot;</span>);net.load_model(<span class="hljs-string">&quot;model.bin&quot;</span>);</code></pre><p>​      </p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_4</title>
    <link href="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/"/>
    <url>/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/</url>
    
    <content type="html"><![CDATA[<p>ncnn源码分析-4 模型量化源码</p><a id="more"></a><p>ncnn量化分析代码对应的文件主要有两个:ncnn/tools/quantize/<strong>ncnn2table.cpp</strong> 和 ncnn/tools/quantize/<strong>ncnn2int8.cpp</strong>。</p><ul><li><strong>ncnn2table.cpp</strong> <strong>主要是分析生成相应的量化表， 其中存储了每个层的 scale 值。</strong></li><li><strong>ncnn2int8.cpp</strong> <strong>则是对网络进行 int8量化</strong></li></ul><h4 id="1-ncnn2int8"><a href="#1-ncnn2int8" class="headerlink" title="1. ncnn2int8"></a>1. ncnn2int8</h4><p>​    我们首先对 ncnn2int8.cpp 这个文件进行分析。 该文件完成的主要功能是将 float32 类型转化为 int8 类型。主要分为四个步骤:</p><p>(1) 读取生成的 table 文件，里面存储了对应的 scale。这本质上就是一个文本文件的读取和解析。</p><p>​    read<em>int8_scale_table 负责读取 table 文件, 将 scale 分别存储在 weight_int8scale_table (带 _param\</em>) 和 blob<em>int8scale_table(不带_param\</em>)。 这里只是一个读取文件并解析的过程。</p><p><strong>weight</strong>( 带_param_)格式示例： scale 个数和通道数相同，是主通道进行量化</p><p><img src="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/1.jpg" alt></p><p><strong>blob</strong> (不带_param_) 格式示例：</p><p><img src="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/2.jpg" alt></p><p>(2) <strong>载入原始的参数和模型文件。</strong>net 中的 .param 和 .bin 载入方式<strong>。</strong></p><p>(3) <strong>对卷积层、深度可分离卷积层和全连接层进行量化。quantize_convolution、quantize_convolutiondepthwise、quantize_innerproduct</strong> 分别对卷积层、可分离卷积层和全连接层进行量化<strong>。 这里其实是构建了一个量化层，然后将原始的fp32类型的权重和scale作为输入, 进行一次forward运算，将结果替换原来的fp32权重</strong></p><p>(4) <strong>保存量化后的权重文件</strong></p><p><img src="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/3.jpg" alt></p><p><strong>从主函数入手</strong>: 分别读取了5个参数, <strong>分别是输入模型文件、参数文件、输出模型文件、输出参数文件和量化表的路径</strong>。 然后执行如上所示的四个步骤。</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">int</span> argc, <span class="hljs-keyword">char</span>** argv)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// 读取相应的参数 </span>    <span class="hljs-keyword">if</span> (argc != <span class="hljs-number">6</span>)    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;usage: %s [inparam] [inbin] [outparam] [outbin] [calibration table]\n&quot;</span>, argv[<span class="hljs-number">0</span>]);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* inparam = argv[<span class="hljs-number">1</span>];  <span class="hljs-comment">// 输入模型文件</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* inbin = argv[<span class="hljs-number">2</span>];  <span class="hljs-comment">// 输入参数文件</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* outparam = argv[<span class="hljs-number">3</span>];  <span class="hljs-comment">// 输出模型文件</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* outbin = argv[<span class="hljs-number">4</span>];  <span class="hljs-comment">// 输出参数文件</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* int8scale_table_path = argv[<span class="hljs-number">5</span>]; <span class="hljs-comment">// 量化表的路径</span>    NetQuantize quantizer; <span class="hljs-comment">// 主要的量化类</span>    <span class="hljs-comment">// (1)  读取并解析 scale table 文件</span>    <span class="hljs-keyword">if</span> (int8scale_table_path)    &#123;        <span class="hljs-keyword">bool</span> s2 = read_int8scale_table(int8scale_table_path, quantizer.blob_int8scale_table, quantizer.weight_int8scale_table);        <span class="hljs-keyword">if</span> (!s2)         &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;read_int8scale_table failed\n&quot;</span>);            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;    &#125;    <span class="hljs-comment">// (2) 载入模型文件和参数</span>    quantizer.load_param(inparam);     quantizer.load_model(inbin);    <span class="hljs-comment">// (3) 量化: 主要量化三层: conv、convdw 和 fc</span>    quantizer.quantize_convolution();    quantizer.quantize_convolutiondepthwise();    quantizer.quantize_innerproduct();    <span class="hljs-comment">// (4) 存储模型和参数文件</span>    quantizer.save(outparam, outbin);    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>​    <strong>代码的最核心的三个函数:</strong> <strong>quantize_convolution、quantize_convolutiondepthwise、quantize_innerproduct</strong> 分别对卷积层、可分离卷积层和全连接层进行量化, </p><p>我们在此以 <strong>quantize_convolution</strong> 为例:</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">NetQuantize::quantize_convolution</span><span class="hljs-params">()</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> layer_count = <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">int</span>&gt;(layers.size());    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; layer_count; i++)    &#123;        <span class="hljs-comment">// 查找所有的卷积层</span>        <span class="hljs-keyword">if</span> (layers[i]-&gt;type != <span class="hljs-string">&quot;Convolution&quot;</span>)            <span class="hljs-keyword">continue</span>;                         <span class="hljs-comment">// 获取卷积层的名称           </span>        <span class="hljs-keyword">char</span> key[<span class="hljs-number">256</span>];        <span class="hljs-built_in">sprintf</span>(key, <span class="hljs-string">&quot;%s_param_0&quot;</span>, layers[i]-&gt;name.c_str());                            <span class="hljs-comment">// 在 blob_int8scale_table 找到该层  /* 其实这里的 blob_int8scale_table 下文并没有用到 */</span>        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">map</span>&lt;<span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span>, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; &gt;::iterator iter_data = blob_int8scale_table.find(layers[i]-&gt;name);        <span class="hljs-keyword">if</span> (iter_data == blob_int8scale_table.end())            <span class="hljs-keyword">continue</span>;        <span class="hljs-comment">// 在 weight_int8scale_table 找到该层</span>        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">map</span>&lt;<span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span>, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; &gt;::iterator iter = weight_int8scale_table.find(key);        <span class="hljs-keyword">if</span> (iter == weight_int8scale_table.end())        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;this layer need to be quantized, but no scale param!\n&quot;</span>);            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;                <span class="hljs-comment">// 卷积层量化 -&gt;  fp32 到 int8</span>        ncnn::Convolution* convolution = (ncnn::Convolution*)layers[i]; <span class="hljs-comment">// (1) 获取该卷积层</span>        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;quantize_convolution %s\n&quot;</span>, convolution-&gt;name.c_str());        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; weight_data_int8_scales = iter-&gt;second; <span class="hljs-comment">//  (2) 获取weight_data_int8_scales</span>        &#123;            <span class="hljs-function">ncnn::Mat <span class="hljs-title">int8_weight_data</span><span class="hljs-params">(convolution-&gt;weight_data_size, (<span class="hljs-keyword">size_t</span>)<span class="hljs-number">1u</span>)</span></span>; <span class="hljs-comment">// (3) 结果，和weight的大小一致</span>            <span class="hljs-keyword">if</span> (int8_weight_data.empty())                <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;            <span class="hljs-comment">// 这里所谓的量化，即进行了一次简单的前向传播，将原来的float32类型的权重替换为int类型结果</span>            <span class="hljs-comment">// 在此之前我们准备的东西有</span>            <span class="hljs-comment">// (1) 卷积层的 fp32 的数据 (2) scale 数据 (3) 声明了一个 int8_weight_data的数据</span>            <span class="hljs-comment">// 我们的目标就是 (1) + (2) -&gt; (3)</span>            <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> weight_data_size_output = convolution-&gt;weight_data_size / convolution-&gt;num_output;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> n = <span class="hljs-number">0</span>; n &lt; convolution-&gt;num_output; n++)   <span class="hljs-comment">// 逐卷积核进行量化</span>            &#123;                <span class="hljs-comment">// (4) 创建一个quantize op</span>                ncnn::Layer* op = ncnn::create_layer(ncnn::LayerType::Quantize);                 <span class="hljs-comment">// (5) 把量化表中的scale设置进op里去</span>                ncnn::ParamDict pd;                pd.<span class="hljs-built_in">set</span>(<span class="hljs-number">0</span>, weight_data_int8_scales[n]);                op-&gt;load_param(pd);                <span class="hljs-comment">// (6) blob_allocator</span>                ncnn::Option opt;                opt.blob_allocator = int8_weight_data.allocator;                <span class="hljs-comment">// (7) weight_data &lt;-&gt; weight_data_n , int8_weight_data &lt;-&gt;  int8_weight_data_n </span>                <span class="hljs-keyword">const</span> ncnn::Mat weight_data_n = convolution-&gt;weight_data.range(weight_data_size_output * n, weight_data_size_output);                ncnn::Mat int8_weight_data_n = int8_weight_data.range(weight_data_size_output * n, weight_data_size_output);                <span class="hljs-comment">// (8) quantitze op前传，计算量化权值 weight_data_n -&gt;  int8_weight_data_n</span>                op-&gt;forward(weight_data_n, int8_weight_data_n, opt);                                 <span class="hljs-keyword">delete</span> op;            &#125;            convolution-&gt;weight_data = int8_weight_data; <span class="hljs-comment">// (9) 用量化后的权值替换原来的权值</span>        &#125;        convolution-&gt;int8_scale_term = <span class="hljs-number">2</span>;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>可以来简单的看一下 <strong>quantize</strong> 层:</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">inline</span> <span class="hljs-keyword">signed</span> <span class="hljs-keyword">char</span> <span class="hljs-title">float2int8</span><span class="hljs-params">(<span class="hljs-keyword">float</span> v)</span></span>&#123;    <span class="hljs-keyword">int</span> int32 = <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">int</span>&gt;(round(v)); <span class="hljs-comment">// 取整数, 然后转化为 int32类型</span>    <span class="hljs-keyword">if</span> (int32 &gt; <span class="hljs-number">127</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">127</span>;  <span class="hljs-comment">// 如果大于127, 返回127</span>    <span class="hljs-keyword">if</span> (int32 &lt; <span class="hljs-number">-127</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">-127</span>; <span class="hljs-comment">// 如果小于 -127, 返回 -127</span>    <span class="hljs-keyword">return</span> (<span class="hljs-keyword">signed</span> <span class="hljs-keyword">char</span>)int32; <span class="hljs-comment">// 返回 int32 -&gt; int8</span>&#125;<span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Quantize::forward</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Mat&amp; bottom_blob, Mat&amp; top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>&#123;    <span class="hljs-keyword">int</span> dims = bottom_blob.dims;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">1</span>)&#123;        <span class="hljs-keyword">int</span> w = bottom_blob.w;        top_blob.create(w, (<span class="hljs-keyword">size_t</span>)<span class="hljs-number">1u</span>, opt.blob_allocator);        <span class="hljs-keyword">if</span> (top_blob.empty())            <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;        <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span>* ptr = bottom_blob;        <span class="hljs-keyword">signed</span> <span class="hljs-keyword">char</span>* outptr = top_blob;        <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;w; i++)        &#123;            <span class="hljs-comment">// ! 这一句是最核心的，也是整个量化部分代码的核心</span>            <span class="hljs-comment">// 将 float32 乘以 scale， 然后将其转化为 int8 类型</span>            outptr[i] = float2int8(ptr[i] * scale);         &#125;    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">2</span>)&#123;        ...    &#125;    <span class="hljs-keyword">if</span> (dims == <span class="hljs-number">3</span>)&#123;        ...    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>2. ncnn2table.cpp</strong> </p><p>ncnn2table.cpp 主要用于<strong>量化表的计算</strong>，说白了就是使用我们之前说的算法来计算各个参数的 scale，在ncnn2table.cpp下，顶层代码核心就一句话：</p><pre><code class="hljs cpp"><span class="hljs-comment">/* ncnn2table.cpp */</span><span class="hljs-comment">// filenames: 用来calibration的图片list </span><span class="hljs-comment">// parampath: 参数文件路径</span><span class="hljs-comment">// binpath: bin 二进制文件路径</span><span class="hljs-comment">// tablepath: 生成的量化表的路径</span><span class="hljs-comment">// pre_param: 参数</span>post_training_quantize(filenames, parampath, binpath, tablepath, pre_param);</code></pre><p>我们接下来重点看post_training_quantize这个函数，该函数做了如下几件事：</p><p><strong>&gt;&gt;&gt;&gt;  (1) 初始化quantitize_datas    (2) 计算最大值    (3) 初始化直方图的间隔      (4) 计算直方图           (5) 计算Scale</strong></p><p><strong>(1) 初始化quantitize_datas</strong></p><p>​    没什么好说的，每一个层有一个<strong>QuantizeData</strong>对象，初始化 num_bins=2048，也就是原始的fp32分布Po，其统计直方图一共有<strong>2048个bins</strong></p><pre><code class="hljs cpp"><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;QuantizeData&gt; quantize_datas;<span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; net.conv_names.size(); i++)&#123;    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> layer_name = net.conv_names[i];    <span class="hljs-function">QuantizeData <span class="hljs-title">quantize_data</span><span class="hljs-params">(layer_name, <span class="hljs-number">2048</span>)</span></span>;    quantize_datas.push_back(quantize_data);&#125;</code></pre><p><strong>(2) 计算最大值</strong></p><p>​    遍历所有图片，计算每个blob的最大激活值，这里找的是绝对值最大的那个</p><pre><code class="hljs cpp">    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; image_list.size(); i++) <span class="hljs-comment">// 遍历calibration数据</span>    &#123;        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> img_name = image_list[i];        <span class="hljs-keyword">if</span> ((i + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>)        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;    %d/%d\n&quot;</span>, <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">int</span>&gt;(i + <span class="hljs-number">1</span>), <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">int</span>&gt;(size));        &#125;        cv::Mat bgr = cv::imread(img_name, cv::IMREAD_COLOR);        <span class="hljs-keyword">if</span> (bgr.empty())        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;cv::imread %s failed\n&quot;</span>, img_name.c_str());            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;        ncnn::Mat in = ncnn::Mat::from_pixels_resize(bgr.data, swapRB ? ncnn::Mat::PIXEL_BGR2RGB : ncnn::Mat::PIXEL_BGR, bgr.cols, bgr.rows, width, height);        in.substract_mean_normalize(mean_vals, norm_vals);        ncnn::Extractor ex = net.create_extractor();        ex.input(net.input_names[<span class="hljs-number">0</span>].c_str(), in);        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> j = <span class="hljs-number">0</span>; j &lt; net.conv_names.size(); j++)        &#123;            <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> layer_name = net.conv_names[j];            <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> blob_name = net.conv_bottom_blob_names[layer_name];            ncnn::Mat out;            ex.extract(blob_name.c_str(), out); <span class="hljs-comment">// 前传网络，相当于caffe的forwardTo，拿到blob数据</span>            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> k = <span class="hljs-number">0</span>; k &lt; quantize_datas.size(); k++)            &#123;                <span class="hljs-keyword">if</span> (quantize_datas[k].name == layer_name)                &#123;                    quantize_datas[k].initial_blob_max(out); <span class="hljs-comment">// 统计最大值</span>                    <span class="hljs-keyword">break</span>;                &#125;            &#125;        &#125;    &#125;    <span class="hljs-comment">// 被调函数:</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">QuantizeData::initial_blob_max</span><span class="hljs-params">(ncnn::Mat data)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> channel_num = data.c;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> size = data.w * data.h;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q = <span class="hljs-number">0</span>; q &lt; channel_num; q++)    &#123;        <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> *data_n = data.channel(q);        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; size; i++)        &#123;            max_value = <span class="hljs-built_in">std</span>::max(max_value, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">fabs</span>(data_n[i]));        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>(3) 初始化直方图间隔</strong></p><p>也很简单，遍历每个层，初始化直方图间隔=最大激活值/2048</p><pre><code class="hljs cpp">    <span class="hljs-comment">// step 2 histogram_interval</span>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;    ====&gt; step 2 : generate the histogram_interval.\n&quot;</span>);    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; net.conv_names.size(); i++)    &#123;        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> layer_name = net.conv_names[i];        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> k = <span class="hljs-number">0</span>; k &lt; quantize_datas.size(); k++)        &#123;            <span class="hljs-keyword">if</span> (quantize_datas[k].name == layer_name)            &#123;                quantize_datas[k].initial_histogram_interval();                <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;%-20s : max = %-15f interval = %-10f\n&quot;</span>, quantize_datas[k].name.c_str(), quantize_datas[k].max_value, quantize_datas[k].histogram_interval);                <span class="hljs-keyword">break</span>;            &#125;        &#125;    &#125;    <span class="hljs-comment">// 被调函数    </span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">QuantizeData::initial_histogram_interval</span><span class="hljs-params">()</span></span><span class="hljs-function"></span>&#123;    histogram_interval = max_value / <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">float</span>&gt;(num_bins);    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>(4) 计算直方图</strong></p><p>​    再前传一次，遍历每个blob，向每个bin中投票，计算出直方图，得到原始fp32分布</p><pre><code class="hljs cpp">    <span class="hljs-comment">// step 3 histogram</span>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;    ====&gt; step 3 : generate the histogram.\n&quot;</span>);    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; image_list.size(); i++)    &#123;        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> img_name = image_list[i];        <span class="hljs-keyword">if</span> ((i + <span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>)            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;          %d/%d\n&quot;</span>, (<span class="hljs-keyword">int</span>)(i + <span class="hljs-number">1</span>), (<span class="hljs-keyword">int</span>)size);                    cv::Mat bgr = cv::imread(img_name, cv::IMREAD_COLOR);        <span class="hljs-keyword">if</span> (bgr.empty())        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;cv::imread %s failed\n&quot;</span>, img_name.c_str());            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;        ncnn::Mat in = ncnn::Mat::from_pixels_resize(bgr.data, swapRB ? ncnn::Mat::PIXEL_BGR2RGB : ncnn::Mat::PIXEL_BGR, bgr.cols, bgr.rows, width, height);        in.substract_mean_normalize(mean_vals, norm_vals);        ncnn::Extractor ex = net.create_extractor();        ex.input(net.input_names[<span class="hljs-number">0</span>].c_str(), in);        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> j = <span class="hljs-number">0</span>; j &lt; net.conv_names.size(); j++)        &#123;            <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> layer_name = net.conv_names[j];            <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> blob_name = net.conv_bottom_blob_names[layer_name];            ncnn::Mat out;            ex.extract(blob_name.c_str(), out);            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> k = <span class="hljs-number">0</span>; k &lt; quantize_datas.size(); k++)            &#123;                <span class="hljs-keyword">if</span> (quantize_datas[k].name == layer_name)                &#123;                    quantize_datas[k].update_histogram(out);                    <span class="hljs-keyword">break</span>;                &#125;            &#125;        &#125;    &#125;    <span class="hljs-comment">// 被调函数      </span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">QuantizeData::update_histogram</span><span class="hljs-params">(ncnn::Mat data)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> channel_num = data.c;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> size = data.w * data.h;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q = <span class="hljs-number">0</span>; q &lt; channel_num; q++)    &#123;        <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> *data_n = data.channel(q);        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; size; i++)        &#123;            <span class="hljs-keyword">if</span> (data_n[i] == <span class="hljs-number">0</span>)                <span class="hljs-keyword">continue</span>;            <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> index = <span class="hljs-built_in">std</span>::min(<span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">int</span>&gt;(<span class="hljs-built_in">std</span>::<span class="hljs-built_in">abs</span>(data_n[i]) / histogram_interval), <span class="hljs-number">2047</span>);            histogram[index]++;        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>(5) 计算Scale</strong></p><pre><code class="hljs cpp">    <span class="hljs-comment">// step4 kld</span>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;    ====&gt; step 4 : using kld to find the best threshold value.\n&quot;</span>);    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; net.conv_names.size(); i++)    &#123;        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> layer_name = net.conv_names[i];        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> blob_name = net.conv_bottom_blob_names[layer_name];        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;%-20s &quot;</span>, layer_name.c_str());        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> k = <span class="hljs-number">0</span>; k &lt; quantize_datas.size(); k++)        &#123;            <span class="hljs-keyword">if</span> (quantize_datas[k].name == layer_name)            &#123;                quantize_datas[k].get_data_blob_scale();                <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;bin : %-8d threshold : %-15f interval : %-10f scale : %-10f\n&quot;</span>,                        quantize_datas[k].threshold_bin,                        quantize_datas[k].threshold,                        quantize_datas[k].histogram_interval,                        quantize_datas[k].scale);                <span class="hljs-built_in">fprintf</span>(fp, <span class="hljs-string">&quot;%s %f\n&quot;</span>, layer_name.c_str(), quantize_datas[k].scale);                <span class="hljs-keyword">break</span>;            &#125;        &#125;    &#125;<span class="hljs-comment">// 被调函数 </span><span class="hljs-function"><span class="hljs-keyword">float</span> <span class="hljs-title">QuantizeData::get_data_blob_scale</span><span class="hljs-params">()</span></span><span class="hljs-function"></span>&#123;       normalize_histogram();   <span class="hljs-comment">// 直方图归一化</span>    threshold_bin = threshold_distribution(histogram);   <span class="hljs-comment">// 计算最后有多少个bins</span>    threshold = (threshold_bin + <span class="hljs-number">0.5</span>) * histogram_interval;   <span class="hljs-comment">// 之后很容易就能找到Threshold</span>    scale = <span class="hljs-number">127</span> / threshold;   <span class="hljs-comment">// Scale也很简单就能z好到</span>    <span class="hljs-keyword">return</span> scale;&#125;</code></pre><p>其实说了半天，<strong>最核心的就在get_data_blob_scale这个函数里，函数分为3步</strong>：</p><p><strong>a. 直方图归一化</strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">QuantizeData::normalize_histogram</span><span class="hljs-params">()</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">size_t</span> length = histogram.size();    <span class="hljs-keyword">float</span> sum = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; length; i++)        sum += histogram[i];    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i = <span class="hljs-number">0</span>; i &lt; length; i++)        histogram[i] /= sum;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>b. 使用KL散度计算最后用多少个bins比较合适</strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">QuantizeData::threshold_distribution</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; &amp;distribution, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> target_bin = <span class="hljs-number">128</span>)</span></span><span class="hljs-function"></span>&#123;    ...    <span class="hljs-comment">// 这里length就是原始分布Po的长度，NCNN默认2048。这里的threshold实际上是num_bins，可以换算成T</span>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> threshold = target_bin; threshold &lt; length; threshold++)   <span class="hljs-comment">// target_bin=128，length = 2048</span>    &#123;         <span class="hljs-comment">// ①. 计算截断的fp32分布P</span>         <span class="hljs-comment">// ②. 计算int8分布Q</span>         <span class="hljs-comment">// ③. 计算扩展分布Q_expand</span>        <span class="hljs-comment">// ④. 计算KL散度</span>        <span class="hljs-comment">// ⑤. 比大小</span>    &#125;&#125;</code></pre><p>①. 计算截断的fp32分布P也很简单：</p><pre><code class="hljs cpp"><span class="hljs-keyword">float</span> threshold_sum = <span class="hljs-number">0</span>;<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> threshold=target_bin; threshold&lt;length; threshold++) &#123;    threshold_sum += distribution[threshold];   <span class="hljs-comment">// 128以上的所有数据和</span>&#125;<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> threshold=target_bin; threshold&lt;length; threshold++) &#123;    <span class="hljs-function"><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; <span class="hljs-title">t_distribution</span><span class="hljs-params">(distribution.begin(), distribution.begin()+threshold)</span></span>;        t_distribution[threshold<span class="hljs-number">-1</span>] += threshold_sum;   <span class="hljs-comment">// P的最后一个bin加上被截断的所有概率，得到截断的fp32分布P</span>    threshold_sum -= distribution[threshold];       <span class="hljs-comment">// 是通过减法来保证数值正确性的，很巧秒</span>    ...</code></pre><p>②. 计算int8分布Q，注意Q是从Po得来的，而不是从P得来的，大于T的部分并不会加进最后一个bin内（存疑，不理解）；另外，当发生了4舍5入时，会有特殊处理</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; <span class="hljs-title">quantize_distribution</span><span class="hljs-params">(target_bin)</span></span>;  <span class="hljs-comment">// 量化后分布Q，长度是128</span> fill(quantize_distribution.begin(), quantize_distribution.end(), <span class="hljs-number">0</span>);<span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> num_per_bin = <span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">float</span>&gt;(threshold) / target_bin;  <span class="hljs-comment">// 其实就是当前T下的Scale </span><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;target_bin; i++) &#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> start = i * num_per_bin;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> end = start + num_per_bin;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> left_upper = <span class="hljs-built_in">ceil</span>(start);    <span class="hljs-keyword">if</span> (left_upper &gt; start)     &#123;   <span class="hljs-comment">// 这里的意思是，如果发生了5入，则需要将舍掉的那个bin按比例加进来</span>        <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> left_scale = left_upper - start;        quantize_distribution[i] += left_scale * distribution[left_upper - <span class="hljs-number">1</span>];    &#125;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> right_lower = <span class="hljs-built_in">floor</span>(end);    <span class="hljs-keyword">if</span> (right_lower &lt; end)     &#123;        <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> right_scale = end - right_lower;        quantize_distribution[i] += right_scale * distribution[right_lower];    &#125;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j=left_upper; j&lt;right_lower; j++)     &#123;        quantize_distribution[i] += distribution[j];    &#125;&#125;</code></pre><p>③. 计算Q_expand，统计count时0不算在内，注意不管是统计数量还是上采样的过程中，都有4舍5入相关的问题</p><pre><code class="hljs cpp"><span class="hljs-comment">// get Q</span><span class="hljs-function"><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; <span class="hljs-title">expand_distribution</span><span class="hljs-params">(threshold, <span class="hljs-number">0</span>)</span></span>;<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;target_bin; i++) &#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> start = i * num_per_bin;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> end = start + num_per_bin;    <span class="hljs-keyword">float</span> count = <span class="hljs-number">0</span>;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> left_upper = <span class="hljs-built_in">ceil</span>(start);    <span class="hljs-keyword">float</span> left_scale = <span class="hljs-number">0</span>;    <span class="hljs-keyword">if</span> (left_upper &gt; start)     &#123;        left_scale = left_upper - start;        <span class="hljs-keyword">if</span> (distribution[left_upper - <span class="hljs-number">1</span>] != <span class="hljs-number">0</span>)         &#123;            count += left_scale;        &#125;    &#125;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> right_lower = <span class="hljs-built_in">floor</span>(end);    <span class="hljs-keyword">float</span> right_scale = <span class="hljs-number">0</span>;    <span class="hljs-keyword">if</span> (right_lower &lt; end)     &#123;        right_scale = end - right_lower;        <span class="hljs-keyword">if</span> (distribution[right_lower] != <span class="hljs-number">0</span>)         &#123;            count += right_scale;        &#125;    &#125;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j=left_upper; j&lt;right_lower; j++)     &#123;        <span class="hljs-keyword">if</span> (distribution[j] != <span class="hljs-number">0</span>)         &#123;            count++;        &#125;    &#125;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> expand_value = quantize_distribution[i] / count;    <span class="hljs-keyword">if</span> (left_upper &gt; start)     &#123;        <span class="hljs-keyword">if</span> (distribution[left_upper - <span class="hljs-number">1</span>] != <span class="hljs-number">0</span>)         &#123;            expand_distribution[left_upper - <span class="hljs-number">1</span>] += expand_value * left_scale;  <span class="hljs-comment">// 上采样过程中一样有四舍五入的问题</span>        &#125;    &#125;    <span class="hljs-keyword">if</span> (right_lower &lt; end)     &#123;        <span class="hljs-keyword">if</span> (distribution[right_lower] != <span class="hljs-number">0</span>)         &#123;            expand_distribution[right_lower] += expand_value * right_scale;        &#125;    &#125;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j=left_upper; j&lt;right_lower; j++)     &#123;        <span class="hljs-keyword">if</span> (distribution[j] != <span class="hljs-number">0</span>)         &#123;            expand_distribution[j] += expand_value;        &#125;    &#125;&#125;</code></pre><p>④. 计算KL散度，注意当Q为0时，KL散度只加一（存疑，不理解）</p><pre><code class="hljs cpp"><span class="hljs-keyword">float</span> kl_divergence = compute_kl_divergence(t_distribution, expand_distribution);<span class="hljs-function"><span class="hljs-keyword">float</span> <span class="hljs-title">QuantizeData::compute_kl_divergence</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; &amp;dist_a, <span class="hljs-keyword">const</span> <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">float</span>&gt; &amp;dist_b)</span> </span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> length = dist_a.size();    assert(dist_b.size() == length);    <span class="hljs-keyword">float</span> result = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;length; i++)     &#123;        <span class="hljs-keyword">if</span> (dist_a[i] != <span class="hljs-number">0</span>)         &#123;            <span class="hljs-keyword">if</span> (dist_b[i] == <span class="hljs-number">0</span>)             &#123;                result += <span class="hljs-number">1</span>;   <span class="hljs-comment">// Q为0时，KL散度只加一</span>            &#125;             <span class="hljs-keyword">else</span>             &#123;                result += dist_a[i] * <span class="hljs-built_in">log</span>(dist_a[i] / dist_b[i]);            &#125;        &#125;    &#125;    <span class="hljs-keyword">return</span> result;&#125;</code></pre><p>⑤. 轻松愉悦的找最大值</p><pre><code class="hljs cpp"><span class="hljs-keyword">if</span> (kl_divergence &lt; min_kl_divergence) &#123;    min_kl_divergence = kl_divergence;       target_threshold = threshold;   <span class="hljs-comment">// 实际上是num_bins</span>&#125;</code></pre><p><strong>c. 计算 Threshold 和 bins</strong></p><p>至此，我们得到了KL散度最小的桶数num_bins，可以通过下述公式得到T和Scale，就三行：</p><pre><code class="hljs cpp">threshold = (<span class="hljs-keyword">static_cast</span>&lt;<span class="hljs-keyword">float</span>&gt;(threshold_bin) + <span class="hljs-number">0.5f</span>) * histogram_interval;scale = <span class="hljs-number">127</span> / threshold;<span class="hljs-keyword">return</span> scale;</code></pre><p>最后就是将Scale数据存下来，得到量化表了。</p><p><strong>总结一下以上的代码逻辑:</strong></p><p><img src="/2020/09/21/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-4/4.png" alt></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>rk3399系统烧写</title>
    <link href="/2020/09/18/rk3399%E7%B3%BB%E7%BB%9F%E7%83%A7%E5%86%99/"/>
    <url>/2020/09/18/rk3399%E7%B3%BB%E7%BB%9F%E7%83%A7%E5%86%99/</url>
    
    <content type="html"><![CDATA[<h4 id="一-相关工具的准备"><a href="#一-相关工具的准备" class="headerlink" title="一. 相关工具的准备"></a>一. 相关工具的准备</h4><ol><li>下载 烧写工具:</li></ol><pre><code class="hljs shell">git clone https://github.com/rockchip-linux/rkbin.git</code></pre><ol><li>下载相关的 ubuntu 镜像。下载地址  <a href="https://github.com/lanseyujie/tn3399_v3/release">https://github.com/lanseyujie/tn3399_v3/release</a></li></ol><h4 id="二-刷机"><a href="#二-刷机" class="headerlink" title="二. 刷机"></a>二. 刷机</h4><ol><li>板卡进入刷机模式</li></ol><p>连接 usb-c 和 PC 主机。断开电源， 按住 recovey 键， 然后插上电源， 让板卡进入刷机模式。</p><ol><li><p>短接板卡反面的两个触点， 进入 maskrom 模式。</p></li><li><p>执行如下命令， 将相应的镜像刷入 板卡</p><pre><code class="hljs apache"><span class="hljs-attribute">sudo</span> rkdeveloptool db rk<span class="hljs-number">3399</span>_loader_v<span class="hljs-number">1</span>.<span class="hljs-number">24</span>.<span class="hljs-number">126</span>.bin<span class="hljs-attribute">sudo</span> rkdeveloptool ef<span class="hljs-attribute">sudo</span> rkdeveloptool wl <span class="hljs-number">0</span>x<span class="hljs-number">0</span> system.img</code></pre></li></ol><h4 id="三-登录"><a href="#三-登录" class="headerlink" title="三. 登录"></a>三. 登录</h4><ol><li><p>重新启动开电脑， 就可以进入 ubuntu 系统， 默认用户名是 root  密码是 1234</p></li><li><p>分区扩容</p><pre><code class="hljs shell">sudo apt install -y partedsudo parted /dev/mmcblk2unit sprintresizepart 5 100%printQsudo resize2fs /dev/mmcblk2p5</code></pre></li><li><p>可以安装 相关的桌面系统， ubuntu 新增账号循环登陆桌面。只是在添加新用户的时候需要使用</p></li></ol><pre><code class="hljs shell">useradd -m usrname # 加 m 参数会创建一个同名文件夹， 如果没有则会导致循环登录</code></pre><h4 id="参考网址"><a href="#参考网址" class="headerlink" title="参考网址:"></a>参考网址:</h4><p><a href="https://github.com/lanseyujie/tn3399_v3">https://github.com/lanseyujie/tn3399_v3</a>     # 很齐全的资料包</p><p><a href="https://naivekun.tk/">https://naivekun.tk/</a></p>]]></content>
    
    
    <categories>
      
      <category>嵌入式相关</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_3</title>
    <link href="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/"/>
    <url>/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/</url>
    
    <content type="html"><![CDATA[<p>ncnn 源码分析 模型量化原理</p><a id="more"></a><h4 id="1-FP32-vs-int8"><a href="#1-FP32-vs-int8" class="headerlink" title="1. FP32 vs int8"></a>1. FP32 vs int8</h4><p>​     来看一下 <strong>FP32、FP16和int8</strong>之间的动态范围和精度的对比， 可以看到float32 的取值范围几乎是无穷的， 而int8只有<strong>-128~127</strong>. 因此需要建立映射关系将float32类型的浮点数映射到指定范围的int8类型。 </p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/ncnn1.png" alt></p><h4 id="2-TensorRT-int8-量化方案"><a href="#2-TensorRT-int8-量化方案" class="headerlink" title="2. TensorRT int8 量化方案"></a>2. TensorRT int8 量化方案</h4><p>Nvidia 的 TensorRT提供了一种量化方案，但是它仅仅提供相应的SDK和解决方案， 没有公布对应的源代码， 诸多第三方厂家则根据该解决方案自己造轮子，产生了对应的解决方案。该量化方案的最重要的两份参考资料如下所示:</p><p>- TensorRT Develop guide: <a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#work-with-qat-networks">https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#work-with-qat-networks</a></p><p>- PDF 链接： <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf</a></p><p><strong>(1)</strong> <strong>max-max 映射</strong>： 最简单粗暴的方式如下左图所示</p><p>​    首先求出一个laye 的激活值范围， 然后按照绝对值的最大值作为阈值， 然后把这个范围按照比例映射到-127到128的范围内, 其fp32和int8的转换公式为:</p><p><strong>FP32 Tensor (T) = scale_factor(sf) * 8-bit Tensor(t) + FP32_bias (b)</strong> </p><p>通过实验得知，bias值去掉对精度的影响不是很大，因此我们直接去掉, 所以该公式可以简化为:</p><p><strong>T = sf * t</strong></p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/ncnn2.png" alt></p><p><strong>(2) 饱和映射</strong></p><p>​    如上方法会有一个问题：<strong>不饱和，即通常在正负上会有一些量化值未被利用，且会产生的精度损失较大。</strong>针对 max-max 映射存在的问题， TensorRT提出了如上右图的饱和映射。 <strong>选取一个阈值T，然后将 -|T|~|T| 之间的值映射到 -127 到 128 这个范围内。这样确定了阈值T之后，其实也能确定Scale，一个简单的线性公式是: Scale = T/127。 所以要计算Scale，只要找到合适的阈值T就可以了。那么问题来了，T应该取何值? 其基本流程如下:</strong></p><p>​    <strong>(a) 选取不同的 T 阈值进行量化, 将 P(fp32) 映射到 Q(int8)。</strong></p><p>​    <strong>(b) 将 Q(int8) 反量化到 P(fp32) 一样长度，得到分布 Q_expand；</strong></p><p>​    <strong>(c) 计算P和Q_expand 的相对熵(KL散度)，然后选择相对熵最少的一个，也就是跟原分布最像的一个,</strong> <strong>从而确定Scale**</strong>。**</p><p><strong>(3) KL 散度</strong></p><p>​    KL散度可以用来<strong>描述P、Q两个分布的差异</strong>。<strong>散度越小，两个分布的差异越小，概率密度函数形状和数值越接近</strong>。这里的所有分布、计算，都是离散形式的。分布是以统计直方图的方式存在，KL散度公式也是离散公式：</p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/ncnn5.png" alt></p><p>从上式中我们还发现一个问题：KL散度计算公式要求P、Q两个统计直方图长度一样（也就是bins的数量一样）。Q一直都是-127～127；可是P的数量会随着T的变化而变化。那这怎么做KL散度呢？</p><p>ncnn 的做法是将 Q扩展到和P一样的长度，下面举个例子(NVIDIA PPT中的例子)：</p><pre><code class="hljs cpp">P = [<span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">5</span> <span class="hljs-number">3</span> <span class="hljs-number">1</span> <span class="hljs-number">7</span>]     <span class="hljs-comment">// fp32的统计直方图，T=8</span><span class="hljs-comment">// 假设只量化到两个bins，即量化后的值只有-1/0/+1三种</span>Q=[<span class="hljs-number">1</span>+<span class="hljs-number">0</span>+<span class="hljs-number">2</span>+<span class="hljs-number">3</span>, <span class="hljs-number">5</span>+<span class="hljs-number">3</span>+<span class="hljs-number">1</span>+<span class="hljs-number">7</span>] = [<span class="hljs-number">6</span>, <span class="hljs-number">16</span>]<span class="hljs-comment">// P和Q现在没法做KL散度，所以要将Q扩展到和P一样的长度</span>Q_expand = [<span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>] = [<span class="hljs-number">2</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span>]  <span class="hljs-comment">// P中有0时，不算在内</span>D = KL(P||Q_expand)  <span class="hljs-comment">// 这样就可以做KL散度计算了</span></code></pre><p>​    这个扩展的操作，就像图像的上采样一样，将低精度的统计直方图(Q)，上采样的高精度的统计直方图上去(Q_expand)。由于Q中一个bin对应P中的4个bin，因此在Q上采样的Q_expand的过程中，所有的数据要除以4。另外，在计算fp32的分布P时，被T截断的数据，是要算在最后一个bin里面的。</p><h4 id="3-ncnn的conv量化计算流程"><a href="#3-ncnn的conv量化计算流程" class="headerlink" title="3. ncnn的conv量化计算流程"></a>3. ncnn的conv量化计算流程</h4><p>正常的 fp32 计算中， 一个conv 的计算流程如下所示， 所有的数据均是 fp32， 没什么特殊的</p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/ncnn3.png" alt="fp32 conv计算流程"></p><p>在 ncnn conv 进行Int8计算时， 计算流程如下所示，ncnn首先<strong>将输入(bottom_blob)和权重量化成Int8，在Int8下计算卷积，然后反量化到 fp32，再和未量化的bias相加，得到输出 top_blob</strong>(ncnn并没有对bias做量化)</p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-3/ncnn4.png" alt="int8 conv 计算流程， 在conv前， 对input和weight做量化， 计算完后反量化到 fp32, 再加 bias"></p><p>输入和权重的<strong>量化公式</strong>为:</p><pre><code class="hljs cpp">bottom_blob(int8) = bottom_blob_in8t_scale * bottom(fp32)weight_blob(int8) = weight_data_int8_scale * weight(fp32)</code></pre><p><strong>反量化</strong>的目的是将int8映射回到原来的fp32,范围保持要一致, 由于 weight_blob(int8) 和 bottom_blob(int8) 相乘， 所以此处的量化反量化的 scale 应该为:</p><pre><code class="hljs cpp">dequantize_scale = <span class="hljs-number">1</span>/(bottom_blob_int8_scale * weight_data_int8_scale)innner_blob(fp32) = dequantize_scale * inner_blob</code></pre><p><strong>! 值得注意的是， 权重是在网络初始化时候就进行量化了， 而输入则是在前向推导时进行量化。</strong></p><h4 id="4-ncnn-量化工具的使用"><a href="#4-ncnn-量化工具的使用" class="headerlink" title="4. ncnn 量化工具的使用"></a>4. <strong>ncnn 量化工具的使用</strong></h4><p>(1) <strong>Optimization graphic 图优化: 最明显的变化是将conv层和bn层进行合并</strong></p><pre><code class="hljs shell">./ncnnoptimize mobilenet-fp32.param mobilenet-fp32.bin mobilenet-nobn-fp32.param mobilenet-nobn-fp32.bin</code></pre><p><strong>(2) Create the calibration table file(建议使用超过5000张图片的验证集进行对齐): 计算产生对应的 scale</strong></p><pre><code class="hljs shell">./ncnn2table --param mobilenet-nobn-fp32.param --bin mobilenet-nobn-fp32.bin --images images/ --output mobilenet-nobn.table --mean 104,117,123 --norm 0.017,0.017,0.017 --size 224,224 --thread 2</code></pre><p><strong>(3) Quantization：量化</strong></p><pre><code class="hljs shell">./ncnn2int8 mobilenet-nobn-fp32.param mobilenet-nobn-fp32.bin mobilenet-int8.param mobilenet-int8.bin mobilenet-nobn.table</code></pre><h4 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h4><p>[1] <a href="https://me.csdn.net/sinat_31425585">https://me.csdn.net/sinat_31425585</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/c_1064124187198705664">https://zhuanlan.zhihu.com/c_1064124187198705664</a></p><p>[3] <a href="https://github.com/BUG1989/caffe-int8-convert-tools">https://github.com/BUG1989/caffe-int8-convert-tools</a></p><p>[4] <a href="https://github.com/Tencent/ncnn/wiki/quantized-int8-inference">Tencent/ncnn</a></p><p>[5] QNNPACK</p><p>[6] Nvidia solution： Szymon Migacz. 8-bit Inference with TensorRT</p><p>[7] Google solution：Quantizing deep convolutional networks for efficient inference: A whitepaper</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_2</title>
    <link href="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-2/"/>
    <url>/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-2/</url>
    
    <content type="html"><![CDATA[<p>ncnn 源码分析（二） Extractor</p><a id="more"></a><p>前面大致总结了一下ncnn模型载入的流程，模型载入之后，就是新建一个<strong>Extractor</strong>，然后设置输入，获取输出：</p><pre><code class="hljs cpp">ncnn::Extractor ex = net.create_extractor();ex.set_num_threads(<span class="hljs-number">4</span>); ex.input(<span class="hljs-string">&quot;data&quot;</span>, in); ncnn::Mat out;ex.extract(<span class="hljs-string">&quot;detection_out&quot;</span>, out);</code></pre><p>现在可以看一下Extractor的定义了：</p><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Extractor</span></span><span class="hljs-class">&#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-comment">// enable light mode, intermediate blob will be recycled when enabled</span>    <span class="hljs-comment">// enabled by default</span>    <span class="hljs-comment">// 设置light模式，启用时中间 blob 将会循环利用</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">set_light_mode</span><span class="hljs-params">(<span class="hljs-keyword">bool</span> enable)</span></span>;     <span class="hljs-comment">// set thread count for this extractor, 设置线程数</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">set_num_threads</span><span class="hljs-params">(<span class="hljs-keyword">int</span> num_threads)</span></span>;     <span class="hljs-comment">// set blob memory allocator, 设置blob的内存分配器</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">set_blob_allocator</span><span class="hljs-params">(Allocator* allocator)</span></span>;     <span class="hljs-comment">// set workspace memory allocator, 设置工作空间的内存分配器</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">set_workspace_allocator</span><span class="hljs-params">(Allocator* allocator)</span></span>;  <span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>    <span class="hljs-comment">// set input by blob name</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 设置网络输入：字符串layer名</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">input</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* blob_name, <span class="hljs-keyword">const</span> Mat&amp; in)</span></span>;     <span class="hljs-comment">// get result by blob name</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 设置提取器的输入：得到对应输出</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">extract</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* blob_name, Mat&amp; feat)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>     <span class="hljs-comment">// set input by blob index</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 设置int类型blob索引及输入</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">input</span><span class="hljs-params">(<span class="hljs-keyword">int</span> blob_index, <span class="hljs-keyword">const</span> Mat&amp; in)</span></span>;     <span class="hljs-comment">// get result by blob index</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 设置int类型blob索引及输出</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">extract</span><span class="hljs-params">(<span class="hljs-keyword">int</span> blob_index, Mat&amp; feat)</span></span>; <span class="hljs-keyword">protected</span>:    <span class="hljs-comment">// 对外提供create_extractor接口</span>    <span class="hljs-function"><span class="hljs-keyword">friend</span> Extractor <span class="hljs-title">Net::create_extractor</span><span class="hljs-params">()</span> <span class="hljs-keyword">const</span></span>;    Extractor(<span class="hljs-keyword">const</span> Net* net, <span class="hljs-keyword">int</span> blob_count); <span class="hljs-keyword">private</span>:    <span class="hljs-comment">// 网络 </span>    <span class="hljs-keyword">const</span> Net* net;    <span class="hljs-comment">// blob的 mat</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt; blob_mats;    <span class="hljs-comment">// 选项</span>    Option opt; &#125;;</code></pre><p>除了设置option的接口之外，就只剩下我们需要使用的几个接口函数了：</p><p><strong>(1) Extractor</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// 创建Extractor</span><span class="hljs-function">Extractor <span class="hljs-title">Net::create_extractor</span><span class="hljs-params">()</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">return</span> Extractor(<span class="hljs-keyword">this</span>, blobs.size());&#125;</code></pre><p>内部调用了接口为，就是将blob_mat数组resize到网络的blob数目大小，然后设置了一下选项：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 执行器</span>Extractor::Extractor(<span class="hljs-keyword">const</span> Net* _net, <span class="hljs-keyword">int</span> blob_count) : net(_net)&#123;    blob_mats.resize(blob_count);    opt = net-&gt;opt;&#125;</code></pre><p><strong>(2) input接口：</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// 设置输入</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Extractor::input</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* blob_name, <span class="hljs-keyword">const</span> Mat&amp; in)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// 获取输入模块对应index</span>    <span class="hljs-keyword">int</span> blob_index = net-&gt;find_blob_index_by_name(blob_name);    <span class="hljs-keyword">if</span> (blob_index == <span class="hljs-number">-1</span>)        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;     <span class="hljs-comment">// 调用直接用index的设置input方法</span>    <span class="hljs-keyword">return</span> input(blob_index, in);&#125;</code></pre><p>内部调用的接口为：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 输入为index的输入接口</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Extractor::input</span><span class="hljs-params">(<span class="hljs-keyword">int</span> blob_index, <span class="hljs-keyword">const</span> Mat&amp; in)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">if</span> (blob_index &lt; <span class="hljs-number">0</span> || blob_index &gt;= (<span class="hljs-keyword">int</span>)blob_mats.size())        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;     <span class="hljs-comment">// 设置blob_index对应 Mat</span>    blob_mats[blob_index] = in;     <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>(3) extract接口：</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// 将输入string类型name转换成对应的索引</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Extractor::extract</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* blob_name, VkMat&amp; feat, VkCompute&amp; cmd)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> blob_index = net-&gt;find_blob_index_by_name(blob_name);    <span class="hljs-keyword">if</span> (blob_index == <span class="hljs-number">-1</span>)        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;     <span class="hljs-keyword">return</span> extract(blob_index, feat, cmd);&#125;</code></pre><p>这里调用的接口为：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 提取特征</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Extractor::extract</span><span class="hljs-params">(<span class="hljs-keyword">int</span> blob_index, Mat&amp; feat)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">if</span> (blob_index &lt; <span class="hljs-number">0</span> || blob_index &gt;= (<span class="hljs-keyword">int</span>)blob_mats.size())        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;     <span class="hljs-keyword">int</span> ret = <span class="hljs-number">0</span>;        <span class="hljs-keyword">if</span> (blob_mats[blob_index].dims == <span class="hljs-number">0</span>)     <span class="hljs-comment">// 如果输出blob为空</span>    &#123;        <span class="hljs-keyword">int</span> layer_index = net-&gt;blobs[blob_index].producer;  <span class="hljs-comment">// 查找输出blob对应的生产者</span>        ret = net-&gt;forward_layer(layer_index, blob_mats, opt);   <span class="hljs-comment">// 前向推理</span>    &#125;    feat = blob_mats[blob_index];   <span class="hljs-comment">// 输出特征</span>     <span class="hljs-keyword">if</span> (opt.use_packing_layout)   <span class="hljs-comment">// 对特征进行unpack</span>    &#123;        Mat bottom_blob_unpacked;        convert_packing(feat, bottom_blob_unpacked, <span class="hljs-number">1</span>, opt);        feat = bottom_blob_unpacked;    &#125;     <span class="hljs-keyword">return</span> ret;&#125;</code></pre><p>​    这里就是调用各层前向推理forward_layer方法来进行推理的，这个对应于特定层的推理过程，后面总结各个层的时候再说。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_1</title>
    <link href="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1/"/>
    <url>/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1/</url>
    
    <content type="html"><![CDATA[<p>ncnn 源码分析 — 参数与模型载入</p><a id="more"></a><h4 id="1-实例代码"><a href="#1-实例代码" class="headerlink" title="1. 实例代码"></a>1. 实例代码</h4><p>使用ncnn进行前向计算的步骤很简单，就如下几行代码即可完成。</p><pre><code class="hljs cpp"> <span class="hljs-comment">// 代码来自 ncnn/examples/shufflenetv2.cpp</span> <span class="hljs-comment">/* Step 1.1 : 加载.parma 文件 和 .bin 文件 */</span> ncnn::Net shufflenetv2; shufflenetv2.load_param(<span class="hljs-string">&quot;shufflenet_v2_x0.5.param&quot;</span>); shufflenetv2.load_model(<span class="hljs-string">&quot;shufflenet_v2_x0.5.bin&quot;</span>); <span class="hljs-comment">/* Step 1.2 : 构建并配置 提取器 */</span> ncnn::Extractor ex = shufflenetv2.create_extractor(); <span class="hljs-comment">/* Step 1.3 : 设置输入（将图片转换成ncnn::Mat结构作为输入） */</span>    ncnn::Mat in = ncnn::Mat::from_pixels_resize(bgr.data, ncnn::Mat::PIXEL_BGR, bgr.cols, bgr.rows, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>);<span class="hljs-keyword">const</span> <span class="hljs-keyword">float</span> norm_vals[<span class="hljs-number">3</span>] = &#123;<span class="hljs-number">1</span>/<span class="hljs-number">255.f</span>, <span class="hljs-number">1</span>/<span class="hljs-number">255.f</span>, <span class="hljs-number">1</span>/<span class="hljs-number">255.f</span>&#125;;in.substract_mean_normalize(<span class="hljs-number">0</span>, norm_vals);ex.input(<span class="hljs-string">&quot;data&quot;</span>, in);        <span class="hljs-comment">/* Step 1.4 : 提取输出 */</span> ncnn::Mat out;ex.extract(<span class="hljs-string">&quot;fc&quot;</span>, out);</code></pre><h4 id="2-代码分析"><a href="#2-代码分析" class="headerlink" title="2. 代码分析"></a>2. 代码分析</h4><p><strong>我姑且将其分为：加载模型</strong>、<strong>构建并配置提取器</strong>、<strong>设置输入</strong>、<strong>输出处理</strong>、<strong>模型封装</strong>五个部分来加以分析</p><p><strong>模型载入</strong>:</p><p><strong>相关代码:</strong></p><p><strong>net.h/cpp</strong>   <strong>blob.h/cpp</strong>   <strong>layer.h/.cpp</strong>  </p><p>​    <strong>paramdict.cpp/h</strong>  </p><p>​    <strong>modelbin.h/.cpp</strong></p><p><strong>相关文件:</strong></p><p>xx.bin  xx.param</p><h4 id="3-加载模型"><a href="#3-加载模型" class="headerlink" title="3. 加载模型"></a>3. 加载模型</h4><p>ncnn 在使用 <strong>.param</strong> 和 <strong>.bin</strong> 两个文件来描述一个神经网络模型。 模型加载的根本目的是将 .param 和 .bin 文件的信息加载到目标神经网络（一个ncnn::Net结构）中</p><p>其中：</p><p><strong>.param</strong>：描述神经网络的结构，包括层名称，层输入输出信息，层参数信息（如卷积层的kernal大小等）等。</p><p><strong>.bin</strong> 文件则记录神经网络运算所需要的数据信息（比如卷积层的权重、偏置信息等）</p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1/ncnn1.png" alt="ncnn官方demo中的模型文件"></p><h6 id="3-1-param-文件"><a href="#3-1-param-文件" class="headerlink" title="3.1 param 文件"></a>3.1 param 文件</h6><p><strong>一个.param文件由以下几部分组成：</strong></p><p><img src="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-1/ncnn2.png" alt></p><p><strong>1）MagicNum</strong></p><p>固定位7767517，可以通过这个数来判定版本 -&gt; 为什么这个数字，不知道问倪神去吧 </p><p>2）<strong>layer、blob个数</strong></p><p>上图示例的文件两个数字分别为：75、83</p><p>​    <strong>layer：我们知道神经网络是一层一层向前推进计算的，每一层我们用一个layer表示；</strong></p><p>​    <strong>blob：每一个layer都可能会有输入、输出，在ncnn中，它们统一用一个多维（3维）向量表示，我们称每一个输入、输出的原子为一个blob，并为它起名</strong></p><p>2.1.1.2 layer的描述</p><p>layer 在 .param 中是一个相对复杂的元素（从第3行起的每一行描述一个layer），所以我们把它单独抽出来进行说明。</p><p><strong>1）层类型</strong>     比如<strong>Input、Convolution、ReLU</strong></p><p><strong>2）层名</strong>       模型训练者为该层起得名字（毕竟相同类型的层可能多次使用，我们要区分它们）</p><p><strong>3）层输入输出  包含：层输入blob数量，层输出blob数量，层输入、输出blob的名称</strong></p><p><strong>4）层配置参数</strong></p><p>比如 <strong>卷积层（Convolution Layer）的 卷积核大小、步长信息</strong> 等</p><p>在 具体层里面都有一个函数: load_param, 从里面可以查询到相关信息。</p><p><strong>data层： 0=长 1=宽 3=通道</strong></p><p><strong>Convolution层 0=输出单元 1=卷积核大小  2=核膨胀[见膨胀卷积]    3=stride</strong>    </p><p>​                                 <strong>4=padding    5=是否存在偏置    6=权重数量</strong></p><p><strong>pooling 层    0=池化类型   1=卷积核大小   2=步长stride   3=padding   4=全局池化  5=padding类型</strong></p><p><strong>ReLU 层 0=0.000000 无参数</strong></p><p><strong>softmax 层 0=0 无参数</strong></p><p><strong>Concat Split Dropout 无参数</strong></p><p><strong>ConvolutionDepthWise    7=group 数目</strong></p><h6 id="3-2-读取"><a href="#3-2-读取" class="headerlink" title="3.2 读取"></a>3.2 读取</h6><p><strong>下面我们具体从代码的角度来看看如何读取这个文件的:(文件为 net.cpp/h 和 paramdict.cpp/h) 为了方便， 我们将 Vulkan 相关代码剔除掉。</strong></p><p><strong>net.h 主要是 Net 类的接口， 其中最重要的功能是实现 load_param(载入模型参数) 和 load_model(载入模型的数据) 功能</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// net.h</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span></span><span class="hljs-class">&#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-comment">// empty init</span>    Net();    <span class="hljs-comment">// clear and destroy</span>    ~Net();<span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>    <span class="hljs-comment">// register custom layer by layer type name</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 注册自定义类型层， 通过string类型名</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">register_custom_layer</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* type, layer_creator_func creator)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>    <span class="hljs-comment">// register custom layer by layer type</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 注册自定义层， 通过int类型的 layer 索引</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">register_custom_layer</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index, layer_creator_func creator)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STDIO</span><span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>    <span class="hljs-comment">// load network structure from plain param file</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 从文件指针中载入参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(FILE* fp)</span></span>;    <span class="hljs-comment">// 从 param 文件中载入参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* protopath)</span></span>;    <span class="hljs-comment">// 从 mem 中载入参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param_mem</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* mem)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>    <span class="hljs-comment">// load network structure from binary param file</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 从二进制文件指针中载入 param 参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param_bin</span><span class="hljs-params">(FILE* fp)</span></span>;    <span class="hljs-comment">// 从二进制文件中载入参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param_bin</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* protopath)</span></span>;    <span class="hljs-comment">// load network weight data from model file</span>    <span class="hljs-comment">// return 0 if success</span>    <span class="hljs-comment">// 从 file 指针中传入模型</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(FILE* fp)</span></span>;    <span class="hljs-comment">// 从二进制文件中载入模型</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* modelpath)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STDIO</span></span>    <span class="hljs-comment">// load network structure from external memory</span>    <span class="hljs-comment">// memory pointer must be 32-bit aligned</span>    <span class="hljs-comment">// return bytes consumed</span>    <span class="hljs-comment">// 从外部内存中载入参数</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>* mem)</span></span>;    <span class="hljs-comment">// reference network weight data from external memory</span>    <span class="hljs-comment">// weight data is not copied but referenced</span>    <span class="hljs-comment">// so external memory should be retained when used</span>    <span class="hljs-comment">// memory pointer must be 32-bit aligned</span>    <span class="hljs-comment">// return bytes consumed</span>    <span class="hljs-comment">// 从外部内存中载入网络权重</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>* mem)</span></span>;    <span class="hljs-comment">// unload network structure and weight data</span>    <span class="hljs-comment">// 清空网络结构</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">clear</span><span class="hljs-params">()</span></span>;    <span class="hljs-comment">// construct an Extractor from network</span>    <span class="hljs-comment">// 从网络构建一个执行器</span>    <span class="hljs-function">Extractor <span class="hljs-title">create_extractor</span><span class="hljs-params">()</span> <span class="hljs-keyword">const</span></span>;<span class="hljs-keyword">protected</span>:    <span class="hljs-keyword">friend</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Extractor</span>;</span> <span class="hljs-comment">// 外部 Extractor 接口</span><span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRIN</span>    <span class="hljs-comment">// 通过name查找blob对应的索引</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">find_blob_index_by_name</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* name)</span> <span class="hljs-keyword">const</span></span>;     <span class="hljs-comment">// 通过name查找对应的 layer 索引</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">find_layer_index_by_name</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* name)</span> <span class="hljs-keyword">const</span></span>;    <span class="hljs-comment">// 通过类型查找对应的 layer索引</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">custom_layer_to_index</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* type)</span></span>;    <span class="hljs-comment">// 通过类型创建layer</span>    <span class="hljs-function">Layer* <span class="hljs-title">create_custom_layer</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* type)</span></span>;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>    <span class="hljs-comment">// 通过 index 穿件 layer</span>    <span class="hljs-function">Layer* <span class="hljs-title">create_custom_layer</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index)</span></span>;    <span class="hljs-comment">// 前向推理层</span>    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">forward_layer</span><span class="hljs-params">(<span class="hljs-keyword">int</span> layer_index, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Mat&gt;&amp; blob_mats, Option&amp; opt)</span> <span class="hljs-keyword">const</span></span>;<span class="hljs-keyword">protected</span>:    <span class="hljs-comment">// blobs &amp; layers</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Blob&gt; blobs;    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;Layer*&gt; layers;    <span class="hljs-comment">// layers</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;layer_registry_entry&gt; custom_layer_registry;&#125;;</code></pre><p>在此我们可以先来看一下 blob类 ! 着重看一下，对应的生产者、消费者模型</p><pre><code class="hljs cpp"><span class="hljs-comment">// Blob 用于记录数据传输过程， producer 记录当前blob从那一层产生的，</span><span class="hljs-comment">// consumer 记录当前blob被哪些层调用:</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Blob</span></span><span class="hljs-class">&#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-comment">// empty</span>    Blob();<span class="hljs-keyword">public</span>:<span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>    <span class="hljs-comment">// blob name</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span> name;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>    <span class="hljs-comment">// layer index which produce this blob as output</span>    <span class="hljs-comment">// 生产者</span>    <span class="hljs-keyword">int</span> producer;    <span class="hljs-comment">// layer index which need this blob as input</span>    <span class="hljs-comment">// 消费者</span>    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; consumers;&#125;;</code></pre><p>然后我们打开 net.cpp 文件，来看一下 load_param 的具体实现:</p><pre><code class="hljs cpp"><span class="hljs-comment">// 从文件中载入 net 参数</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Net::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* protopath)</span></span><span class="hljs-function"></span>&#123;    FILE* fp = fopen(protopath, <span class="hljs-string">&quot;rb&quot;</span>);    <span class="hljs-keyword">if</span> (!fp)    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;fopen %s failed\n&quot;</span>, protopath);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;    <span class="hljs-comment">// 从文件指针中载入 param</span>    <span class="hljs-keyword">int</span> ret = load_param(fp);    fclose(fp);    <span class="hljs-keyword">return</span> ret;&#125;</code></pre><p>参数载入接口中， 调用了另外一个参数载入接口: load_param(FILE * fp)</p><p>(1) 读取 magic number, 通过判断 magic number 是否等于 7767517, 就可以判断当前param文件是否是最新的 param 文件 </p><pre><code class="hljs cpp"><span class="hljs-keyword">int</span> magic = <span class="hljs-number">0</span>;<span class="hljs-comment">// 读取 magic number</span><span class="hljs-keyword">int</span> nbr = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%d&quot;</span>, &amp;magic);<span class="hljs-comment">// 读取失败</span><span class="hljs-keyword">if</span> (nbr != <span class="hljs-number">1</span>)&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;issue with param file\n&quot;</span>);    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;&#125;<span class="hljs-comment">// 判断是否是最新的 magic number</span><span class="hljs-keyword">if</span> (magic != <span class="hljs-number">7767517</span>)&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;param is too old, please regenerate\n&quot;</span>);    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;&#125;</code></pre><p>(2) 解析出网络的 layer 层数和 blob 数目 </p><pre><code class="hljs cpp"><span class="hljs-comment">// 对 layer 和 blob 进行解析</span><span class="hljs-keyword">int</span> layer_count = <span class="hljs-number">0</span>;<span class="hljs-keyword">int</span> blob_count = <span class="hljs-number">0</span>;<span class="hljs-comment">// 层数 &amp;&amp; blob 数目</span>nbr = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%d %d&quot;</span>, &amp;layer_count, &amp;blob_count);<span class="hljs-comment">// 层数和 blob数读取失败</span><span class="hljs-keyword">if</span> (nbr != <span class="hljs-number">2</span> || layer_count &lt;= <span class="hljs-number">0</span> || blob_count &lt;= <span class="hljs-number">0</span>)&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;issue with param file\n&quot;</span>);    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;&#125;<span class="hljs-comment">// resize 网络的层数和blob数目</span>layers.resize((<span class="hljs-keyword">size_t</span>)layer_count);blobs.resize((<span class="hljs-keyword">size_t</span>)blob_count);</code></pre><p>(3) 遍历所有的 layer, 解析每层 layer 层的类型(layer type)、名称(layer name)、输入数目(bottom_count) 和 输出数目(top_count)</p><pre><code class="hljs cpp"><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;layer_count; i++)&#123;    <span class="hljs-keyword">int</span> nscan = <span class="hljs-number">0</span>;    <span class="hljs-comment">// layer 的类型和名称</span>    <span class="hljs-keyword">char</span> layer_type[<span class="hljs-number">257</span>];    <span class="hljs-keyword">char</span> layer_name[<span class="hljs-number">257</span>];    <span class="hljs-keyword">int</span> bottom_count = <span class="hljs-number">0</span>;    <span class="hljs-keyword">int</span> top_count = <span class="hljs-number">0</span>;    <span class="hljs-comment">// 读取层类型、名称。输入bottom数目和输出top数目</span>    nscan = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%256s %256s %d %d&quot;</span>, layer_type, layer_name, &amp;bottom_count, &amp;top_count);    <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">4</span>) <span class="hljs-comment">// 解析失败</span>    &#123;        <span class="hljs-keyword">continue</span>;    &#125;</code></pre><p>(4) 根据layer的类型， 创建 layer</p><pre><code class="hljs cpp"><span class="hljs-comment">// 创建 layer</span>Layer* layer = create_layer(layer_type);<span class="hljs-comment">// layer_type 不是默认类型</span><span class="hljs-keyword">if</span> (!layer)&#123;    <span class="hljs-comment">// 从自定义 layer 读取</span>    layer = create_custom_layer(layer_type);&#125;<span class="hljs-keyword">if</span> (!layer) <span class="hljs-comment">// 如果自定义 layer 中不存在当前类型的 layer </span>&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;layer %s not exists or registered\n&quot;</span>, layer_type);    clear();    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;&#125;<span class="hljs-comment">// 设置 layer 参数: layer的类型、名称、输入和输出      </span>layer-&gt;type = <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span>(layer_type);layer-&gt;name = <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span>(layer_name);</code></pre><p>(5) 在设置输入时，如果当前blob名不存在，就将当前blob名添加到net的blobs数组里面</p><pre><code class="hljs cpp">layer-&gt;bottoms.resize(bottom_count); <span class="hljs-comment">// layer的输入</span><span class="hljs-comment">// 解析 layer 的输入</span><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j=<span class="hljs-number">0</span>; j&lt;bottom_count; j++)&#123;    <span class="hljs-keyword">char</span> bottom_name[<span class="hljs-number">257</span>];    <span class="hljs-comment">// 解析 bottom的name</span>    nscan = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%256s&quot;</span>, bottom_name);    <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>)    &#123;        <span class="hljs-keyword">continue</span>;    &#125;    <span class="hljs-comment">// 按照 bottom 的name 查找对应 blob 的index</span>    <span class="hljs-keyword">int</span> bottom_blob_index = find_blob_index_by_name(bottom_name);    <span class="hljs-comment">// 如果没有找到 bottom_name 对应的 blob</span>    <span class="hljs-comment">// 将向 blobs 数组中插入一个名为 bottom_name 的 blob</span>    <span class="hljs-keyword">if</span> (bottom_blob_index == <span class="hljs-number">-1</span>)    &#123;        <span class="hljs-comment">// 设置第blob_index个blob 的参数</span>        Blob&amp; blob = blobs[blob_index];        <span class="hljs-comment">// blob的索引</span>        bottom_blob_index = blob_index;        <span class="hljs-comment">// 设置blob的name</span>        blob.name = <span class="hljs-built_in">std</span>::<span class="hljs-built_in">string</span>(bottom_name);        <span class="hljs-comment">// 更新全局的 blob 索引</span>        blob_index++;    &#125;    <span class="hljs-comment">// 设置当前的blob的参数</span>    Blob&amp; blob = blobs[bottom_blob_index];    <span class="hljs-comment">// 使用当前的blob记录传输关系， 第i层以当前blob为输入</span>    blob.consumers.push_back(i);    <span class="hljs-comment">// 第i层layer的第j个输入</span>    layer-&gt;bottoms[j] = bottom_blob_index;&#125;</code></pre><p> (6) 设置输出的过程和这个类似，在此不再赘述，最后就是<strong>参数载入了</strong>:</p><p>例如: <strong>conv1的参数: 0=64 1=3 11=3 5=1 6=1728</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">//解析 blob后面跟随的特定参数字典 pd</span><span class="hljs-keyword">int</span> pdlr = pd.load_param(fp);<span class="hljs-keyword">if</span> (pdlr != <span class="hljs-number">0</span>)&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict load_param failed\n&quot;</span>);    <span class="hljs-keyword">continue</span>;&#125;<span class="hljs-comment">// layer 载入 param</span><span class="hljs-keyword">int</span> lr = layer-&gt;load_param(pd);<span class="hljs-keyword">if</span> (lr != <span class="hljs-number">0</span>)&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;layer load_param failed\n&quot;</span>);    <span class="hljs-keyword">continue</span>;&#125;layers[i] = layer;</code></pre><p>这其中有两个重要的函数:</p><p>pd.load_param(fp) 和  layer_param(pd)。前者负责解析 .param 文件中特定的参数， 后者则是用解析的参数来构建对应的layer</p><p>(7) 用参数字典来解析layer相关参数！ 看一下这个自己构造layer， 以及解析的过程 </p><p>   在使用load_param接口载入参数时，需要用参数字典ParamDict来解析.param文件中的特定参数，那么参数字典具体如何进行解析的？我们首先看一下paramdict.h文件中定义的数据成员变量：</p><pre><code class="hljs cpp"><span class="hljs-comment">// parameters</span><span class="hljs-class"><span class="hljs-keyword">struct</span></span><span class="hljs-class">&#123;</span>    <span class="hljs-comment">// 是否已经被载入：1表示已载入</span>    <span class="hljs-keyword">int</span> loaded;    <span class="hljs-comment">// 单个值可能为整形也有可能为浮点型</span>    <span class="hljs-keyword">union</span> &#123; <span class="hljs-keyword">int</span> i; <span class="hljs-keyword">float</span> f; &#125;;    <span class="hljs-comment">// 还有可能是数组</span>    Mat v;&#125; params[NCNN_MAX_PARAM_COUNT];</code></pre><p>​    这里，NCNN_MAX_PARAM_COUNT大小为20，params是一个大小为32的结构体数组，即一行中特定参数数量不能超过20，当然，一般情况下也不会超过20。</p><pre><code class="hljs cpp"><span class="hljs-comment">// at most 20 parameters</span><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> NCNN_MAX_PARAM_COUNT 20</span></code></pre><p>​    然后，我们看一下paramdict.cpp源码，可以看到，这里会解析出当前行的index（id），也即是等号左边部分： </p><pre><code class="hljs cpp"><span class="hljs-comment">// 解析之后的 key=value 对</span><span class="hljs-keyword">int</span> id = <span class="hljs-number">0</span>;<span class="hljs-keyword">while</span> (<span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%d=&quot;</span>, &amp;id) == <span class="hljs-number">1</span>)&#123;    ...&#125;</code></pre><p> 在这里可以结合 <a href="https://github.com/Tencent/ncnn/wiki/param-and-model-file-structure">https://github.com/Tencent/ncnn/wiki/param-and-model-file-structure</a> 阅读源码：</p><p><strong>index-value 的规则为:</strong></p><ul><li>index为0~19: 对应整形或浮点型数据</li><li><p>index小于 -23000： 对应整形或浮点型数组, 等号右边第一个参数就是数组长度，后面顺序就是数组内容，[array size],int,int,…,int或[array size],float,float,…,float，例如：</p><p><strong>0=1 1=2.5 -23303=2,2.0,3.0</strong></p><p>index为 -23303，表明当前参数为数组，等号右边第一个参数为2，表明数组长度为2，后面2.0,3.0就是数组的内容</p></li></ul><pre><code class="hljs cpp"><span class="hljs-keyword">bool</span> is_array = id &lt;= <span class="hljs-number">-23300</span>; <span class="hljs-comment">// index &lt;= -23300：数组</span><span class="hljs-keyword">if</span> (is_array) <span class="hljs-comment">// 如果是数组</span>&#123;  <span class="hljs-comment">// 计算id</span>    id = -id - <span class="hljs-number">23300</span>;&#125;<span class="hljs-keyword">if</span> (is_array)  <span class="hljs-comment">// 如果当前参数是数组类型</span>&#123;    <span class="hljs-keyword">int</span> len = <span class="hljs-number">0</span>;  <span class="hljs-comment">// 数组长度</span>    <span class="hljs-keyword">int</span> nscan = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%d&quot;</span>, &amp;len);    <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>)             <span class="hljs-comment">// 等于1才表示读取成功</span>    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict read array length failed\n&quot;</span>);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;        params[id].v.create(len);  <span class="hljs-comment">// 创建数组：就是一个Mat</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; len; j++)    &#123;        <span class="hljs-keyword">char</span> vstr[<span class="hljs-number">16</span>]; <span class="hljs-comment">// 从二值文件中读取string</span>        nscan = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;,%15[^,\n ]&quot;</span>, vstr);        <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>) <span class="hljs-comment">// 如果读取失败</span>        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict read array element failed\n&quot;</span>);            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;         <span class="hljs-comment">// 是否为浮点型：看解析的字符串中是否存在&#x27;.&#x27;或&#x27;e&#x27;</span>        <span class="hljs-comment">// 小数点计数法和科学计数法</span>        <span class="hljs-keyword">bool</span> is_float = vstr_is_float(vstr);        <span class="hljs-comment">// 如果是浮点数</span>        <span class="hljs-keyword">if</span> (is_float)  <span class="hljs-comment">// vstr赋值给params[id].v[j]</span>        &#123;            <span class="hljs-keyword">float</span>* ptr = params[id].v;            nscan = <span class="hljs-built_in">sscanf</span>(vstr, <span class="hljs-string">&quot;%f&quot;</span>, &amp;ptr[j]);        &#125;        <span class="hljs-keyword">else</span>  <span class="hljs-comment">// vstr赋值给params[id].v[j]</span>        &#123;            <span class="hljs-keyword">int</span>* ptr = params[id].v;            nscan = <span class="hljs-built_in">sscanf</span>(vstr, <span class="hljs-string">&quot;%d&quot;</span>, &amp;ptr[j]);        &#125;        <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>)  <span class="hljs-comment">// 赋值失败</span>        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict parse array element failed\n&quot;</span>);            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;        &#125;    &#125;&#125;</code></pre><p>​    这里有个vstr_is_float函数，原理很简单，就是判断数字对应字符串中是否存在小数点’.’或字母’e’，对应小数的两种写法，一种正常的小数点表示法，一种是科学计数法。</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">bool</span> <span class="hljs-title">vstr_is_float</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span> vstr[<span class="hljs-number">16</span>])</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// look ahead for determine isfloat</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j=<span class="hljs-number">0</span>; j&lt;<span class="hljs-number">16</span>; j++)    &#123;        <span class="hljs-keyword">if</span> (vstr[j] == <span class="hljs-string">&#x27;\0&#x27;</span>)            <span class="hljs-keyword">break</span>;        <span class="hljs-keyword">if</span> (vstr[j] == <span class="hljs-string">&#x27;.&#x27;</span> || <span class="hljs-built_in">tolower</span>(vstr[j]) == <span class="hljs-string">&#x27;e&#x27;</span>)            <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;&#125;</code></pre><p>​    如果不是数组，直接读取即可：</p><pre><code class="hljs cpp"><span class="hljs-keyword">else</span>   <span class="hljs-comment">// 不是数组</span>&#123;    <span class="hljs-keyword">char</span> vstr[<span class="hljs-number">16</span>];    <span class="hljs-keyword">int</span> nscan = <span class="hljs-built_in">fscanf</span>(fp, <span class="hljs-string">&quot;%15s&quot;</span>, vstr); <span class="hljs-comment">// 直接将字符串赋值给vstr</span>    <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>)  <span class="hljs-comment">// 赋值失败</span>    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict read value failed\n&quot;</span>);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;    <span class="hljs-keyword">bool</span> is_float = vstr_is_float(vstr);  <span class="hljs-comment">// 判断是否为浮点数</span>    <span class="hljs-keyword">if</span> (is_float)  <span class="hljs-comment">// 将字符串中的值赋给参数字典</span>        nscan = <span class="hljs-built_in">sscanf</span>(vstr, <span class="hljs-string">&quot;%f&quot;</span>, &amp;params[id].f);    <span class="hljs-keyword">else</span>        nscan = <span class="hljs-built_in">sscanf</span>(vstr, <span class="hljs-string">&quot;%d&quot;</span>, &amp;params[id].i);    <span class="hljs-keyword">if</span> (nscan != <span class="hljs-number">1</span>)  <span class="hljs-comment">// 赋值失败</span>    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;ParamDict parse value failed\n&quot;</span>);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;&#125;params[id].loaded = <span class="hljs-number">1</span>;         <span class="hljs-comment">// 载入成功</span></code></pre><p>(8) 用参数字典来解析layer相关参数</p><p>如前所示, layer会根据参数字典来构造 layer</p><pre><code class="hljs cpp"> <span class="hljs-comment">// layer载入param</span><span class="hljs-keyword">int</span> lr = layer-&gt;load_param(pd);</code></pre><p>转到 layer层的 load_param 接口可以看到：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 载入参数：参数列表</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Layer::load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; <span class="hljs-comment">/*pd*/</span>)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>这里并没有实现，在layer.h头文件中有：</p><pre><code class="hljs cpp"><span class="hljs-comment">// load layer specific parameter from parsed dict</span><span class="hljs-comment">// return 0 if success</span><span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">load_param</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ParamDict&amp; pd)</span></span>;</code></pre><p> ! load_param实际上是一个虚函数，熟悉C++的同学应该知道，调用虚函数时，实际调用的是继承类的版本，那么到底如何调用的？，我们可以往回看，有这样一段代码：</p><pre><code class="hljs cpp">Layer* layer = create_layer(layer_type)  <span class="hljs-comment">// 创建layer</span><span class="hljs-keyword">if</span> (!layer) <span class="hljs-comment">// layer_type不是默认类型</span>&#123;       layer = create_custom_layer(layer_type);   <span class="hljs-comment">// 从自定义layer读取</span>&#125;<span class="hljs-keyword">if</span> (!layer)   <span class="hljs-comment">// 如果自定义layer中也不存在当前类型layer</span>&#123;    <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;layer %s not exists or registered\n&quot;</span>, layer_type);    clear();    <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;&#125;</code></pre><p>回到layer.cpp文件中，可以看到，代码中先找到当前层layer类型对应层注册器中类型的索引index。</p><p><strong>layer_type -&gt; index -&gt; create_layer</strong></p><pre><code class="hljs cpp"> <span class="hljs-comment">// 将string对应layer类型转换成对应index</span> <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">layer_to_index</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* type)</span></span><span class="hljs-function"> </span>&#123;     <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;layer_registry_entry_count; i++)     &#123;         <span class="hljs-keyword">if</span> (<span class="hljs-built_in">strcmp</span>(type, layer_registry[i].name) == <span class="hljs-number">0</span>)             <span class="hljs-keyword">return</span> i;     &#125;     <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>; &#125;   <span class="hljs-comment">// 根据index创建layer：</span><span class="hljs-function">Layer* <span class="hljs-title">create_layer</span><span class="hljs-params">(<span class="hljs-keyword">int</span> index)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">if</span> (index &lt; <span class="hljs-number">0</span> || index &gt;= layer_registry_entry_count)     <span class="hljs-comment">// index不能超过索引范围</span>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;     layer_creator_func layer_creator = layer_registry[index].creator;     <span class="hljs-comment">// 创建layer构造器</span>    <span class="hljs-keyword">if</span> (!layer_creator)     <span class="hljs-comment">// layer构造器创建失败</span>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;     Layer* layer = layer_creator();     <span class="hljs-comment">// 构造layer</span>    layer-&gt;typeindex = index;    <span class="hljs-comment">// 设置layer的类型index</span>    <span class="hljs-keyword">return</span> layer;&#125;<span class="hljs-comment">// 根据字符串layer类型创建layer -&gt; 调用上面两个函数， 创建layer</span><span class="hljs-function">Layer* <span class="hljs-title">create_layer</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* type)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">int</span> index = layer_to_index(type);    <span class="hljs-keyword">if</span> (index == <span class="hljs-number">-1</span>)        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;     <span class="hljs-keyword">return</span> create_layer(index);&#125;</code></pre><p>line 18 有个layer_registry，其定义为：</p><pre><code class="hljs cpp"><span class="hljs-keyword">static</span> <span class="hljs-keyword">const</span> layer_registry_entry layer_registry[] =&#123;    #include <span class="hljs-string">&quot;layer_registry.h&quot;</span>&#125;;</code></pre><p>而这个”layer_registry.h”文件是在build项目的时候自动产生的，部分内容如下：</p><pre><code class="hljs cpp"> <span class="hljs-comment">// Layer Registry header</span> <span class="hljs-comment">//</span> <span class="hljs-comment">// This file is auto-generated by cmake, don&#x27;t edit it.</span>  <span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span> &#123;<span class="hljs-string">&quot;AbsVal&quot;</span>,AbsVal_final_layer_creator&#125;, <span class="hljs-meta">#<span class="hljs-meta-keyword">else</span></span> &#123;AbsVal_final_layer_creator&#125;, <span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span> <span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>&#123;<span class="hljs-string">&quot;ArgMax&quot;</span>,<span class="hljs-number">0</span>&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">else</span></span>&#123;<span class="hljs-number">0</span>&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span><span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>&#123;<span class="hljs-string">&quot;BatchNorm&quot;</span>,BatchNorm_final_layer_creator&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">else</span></span>&#123;BatchNorm_final_layer_creator&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span><span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>&#123;<span class="hljs-string">&quot;Bias&quot;</span>,Bias_final_layer_creator&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">else</span></span>&#123;Bias_final_layer_creator&#125;,<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span></span></code></pre><p>而 layer_registry_entry 的结构为：</p><pre><code class="hljs cpp"><span class="hljs-comment">// layer factory function</span><span class="hljs-keyword">typedef</span> Layer* (*layer_creator_func)(); <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">layer_registry_entry</span></span><span class="hljs-class">&#123;</span><span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STRING</span>    <span class="hljs-comment">// layer type name</span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* name;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STRING</span></span>    <span class="hljs-comment">// layer factory entry</span>    layer_creator_func creator;&#125;;</code></pre><p>我们代入一组参数进去就是：</p><pre><code class="hljs cpp">name = <span class="hljs-string">&quot;AbsVal&quot;</span>;layer_creator_func = AbsVal_final_layer_creator;</code></pre><p>这里layer_creator_func定义为：</p><pre><code class="hljs cpp"><span class="hljs-keyword">typedef</span> Layer* (*layer_creator_func)();</code></pre><p>那么，layer_creator_func AbsVal_final_layer_creator转换过去就是： </p><pre><code class="hljs isbl"><span class="hljs-variable">Layer</span>* <span class="hljs-function"><span class="hljs-title">AbsVal_final_layer_creator</span>()</span></code></pre><p>在layer.h文件最下面还有一个定义：</p><pre><code class="hljs cpp"><span class="hljs-comment">// ## 字符串连接</span><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> DEFINE_LAYER_CREATOR(name) \</span>    ::ncnn::Layer* name##_layer_creator() &#123; <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> name; &#125;</code></pre><p>我们在absval.cpp文件中可以看到 DEFINE_LAYER_CREATOR(AbsVal)，相当于就是声明了一个函数：</p><pre><code class="hljs cpp"><span class="hljs-comment">// #define DEFINE_LAYER_CREATOR(name) \</span><span class="hljs-comment">//    ::ncnn::Layer* name##_layer_creator() &#123; return new name; &#125;</span><span class="hljs-comment">// 由上面这段代码可知，DEFINE_LAYER_CREATOR(AbsVal)等价于：</span>::<span class="hljs-function">ncnn::Layer* <span class="hljs-title">AbsVal_layer_creator</span><span class="hljs-params">()</span> </span>&#123; <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> AbsVal; &#125;</code></pre><p>​    上面那句话相当于就是new了一个AbsVal层，但是这里还是对应不起来，上面的是 AbsVal_final_layer_creator()，这里声明的是AbsVal_layer_creator()，这里就涉及到ncnn还有一层继承，使用cmake编译ncnn项目后，除了生成了layer_registry.h文件之外，还生成了一个layer_declaration.h文件，打开这个文件，一切就清楚了：</p><pre><code class="hljs cpp"><span class="hljs-comment">// Layer Declaration header</span><span class="hljs-comment">//</span><span class="hljs-comment">// This file is auto-generated by cmake, don&#x27;t edit it.</span> <span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer/absval.h&quot;</span></span><span class="hljs-keyword">namespace</span> ncnn &#123;    <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AbsVal_final</span> :</span> <span class="hljs-keyword">virtual</span> <span class="hljs-keyword">public</span> AbsVal&#123;<span class="hljs-keyword">public</span>:    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">create_pipeline</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Option&amp; opt)</span> </span>&#123;        &#123; <span class="hljs-keyword">int</span> ret = AbsVal::create_pipeline(opt); <span class="hljs-keyword">if</span> (ret) <span class="hljs-keyword">return</span> ret; &#125;        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    &#125;    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">destroy_pipeline</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Option&amp; opt)</span> </span>&#123;        &#123; <span class="hljs-keyword">int</span> ret = AbsVal::destroy_pipeline(opt); <span class="hljs-keyword">if</span> (ret) <span class="hljs-keyword">return</span> ret; &#125;        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    &#125;&#125;;DEFINE_LAYER_CREATOR(AbsVal_final)&#125; <span class="hljs-comment">// namespace ncnn</span> <span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;layer/batchnorm.h&quot;</span></span><span class="hljs-keyword">namespace</span> ncnn &#123;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BatchNorm_final</span> :</span> <span class="hljs-keyword">virtual</span> <span class="hljs-keyword">public</span> BatchNorm&#123;<span class="hljs-keyword">public</span>:    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">create_pipeline</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Option&amp; opt)</span> </span>&#123;        &#123; <span class="hljs-keyword">int</span> ret = BatchNorm::create_pipeline(opt); <span class="hljs-keyword">if</span> (ret) <span class="hljs-keyword">return</span> ret; &#125;        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    &#125;    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-keyword">int</span> <span class="hljs-title">destroy_pipeline</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Option&amp; opt)</span> </span>&#123;        &#123; <span class="hljs-keyword">int</span> ret = BatchNorm::destroy_pipeline(opt); <span class="hljs-keyword">if</span> (ret) <span class="hljs-keyword">return</span> ret; &#125;        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    &#125;&#125;;DEFINE_LAYER_CREATOR(BatchNorm_final)&#125; <span class="hljs-comment">// namespace ncnn</span></code></pre><p>​    AbsVal_final层继承了AbsVal层，如果当前操作系统不是linux系统，就会将create_pipeline()和destroy_pipeline()抽象出来，具体调用时，就调用对应优化了的代码。那么layer载入ParamDict具体实现就对应于各个layer的载入流程了。</p><h6 id="3-3-bin文件"><a href="#3-3-bin文件" class="headerlink" title="3.3 bin文件"></a>3.3 bin文件</h6><p>​    前面已经大致总结了ncnn的param文件载入，根据param文件创建网络结构，然后通过bin文件载入每一层对应的网络参数。这里就总结一下，如何载入每一层的参数：</p><p>​        我们常用的网络参数载入的接口为：</p><pre><code class="hljs reasonml"><span class="hljs-comment">// 从二进制文件中载入模型</span><span class="hljs-built_in">int</span> load<span class="hljs-constructor">_model(<span class="hljs-params">const</span> <span class="hljs-params">char</span><span class="hljs-operator">*</span> <span class="hljs-params">modelpath</span>)</span>;</code></pre><p>​    找到对应net.cpp文件实现部分有：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 从二进制文件中载入模型</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Net::load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> <span class="hljs-keyword">char</span>* modelpath)</span></span><span class="hljs-function"></span>&#123;    FILE* fp = fopen(modelpath, <span class="hljs-string">&quot;rb&quot;</span>);    <span class="hljs-keyword">if</span> (!fp)    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;fopen %s failed\n&quot;</span>, modelpath);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;     <span class="hljs-keyword">int</span> ret = load_model(fp);     fclose(fp);     <span class="hljs-keyword">return</span> ret;&#125;</code></pre><p>和载入模型参数一样，ncnn模型载入这里调用了另外一个接口，从文件指针载入权重参数：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 从文件指针载入模型</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">Net::load_model</span><span class="hljs-params">(FILE* fp)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">if</span> (layers.empty()) <span class="hljs-comment">// 判断当前layer是否为空</span>    &#123;        <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;network graph not ready\n&quot;</span>);        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;    &#125;        <span class="hljs-keyword">int</span> ret = <span class="hljs-number">0</span>;     <span class="hljs-comment">// load file</span>    <span class="hljs-function">ModelBinFromStdio <span class="hljs-title">mb</span><span class="hljs-params">(fp)</span></span>;     <span class="hljs-comment">// 从二进制文件读取</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">size_t</span> i=<span class="hljs-number">0</span>; i&lt;layers.size(); i++)     <span class="hljs-comment">// 遍历所有的层</span>    &#123;        Layer* layer = layers[i]; <span class="hljs-comment">// 读取第i层</span>        <span class="hljs-comment">//Here we found inconsistent content in the parameter file.</span>        <span class="hljs-keyword">if</span> (!layer)&#123;    <span class="hljs-comment">// 如果第i层不存在</span>            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;load_model error at layer %d, parameter file has inconsistent content.\n&quot;</span>, (<span class="hljs-keyword">int</span>)i);            ret = <span class="hljs-number">-1</span>;            <span class="hljs-keyword">break</span>;        &#125;         <span class="hljs-comment">// 载入模型参数</span>        <span class="hljs-keyword">int</span> lret = layer-&gt;load_model(mb);        <span class="hljs-keyword">if</span> (lret != <span class="hljs-number">0</span>)        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;layer load_model %d failed\n&quot;</span>, (<span class="hljs-keyword">int</span>)i);            ret = <span class="hljs-number">-1</span>;            <span class="hljs-keyword">break</span>;        &#125;         <span class="hljs-keyword">int</span> cret = layer-&gt;create_pipeline(opt);  <span class="hljs-comment">// 从opt处创建网络的pipline</span>        <span class="hljs-keyword">if</span> (cret != <span class="hljs-number">0</span>) <span class="hljs-comment">// 如果创建第i层的pipline失败</span>        &#123;            <span class="hljs-built_in">fprintf</span>(<span class="hljs-built_in">stderr</span>, <span class="hljs-string">&quot;layer create_pipeline %d failed\n&quot;</span>, (<span class="hljs-keyword">int</span>)i);            ret = <span class="hljs-number">-1</span>;            <span class="hljs-keyword">break</span>;        &#125;    &#125;     <span class="hljs-comment">// 网络复用</span>    fuse_network();    <span class="hljs-keyword">return</span> ret;&#125;</code></pre><p>​    按照代码注释，应该还是比较好懂得，这里需要解析两个部分，第一个部分为<strong>ModelBinFromStdio</strong>，对应于二进制模型文件解析，另外一部分为 <strong>layer-&gt;load_model(mb)</strong>，对应于具体某个层的参数载入：</p><p>​    （1）二进制模型文件解析</p><p>​    这里对应于modelbin.h和modelbin.cpp文件，首先看一下modelbin.h文件：</p><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ModelBin</span></span><span class="hljs-class">&#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-keyword">virtual</span> ~ModelBin();    <span class="hljs-comment">// element type</span>    <span class="hljs-comment">// 0 = auto</span>    <span class="hljs-comment">// 1 = float32</span>    <span class="hljs-comment">// 2 = float16</span>    <span class="hljs-comment">// 3 = int8</span>    <span class="hljs-comment">// load vec</span>    <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span> </span>= <span class="hljs-number">0</span>;    <span class="hljs-comment">// load image</span>    <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> h, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span>;    <span class="hljs-comment">// load dim</span>    <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> h, <span class="hljs-keyword">int</span> c, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span>;&#125;; <span class="hljs-meta">#<span class="hljs-meta-keyword">if</span> NCNN_STDIO</span><span class="hljs-comment">// 载入模型参数到一个Mat中</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ModelBinFromStdio</span> :</span> <span class="hljs-keyword">public</span> ModelBin&#123;<span class="hljs-keyword">public</span>:    <span class="hljs-comment">// construct from file</span>    ModelBinFromStdio(FILE* binfp);     <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span>; <span class="hljs-keyword">protected</span>:    FILE* binfp;&#125;;<span class="hljs-meta">#<span class="hljs-meta-keyword">endif</span> <span class="hljs-comment">// NCNN_STDIO</span></span> <span class="hljs-comment">// 载入模型参数到一个Mat中</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ModelBinFromMemory</span> :</span> <span class="hljs-keyword">public</span> ModelBin&#123;<span class="hljs-keyword">public</span>:    <span class="hljs-comment">// construct from external memory</span>    ModelBinFromMemory(<span class="hljs-keyword">const</span> <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>*&amp; mem);     <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span>; <span class="hljs-keyword">protected</span>:    <span class="hljs-keyword">const</span> <span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">char</span>*&amp; mem;&#125;; <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ModelBinFromMatArray</span> :</span> <span class="hljs-keyword">public</span> ModelBin&#123;<span class="hljs-keyword">public</span>:    <span class="hljs-comment">// construct from weight blob array</span>    ModelBinFromMatArray(<span class="hljs-keyword">const</span> Mat* weights);     <span class="hljs-function"><span class="hljs-keyword">virtual</span> Mat <span class="hljs-title">load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span>; <span class="hljs-keyword">protected</span>:    <span class="hljs-keyword">mutable</span> <span class="hljs-keyword">const</span> Mat* weights;&#125;;</code></pre><p>   找到对应实现部分，就是modelbin.cpp，可以看到，ModelBinFromStdio mb(fp);就是将文件指针传给binfp对象</p><pre><code class="hljs cpp">ModelBinFromStdio::ModelBinFromStdio(FILE* _binfp) : binfp(_binfp)&#123;&#125;</code></pre><p>​    下面再看一下layer载入参数，layer具体操作对应于具体类型的层操作，例如batchnorm，可以看到：</p><pre><code class="hljs cpp"><span class="hljs-comment">// 载入模型</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">BatchNorm::load_model</span><span class="hljs-params">(<span class="hljs-keyword">const</span> ModelBin&amp; mb)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-comment">// slope数据</span>    slope_data = mb.load(channels, <span class="hljs-number">1</span>);    <span class="hljs-comment">// 载入失败：返还-100</span>    <span class="hljs-keyword">if</span> (slope_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;     <span class="hljs-comment">// mean数据</span>    mean_data = mb.load(channels, <span class="hljs-number">1</span>);    <span class="hljs-comment">// 载入数据失败，返还-100</span>    <span class="hljs-keyword">if</span> (mean_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;     <span class="hljs-comment">// variance数据</span>    var_data = mb.load(channels, <span class="hljs-number">1</span>);    <span class="hljs-comment">// 载入数据失败，返还-100</span>    <span class="hljs-keyword">if</span> (var_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;     <span class="hljs-comment">// bias数据</span>    bias_data = mb.load(channels, <span class="hljs-number">1</span>);    <span class="hljs-comment">// 载入数据失败，返还-100</span>    <span class="hljs-keyword">if</span> (bias_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;     <span class="hljs-comment">// 创建矩阵</span>    a_data.create(channels);    <span class="hljs-keyword">if</span> (a_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;    <span class="hljs-comment">// 创建矩阵</span>    b_data.create(channels);    <span class="hljs-keyword">if</span> (b_data.empty())        <span class="hljs-keyword">return</span> <span class="hljs-number">-100</span>;     <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;channels; i++)    &#123;        <span class="hljs-comment">// sqrt variance</span>        <span class="hljs-keyword">float</span> sqrt_var = <span class="hljs-built_in">sqrt</span>(var_data[i] + eps);        a_data[i] = bias_data[i] - slope_data[i] * mean_data[i] / sqrt_var;        b_data[i] = slope_data[i] / sqrt_var;    &#125;     <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>实际上调用的是ModelBinFromStdio 的load接口：</p><pre><code class="hljs cpp"><span class="hljs-function">Mat <span class="hljs-title">ModelBinFromStdio::load</span><span class="hljs-params">(<span class="hljs-keyword">int</span> w, <span class="hljs-keyword">int</span> type)</span> <span class="hljs-keyword">const</span></span></code></pre><p>后面type对应有四种类型：auto，float32，float16和int8</p><pre><code class="hljs cpp"><span class="hljs-comment">// 0 = auto</span><span class="hljs-comment">// 1 = float32</span><span class="hljs-comment">// 2 = float16</span><span class="hljs-comment">// 3 = int8</span></code></pre><p> 然后，根据这四种类型进行模型参数载入，感觉没什么好说的，主要是里面有个alignSize函数需要做个笔记：</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">static</span> <span class="hljs-keyword">inline</span> <span class="hljs-keyword">size_t</span> <span class="hljs-title">alignSize</span><span class="hljs-params">(<span class="hljs-keyword">size_t</span> sz, <span class="hljs-keyword">int</span> n)</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">return</span> (sz + n<span class="hljs-number">-1</span>) &amp; -n;&#125;</code></pre><p>alignSize就是申请sz大小的内存，实际申请内存是 y =(sz+n-1)&amp;-n 大小的内存，y &gt;= sz，且y是n的整数倍，然后对(sz+n-1)&amp; -n的解释是：</p><p>​    假设n为16，-n就是0xfffffff0，(sz+n-1)，加这个n-1一是为了保证sz刚好是16的倍数不会多算，二十为了防止不是16的倍数会少算，如，sz=3, 就是从二进制角度舍弃19小于16部分。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ncnn源码分析_0</title>
    <link href="/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-0/"/>
    <url>/2020/09/17/ncnn%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-0/</url>
    
    <content type="html"><![CDATA[<p>ncnn 源码分析第一篇</p><a id="more"></a><p>ncnn 是一个架构比较简单的深度学习推导框架。 其项目目录如下所示:</p><pre><code class="hljs ncnn">.├── CMakeLists.txt├── CONTRIBUTING.md├── Info.plist├── LICENSE.txt├── README.md├── benchmark├── build.sh├── cmake├── docs    &#x2F;&#x2F; 文档├── examples    &#x2F;&#x2F; 实例├── images   &#x2F;&#x2F; 图片├── package.sh├── src      &#x2F;&#x2F; 主要的源代码│   ├── arm &#x2F;&#x2F; 针对于 arm 平台的优化│   ├── mips &#x2F;&#x2F; 针对于 mips 平台的优化│   ├── vulkan &#x2F;&#x2F; GPU 优化代码│   ├── x86 &#x2F;&#x2F; 针对于 x86 平台的优化├── ... &#x2F;&#x2F; 通用平台代码└── xxx.h├── tests  &#x2F;&#x2F; 测试├── toolchains└── tools   &#x2F;&#x2F; 工具: (1) 其他模型 -&gt; ncnn  (2) 模型优化和量化</code></pre><p>本系列旨在于去深入了解 ncnn 的内部机理， 主要分为如下几个部分：</p><p>ncnn 源码分析_1  参数与模型载入</p><p>ncnn 源码分析_2 Extractor</p><p>ncnn 源码分析_3 模型量化原理</p><p>ncnn 源码分析_4 模型量化源码</p><p>ncnn 源码分析_5 添加 layer</p><p>ncnn 源码分析_6 数据的读取与处理</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>mxnet2ncnn</title>
    <link href="/2020/09/17/mxnet2ncnn/"/>
    <url>/2020/09/17/mxnet2ncnn/</url>
    
    <content type="html"><![CDATA[<p>mxnet2ncnn 部署方案</p><a id="more"></a><h4 id="1-首先下载并编译-ncnn-框架"><a href="#1-首先下载并编译-ncnn-框架" class="headerlink" title="1. 首先下载并编译 ncnn 框架"></a>1. 首先下载并编译 ncnn 框架</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> install xcode and protobuf</span><span class="hljs-meta">#</span><span class="bash"> install protobuf</span><span class="hljs-meta">$</span><span class="bash"> brew install protobuf</span><span class="hljs-meta">#</span><span class="bash"> build &amp; install ncnn</span><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> &lt;ncnn-root-dir&gt;</span><span class="hljs-meta">$</span><span class="bash"> mkdir -p build</span><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> build</span><span class="hljs-meta">$</span><span class="bash"> cmake -DNCNN_VULKAN=OFF ..</span><span class="hljs-meta">$</span><span class="bash"> make -j4</span><span class="hljs-meta">$</span><span class="bash"> make install</span></code></pre><h4 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h4><p>从如下地址下载 mobileface 的文件：</p><p><a href="https://pan.baidu.com/s/1If28BkHde4fiuweJrbicVA"><strong>https://pan.baidu.com/s/1If28BkHde4fiuweJrbicVA</strong></a></p><p>解压可以得到三个文件：  log   model-0000.params   model-symbol.json</p><p>执行：python model_slim.py  —model  ../models/model-y1-test2/model,0</p><p>此时 会生成两个文件：../models/model-y1-test2/models-</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>pytorch2ncnn</title>
    <link href="/2020/09/17/pytorch2ncnn/"/>
    <url>/2020/09/17/pytorch2ncnn/</url>
    
    <content type="html"><![CDATA[<p>借助 onnx 实现 pytorch 到 ncnn 的转换</p><a id="more"></a><p><strong>1. 安装 onnx</strong> </p><pre><code class="hljs shell">pip3 install onnx</code></pre><p><strong>2. pytorch model &gt; onnx</strong></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> models.pfld <span class="hljs-keyword">import</span> PFLDInference, AuxiliaryNet<span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable<span class="hljs-keyword">import</span> onnx<span class="hljs-keyword">import</span> torchcheckpoint = torch.load(<span class="hljs-string">&quot;./checkpoint/snapshot/checkpoint.pth.tar&quot;</span>, map_location=<span class="hljs-string">&#x27;cpu&#x27;</span>)pfld_backbone = PFLDInference()pfld_backbone.load_state_dict(checkpoint[<span class="hljs-string">&#x27;pfld_backbone&#x27;</span>])dummy_input = Variable(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">112</span>, <span class="hljs-number">112</span>)) torch.onnx.export(plfd_backbone, dummy_input, <span class="hljs-string">&quot;output/plfd.onnx&quot;</span>)model = onnx.load(<span class="hljs-string">&#x27;output/pfld.onnx&#x27;</span>)</code></pre><p><strong>3. 安装ncnn</strong></p><p>(1) 安装 protobuf</p><pre><code class="hljs shell">unzip protobuf-all-3.7.1.zipcd protobuf-all-3.7.1./configure --prefix=/usr/localmake -j4make check -j4sudo make installsudo ldconfig</code></pre><p>(2) 安装 opencv， opencv 这里用的是2.4.13.6</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment"># 源码编译</span></span>unzip opencv-2.4.13.6.zipcd opencv-2.4.13.6mkdir buildcd buildcmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local ..make -j8sudo make installsudo ldconfig</code></pre><p>(3) 安装依赖项</p><pre><code class="hljs shell">sudo apt-get install build-essentialsudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-devsudo apt-get install  libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev</code></pre><p>(4) 把ncnn的源码clone下来，我们这里主要想使用tools下面的onnx转换工具。</p><p>这里作一点修改，pytorch1.0之后支持0维的张量，这在ncnn转换中会出现问题，修改onnx2ncnn.cpp中Constant和MemoryData的转换，<strong>有2处</strong></p><pre><code class="hljs cpp"><span class="hljs-keyword">if</span> (M.dims_size() == <span class="hljs-number">1</span>) &#123;   <span class="hljs-built_in">fprintf</span>(pp, <span class="hljs-string">&quot; 0=%d&quot;</span>, (<span class="hljs-keyword">int</span>)M.dims(<span class="hljs-number">0</span>));&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (M.dims_size() == <span class="hljs-number">2</span>) &#123;    <span class="hljs-built_in">fprintf</span>(pp, <span class="hljs-string">&quot; 0=%d&quot;</span>, (<span class="hljs-keyword">int</span>)M.dims(<span class="hljs-number">1</span>));    <span class="hljs-built_in">fprintf</span>(pp, <span class="hljs-string">&quot; 1=%d&quot;</span>, (<span class="hljs-keyword">int</span>)M.dims(<span class="hljs-number">0</span>));&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (M.dims_size() == <span class="hljs-number">3</span>) &#123;    <span class="hljs-built_in">fprintf</span>(pp, <span class="hljs-string">&quot; 0=%d&quot;</span>, (<span class="hljs-keyword">int</span>)M.dims(<span class="hljs-number">2</span>));    <span class="hljs-built_in">fprintf</span>(pp, <span class="hljs-string">&quot; 1=%d&quot;</span>, (<span class="hljs-keyword">int</span>)M.dims(<span class="hljs-number">1</span>));    <span class="hljs-built_in">fprintf</span>(pp, <span class="hljs-string">&quot; 2=%d&quot;</span>, (<span class="hljs-keyword">int</span>)M.dims(<span class="hljs-number">0</span>));&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (get_tensor_proto_data_size(M)==<span class="hljs-number">1</span>) &#123;    <span class="hljs-comment">// scalar tensor!!! </span>    <span class="hljs-built_in">fprintf</span>(pp, <span class="hljs-string">&quot; 0=1&quot;</span>);&#125;</code></pre><p>(3) 编译 ncnn</p><pre><code class="hljs shell">cd ncnnmkdir -p buildcd buildcmake ..make -j4</code></pre><p>这样在 build/tools/onnx/ 目录下就有转换工具 onnx2ncnn 了</p><p><strong>4. 简化一些onnx 模型</strong></p><p>先别急着转换，onnx转换模型时有一些冗余，我们用工具简化一些onnx模型</p><pre><code class="hljs shell">pip3 install onnx-simplifierpython3 -m onnxsim pfld.onnx pfld-sim.onnx</code></pre><p><strong>5.  onnx &gt; ncnn</strong></p><pre><code class="hljs shell">cd ~/xxx/ncnn/build/tools/onnx/./onnx2ncnn pfld-sim.onnx pfld-sim.param pfld-sim.bin</code></pre><p>现在就生成了两个文件可以供使用: <strong>pfld-sim.bin</strong> 和 <strong>pfld-sim.param</strong></p><p><strong>6. 编写一个可以供调用的ncnn文件进行调用即可。</strong></p><p><strong>错误信息:</strong></p><p><strong>Q: Python中Import Error: no module named ‘past’错误以及解决方法</strong></p><pre><code class="hljs shell">pip3 install future</code></pre>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>3D点云综述</title>
    <link href="/2020/09/17/3D%E7%82%B9%E4%BA%91%E7%BB%BC%E8%BF%B0/"/>
    <url>/2020/09/17/3D%E7%82%B9%E4%BA%91%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>探索</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>图像处理基础知识</title>
    <link href="/2020/09/17/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <url>/2020/09/17/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>暗光增强</title>
    <link href="/2020/09/17/%E6%9A%97%E5%85%89%E5%A2%9E%E5%BC%BA/"/>
    <url>/2020/09/17/%E6%9A%97%E5%85%89%E5%A2%9E%E5%BC%BA/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>智能图像处理</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>leetcode</title>
    <link href="/2020/09/13/leetcode/"/>
    <url>/2020/09/13/leetcode/</url>
    
    <content type="html"><![CDATA[<h2 id="Leetcode-刷题记录-（一）"><a href="#Leetcode-刷题记录-（一）" class="headerlink" title="Leetcode 刷题记录 （一）"></a>Leetcode 刷题记录 （一）</h2><p>主要用来记录在刷题过程中的心得体会， 题目的解法， 容易犯的错误和题目分类。估计以后可能不会再刷题了吧。 🙂</p><p>这部分主要是关于三个知识点: 栈/堆/队列、位运算、链表。</p><a id="more"></a><h3 id="1-栈、堆、队列"><a href="#1-栈、堆、队列" class="headerlink" title="1. 栈、堆、队列"></a>1. 栈、堆、队列</h3><p><strong>常见技巧总结</strong></p><ul><li>不要对空容器(栈、堆、队列进行操作)</li><li>单调栈/队列的应用</li></ul><p><strong>栈 : 先进后出</strong></p><pre><code class="hljs c++"><span class="hljs-built_in">stack</span>&lt;<span class="hljs-keyword">int</span>&gt; m_stack;size()  empty()push()     pop()     top()</code></pre><p><strong>队列：先进先出</strong></p><pre><code class="hljs cpp"><span class="hljs-built_in">queue</span>&lt;<span class="hljs-keyword">int</span>&gt; m_queue;size()    empty()push()     pop()     front()</code></pre><p><strong>双端队列</strong></p><pre><code class="hljs cpp"><span class="hljs-built_in">deque</span>&lt;<span class="hljs-keyword">int</span>&gt; m_deque;size()    empty()push_back()   pop_back()    pop_back()   pop_front()    front()    back()</code></pre><p><strong>优先队列</strong></p><p>实现：二叉堆， 最大(小)值 先出的完全二叉树</p><pre><code class="hljs cpp"><span class="hljs-built_in">priority_queue</span>&lt;<span class="hljs-keyword">int</span>&gt; max_heap;      <span class="hljs-comment">// 默认构造最大堆</span><span class="hljs-built_in">priority_queue</span>&lt;<span class="hljs-keyword">int</span>, <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;, less&lt;<span class="hljs-keyword">int</span>&gt; &gt; max_heap;     <span class="hljs-comment">// 构造最大堆</span><span class="hljs-built_in">priority_queue</span>&lt;<span class="hljs-keyword">int</span>, <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;, greater&lt;<span class="hljs-keyword">int</span>&gt; &gt; min_heap;   <span class="hljs-comment">// 构造最小堆 </span>size()    empty()push()     pop()     top()</code></pre><p><strong>[155] <a href="https://leetcode-cn.com/problems/min-stack/">https://leetcode-cn.com/problems/min-stack/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// 解题思路:</span><span class="hljs-comment">// 使用辅助栈记录栈的最小值</span><span class="hljs-comment">// 当添加元素时， 将待添加元素与辅助栈栈顶元素进行比较</span><span class="hljs-comment">//     当栈顶元素小于待添加元素 -&gt; 辅助栈添加栈顶元素</span><span class="hljs-comment">//     当栈顶元素大于待添加元素 -&gt; 辅助栈添加待添加元素</span><span class="hljs-comment">// !! 题目默认各种操作合法:即不回对空栈进行top、min和 pop 操作，所以代码中没有进行相关判定</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MinStack</span> &#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-comment">/** initialize your data structure here. */</span>    MinStack() &#123;    &#125;        <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">push</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x)</span> </span>&#123;        m_stk.push(x);        <span class="hljs-keyword">if</span>(min_stk.empty())   min_stk.push(x);        <span class="hljs-keyword">else</span> min_stk.push(<span class="hljs-built_in">std</span>::min(min_stk.top(), x));    &#125;        <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">pop</span><span class="hljs-params">()</span> </span>&#123;        m_stk.pop();        min_stk.pop();    &#125;        <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">top</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> m_stk.top();    &#125;        <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">min</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> min_stk.top();    &#125;<span class="hljs-keyword">private</span>:    <span class="hljs-built_in">stack</span>&lt;<span class="hljs-keyword">int</span>&gt; min_stk;    <span class="hljs-built_in">stack</span>&lt;<span class="hljs-keyword">int</span>&gt; m_stk;&#125;;</code></pre><p><strong>[232] <a href="https://leetcode.com/problems/implement-queue-using-stacks/">https://leetcode.com/problems/implement-queue-using-stacks/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// 解题思路:</span><span class="hljs-comment">// (1) 申请两个栈 data_stack 和 m_stack</span><span class="hljs-comment">// (2) 当入队(添加数据)时, 执行如下三个步骤</span><span class="hljs-comment">//      - 将 data_stack 中的数据逐个 放到 m_stack 中</span><span class="hljs-comment">//      - 将 value 添加到 m_stack 中</span><span class="hljs-comment">//      - 将 m_stack 中的数据 逐个放入到 data_stack 中 </span><span class="hljs-comment">// (3) 当出队(删除数据)时， 直接弹出 data_stack 栈顶数据即可    </span><span class="hljs-keyword">private</span>:    <span class="hljs-built_in">stack</span>&lt;<span class="hljs-keyword">int</span>&gt; data_stack;    <span class="hljs-built_in">stack</span>&lt;<span class="hljs-keyword">int</span>&gt; m_stack;<span class="hljs-keyword">public</span>:    CQueue() &#123;    &#125;        <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">appendTail</span><span class="hljs-params">(<span class="hljs-keyword">int</span> value)</span> </span>&#123;        <span class="hljs-keyword">while</span>(!data_stack.empty())&#123;            m_stack.push(data_stack.top());            data_stack.pop();        &#125;        data_stack.push(value);        <span class="hljs-keyword">while</span>(!m_stack.empty())&#123;            data_stack.push(m_stack.top());            m_stack.pop();        &#125;    &#125;        <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">deleteHead</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">if</span>(data_stack.empty()) <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>; <span class="hljs-comment">// 边界条件!  栈空!</span>              <span class="hljs-keyword">int</span> x = data_stack.top();        data_stack.pop();        <span class="hljs-keyword">return</span> x;    &#125;</code></pre><p><strong>[946] <a href="https://leetcode-cn.com/problems/validate-stack-sequences/">https://leetcode-cn.com/problems/validate-stack-sequences/</a></strong></p><pre><code class="hljs arduino"><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">validateStackSequences</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; pushed, <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; popped)</span> </span>&#123;    <span class="hljs-built_in">stack</span>&lt;<span class="hljs-keyword">int</span>&gt; m_stk;    <span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>;  i &lt; pushed.<span class="hljs-built_in">size</span>(); i++)&#123;        m_stk.push(pushed[i]);        <span class="hljs-keyword">while</span>(!m_stk.empty() &amp;&amp; m_stk.top() == popped[j])&#123;            j++;            m_stk.pop();        &#125;    &#125;    <span class="hljs-keyword">return</span> m_stk.empty();&#125;</code></pre><p><strong>[259] <a href="https://leetcode-cn.com/problems/find-median-from-data-stream/">https://leetcode-cn.com/problems/find-median-from-data-stream/</a></strong></p><p><strong>[lcof 59]<a href="https://leetcode-cn.com/problems/find-median-from-data-stream/">https://leetcode-cn.com/problems/find-median-from-data-stream/</a></strong></p><p><strong>[239] <a href="https://leetcode-cn.com/problems/sliding-window-maximum/">https://leetcode-cn.com/problems/sliding-window-maximum/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// 使用了单调双向队列</span><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">maxSlidingWindow</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> k)</span> </span>&#123;    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; res;    <span class="hljs-built_in">deque</span>&lt;<span class="hljs-keyword">int</span>&gt; m_deque;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; nums.size(); i++)&#123;        <span class="hljs-keyword">while</span>(!m_deque.empty() &amp;&amp; nums[i] &gt; nums[m_deque.back()])&#123;            m_deque.pop_back();        &#125;        m_deque.push_back(i);   <span class="hljs-comment">// 这里进入队列的是 index， 而不是 value</span>        <span class="hljs-keyword">if</span>(i - m_deque.front() &gt;= k) m_deque.pop_front();        <span class="hljs-keyword">if</span>(i &gt;= k<span class="hljs-number">-1</span>) res.push_back(nums[m_deque.front()]);    &#125;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><div class="table-container"><table><thead><tr><th>面试题 08.14</th><th><a href="https://leetcode-cn.com/problems/boolean-evaluation-lcci">布尔运算</a></th></tr></thead><tbody><tr><td>面试题 17.21</td><td><a href="https://leetcode-cn.com/problems/volume-of-histogram-lcci">直方图的水量</a></td></tr><tr><td>225</td><td><a href="https://leetcode.com/problems/implement-stack-using-queues/description/">Implement Stack using Queues</a></td></tr><tr><td>150</td><td><a href="https://leetcode.com/problems/evaluate-reverse-polish-notation/description/">Evaluate Reverse Polish Notation</a></td></tr><tr><td>71</td><td><a href="https://leetcode.com/problems/simplify-path/description/">Simplify Path</a></td></tr><tr><td>388</td><td><a href="https://leetcode.com/problems/longest-absolute-file-path/description/">Longest Absolute File Path</a></td></tr><tr><td>394</td><td><a href="https://leetcode.com/problems/decode-string/">Decode String</a></td></tr><tr><td>224</td><td><a href="https://leetcode.com/problems/basic-calculator/description/">Basic Calculator</a></td></tr><tr><td>227</td><td><a href="https://leetcode.com/problems/basic-calculator-ii/description/">Basic Calculator II</a></td></tr><tr><td>385</td><td><a href="https://leetcode.com/problems/mini-parser/description/">Mini Parser</a></td></tr><tr><td>84</td><td><a href="https://leetcode.com/problems/largest-rectangle-in-histogram/description/">Largest Rectangle in Histogram</a></td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>题号</th><th>题目链接</th><th></th></tr></thead><tbody><tr><td>215</td><td><a href="https://leetcode.com/problems/kth-largest-element-in-an-array/description/">Kth Largest Element in an Array</a></td><td></td></tr><tr><td>347</td><td><a href="https://leetcode.com/problems/top-k-frequent-elements/description/">Top K Frequent Elements</a></td><td></td></tr><tr><td>313</td><td><a href="https://leetcode.com/problems/super-ugly-number/description/">Super Ugly Number</a></td><td>很少考</td></tr><tr><td>373</td><td><a href="https://leetcode.com/problems/find-k-pairs-with-smallest-sums/description/">Find K Pairs with Smallest Sums</a></td><td>很少考</td></tr><tr><td>218</td><td><a href="https://leetcode.com/problems/the-skyline-problem/description/">The Skyline Problem</a></td><td></td></tr><tr><td>332</td><td><a href="https://leetcode.com/problems/reconstruct-itinerary/description/">Reconstruct Itinerary</a></td><td></td></tr><tr><td>341</td><td><a href="https://leetcode.com/problems/flatten-nested-list-iterator/">Flatten Nested List Iterator</a></td><td></td></tr><tr><td></td><td></td></tr></tbody></table></div><h3 id="2-位运算"><a href="#2-位运算" class="headerlink" title="2. 位运算"></a>2. 位运算</h3><p><img src="/2020/09/13/leetcode/bit.png" alt></p><h5 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h5><pre><code class="hljs cpp"><span class="hljs-comment">// &amp;   与    |   或    ~  非      ^   异或</span><span class="hljs-comment">// 最常见的几个用法</span><span class="hljs-comment">// (1) </span>n &amp; (n<span class="hljs-number">-1</span>)  <span class="hljs-comment">// 将二进制 n 的最后一位 由 1 变成 0</span>n ^ (n &amp; (n<span class="hljs-number">-1</span>)) <span class="hljs-comment">// 只保留最后一个 bit 位， 将 n = 1110 变为 n = 0010</span><span class="hljs-comment">// (2) </span><span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> n:nums) ret ^= num;  <span class="hljs-comment">// 利用 n ^ n = 0 进行去重  </span><span class="hljs-comment">// (3) 取位和置位</span>n |= <span class="hljs-number">1</span> &lt;&lt; bit<span class="hljs-comment">// 置位操作: 将 n 的 bit 位设置为 1, 1 &lt;&lt; bit 产生一个只有 bit 位为 1， 其他均为 0 的数</span>n &amp;= ~(<span class="hljs-number">1</span> &lt;&lt; bit) <span class="hljs-comment">// 置位操作， 将 n 的 bit 位置 0</span>n &amp; <span class="hljs-number">1</span> &lt;&lt; bit   <span class="hljs-comment">// 取位操作: 判断 n 的 bit 位是否是 1</span></code></pre><h5 id="1-使用位运算替换-、-gt-、swap-操作"><a href="#1-使用位运算替换-、-gt-、swap-操作" class="headerlink" title="(1)  使用位运算替换 +、&gt;、swap 操作"></a>(1)  使用位运算替换 +、&gt;、swap 操作</h5><p><strong>[371] <a href="https://leetcode-cn.com/problems/sum-of-two-integers/">https://leetcode-cn.com/problems/sum-of-two-integers/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">getSum</span><span class="hljs-params">(<span class="hljs-keyword">int</span> a, <span class="hljs-keyword">int</span> b)</span> </span>&#123;    <span class="hljs-keyword">while</span>(b)&#123;        <span class="hljs-keyword">int</span> carry = (<span class="hljs-keyword">unsigned</span> <span class="hljs-keyword">int</span>)(a &amp; b) &lt;&lt; <span class="hljs-number">1</span>;        a = a ^ b;        b = carry;    &#125;    <span class="hljs-keyword">return</span> a;&#125;</code></pre><h5 id="2-n-amp-n-1-将二进制-n-的最后一位-1-变成-0"><a href="#2-n-amp-n-1-将二进制-n-的最后一位-1-变成-0" class="headerlink" title="(2) n &amp; (n - 1) 将二进制 n 的最后一位 1 变成 0"></a>(2) n &amp; (n - 1) 将二进制 n 的最后一位 1 变成 0</h5><p><strong>[191] <a href="https://leetcode-cn.com/problems/number-of-1-bits/">https://leetcode-cn.com/problems/number-of-1-bits/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">hammingWeight</span><span class="hljs-params">(<span class="hljs-keyword">uint32_t</span> n)</span> </span>&#123;    <span class="hljs-keyword">int</span> res = <span class="hljs-number">0</span>;    <span class="hljs-keyword">while</span>(n &gt; <span class="hljs-number">0</span>)&#123;        n = n &amp; (n<span class="hljs-number">-1</span>);        res++;    &#125;    <span class="hljs-keyword">return</span> res; &#125;</code></pre><p><strong>[338] <a href="https://leetcode.com/problems/counting-bits/">https://leetcode.com/problems/counting-bits/</a></strong>   🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-comment">// i &amp; (i - 1) 可以去掉 i 最右边的一个1(如果有)，</span><span class="hljs-comment">// 所以 i 的1的个数就是 i &amp; (i - 1)的1的个数加上1</span><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">countBits</span><span class="hljs-params">(<span class="hljs-keyword">int</span> num)</span> </span>&#123;    <span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">dp</span><span class="hljs-params">(num + <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)</span></span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt;= num; i++)&#123;        dp[i] = dp[i &amp; (i<span class="hljs-number">-1</span>)] + <span class="hljs-number">1</span>;    &#125;    <span class="hljs-keyword">return</span> dp;&#125;</code></pre><p><strong>[461] <a href="https://leetcode-cn.com/problems/hamming-distance/">https://leetcode-cn.com/problems/hamming-distance/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// 求异或结果的 1 的个数</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">hammingDistance</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x, <span class="hljs-keyword">int</span> y)</span> </span>&#123;    <span class="hljs-keyword">int</span> cnt = <span class="hljs-number">0</span>, n = x ^ y;    <span class="hljs-keyword">while</span>(n)&#123;        n &amp;= (n<span class="hljs-number">-1</span>);        cnt++;    &#125;    <span class="hljs-keyword">return</span> cnt;&#125;</code></pre><p><strong>[477] <a href="https://leetcode-cn.com/problems/total-hamming-distance/">https://leetcode-cn.com/problems/total-hamming-distance/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// !! 对 32 bit 的每一位进行处理，这是一种思想</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">totalHammingDistance</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;    <span class="hljs-keyword">int</span> res = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">32</span>; i++)&#123;        <span class="hljs-keyword">int</span> cnt = <span class="hljs-number">0</span>;        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> n:nums) <span class="hljs-keyword">if</span>(n &amp; (<span class="hljs-number">1</span> &lt;&lt; i)) cnt += <span class="hljs-number">1</span>;        res += (cnt) * (nums.size() - cnt);    &#125;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><p><strong>[231] <a href="https://leetcode-cn.com/problems/power-of-two/">https://leetcode-cn.com/problems/power-of-two/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// 如果一个数是2的幂，那么其二进制中只有一位数为 1</span><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">isPowerOfTwo</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n)</span> </span>&#123;    <span class="hljs-keyword">if</span>(n &lt;= <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;    <span class="hljs-keyword">return</span> (n &amp; (n<span class="hljs-number">-1</span>)) == <span class="hljs-number">0</span>;&#125;</code></pre><p><strong>[326] <a href="https://leetcode-cn.com/problems/power-of-three/">https://leetcode-cn.com/problems/power-of-three/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">isPowerOfThree</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n)</span> </span>&#123;    <span class="hljs-keyword">if</span>(n &lt; <span class="hljs-number">3</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;    <span class="hljs-keyword">while</span>( n % <span class="hljs-number">3</span> == <span class="hljs-number">0</span>)&#123;        n /= <span class="hljs-number">3</span>;    &#125;    <span class="hljs-keyword">return</span> n == <span class="hljs-number">1</span>;&#125;</code></pre><p><strong>[342] <a href="https://leetcode-cn.com/problems/power-of-four/">https://leetcode-cn.com/problems/power-of-four/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// !!! 等号的优先级是要大于 位运算的</span><span class="hljs-comment">// power of four的特点是其二进制表示除了只有1位为1外，</span><span class="hljs-comment">// 其二进制为1的数在奇数数位，所以可以根据 num &amp; 0xaaaaaaaa 来获取结果</span><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">isPowerOfFour</span><span class="hljs-params">(<span class="hljs-keyword">int</span> num)</span> </span>&#123;    <span class="hljs-keyword">return</span> (num &gt; <span class="hljs-number">0</span>) &amp;&amp; (num &amp; (num - <span class="hljs-number">1</span>)) == <span class="hljs-number">0</span> &amp;&amp; (num &amp; <span class="hljs-number">0xaaaaaaaa</span>) == <span class="hljs-number">0</span>; &#125;</code></pre><h5 id="3-异或运算-n-n-利用-n-n-0-来去重"><a href="#3-异或运算-n-n-利用-n-n-0-来去重" class="headerlink" title="(3) 异或运算  n ^ n, 利用 n ^ n = 0 来去重"></a>(3) 异或运算  n ^ n, 利用 n ^ n = 0 来去重</h5><pre><code class="hljs cpp"><span class="hljs-comment">// 常见用法</span><span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> n:nums) ret ^= num;</code></pre><p><strong>[136] <a href="https://leetcode.com/problems/single-number/">https://leetcode.com/problems/single-number/</a></strong>    🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">singleNumber</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;    <span class="hljs-keyword">int</span> res = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> n:nums) res ^= n;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><p><strong>[389] <a href="https://leetcode.com/problems/find-the-difference/">https://leetcode.com/problems/find-the-difference/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">char</span> <span class="hljs-title">findTheDifference</span><span class="hljs-params">(<span class="hljs-built_in">string</span> s, <span class="hljs-built_in">string</span> t)</span> </span>&#123;    <span class="hljs-keyword">char</span> res = t[<span class="hljs-number">0</span>];    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; s.size(); i++) res ^= s[i];    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; t.size(); i++) res ^= t[i];    <span class="hljs-keyword">return</span> res;&#125;</code></pre><p><strong>[268] <a href="https://leetcode.com/problems/missing-number/">https://leetcode.com/problems/missing-number/</a></strong>  🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">missingNumber</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;    <span class="hljs-keyword">int</span> len = nums.size()， res = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt;= len; i++)   res ^= i;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; len; i++)  res ^= nums[i];    <span class="hljs-keyword">return</span> res;&#125;<span class="hljs-comment">// 解法二: 求和 0-n, 然后减去数组中所有的数</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">missingNumber</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;    <span class="hljs-keyword">if</span>(nums.size() == <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    <span class="hljs-keyword">int</span> res = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt;= nums.size(); i++) res ^= i;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; nums.size(); i++) res ^= nums[i];    <span class="hljs-keyword">return</span> res;&#125;</code></pre><p><strong>[260] <a href="https://leetcode-cn.com/problems/single-number-iii/">https://leetcode-cn.com/problems/single-number-iii/</a></strong>  🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">singleNumbers</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;    <span class="hljs-keyword">int</span> diff = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> n:nums) diff ^= n;    <span class="hljs-keyword">int</span> last_diff = (diff &amp; (diff - <span class="hljs-number">1</span>)) ^ diff;    <span class="hljs-keyword">int</span> resA = <span class="hljs-number">0</span>, resB = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; nums.size(); i++)&#123;        <span class="hljs-keyword">if</span>(nums[i] &amp; last_diff) resA ^= nums[i];        <span class="hljs-keyword">else</span> resB ^= nums[i];    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; &#123;resA, resB&#125;;&#125;</code></pre><p><strong>[169] <a href="https://leetcode-cn.com/problems/majority-element/">https://leetcode-cn.com/problems/majority-element/</a></strong>   🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-comment">// 没有用到 位运算， 但是大部分题目都会将其放到位运算这个类别当中</span><span class="hljs-comment">// 解法二: 摩尔投票法</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">majorityElement</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;    <span class="hljs-keyword">int</span> cur = nums[<span class="hljs-number">0</span>];    <span class="hljs-keyword">int</span> cnt = <span class="hljs-number">1</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; nums.size(); i++)&#123;        <span class="hljs-keyword">if</span>(nums[i] != cur)&#123;            --cnt;            <span class="hljs-keyword">if</span>(cnt &lt;= <span class="hljs-number">0</span>)&#123;                cur = nums[i];                cnt = <span class="hljs-number">1</span>;            &#125;        &#125;<span class="hljs-keyword">else</span>&#123;            cnt += <span class="hljs-number">1</span>;         &#125;    &#125;    <span class="hljs-keyword">return</span> cur;&#125;</code></pre><h5 id="4-取位和-置位操作"><a href="#4-取位和-置位操作" class="headerlink" title="(4) 取位和 置位操作"></a>(4) 取位和 置位操作</h5><pre><code class="hljs cpp">n |= <span class="hljs-number">1</span> &lt;&lt; bit<span class="hljs-comment">// 置位操作: 将 n 的 bit 位设置为 1, 1 &lt;&lt; bit 产生一个只有 bit 位为 1， 其他均为 0 的数</span>n &amp;= ~(<span class="hljs-number">1</span> &lt;&lt; bit) <span class="hljs-comment">// 置位操作， 将 n 的 bit 位置 0</span>n &amp; <span class="hljs-number">1</span> &lt;&lt; bit   <span class="hljs-comment">// 取位操作: 判断 n 的 bit 位是否是 1</span></code></pre><p><strong>[190] <a href="https://leetcode.com/problems/reverse-bits/">https://leetcode.com/problems/reverse-bits/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-comment">// A &amp; (1 &lt;&lt; i)  获取 i 位的 bit</span><span class="hljs-comment">// A |= (1 &lt;&lt; i)  将 i 位 设置为 bit</span><span class="hljs-function"><span class="hljs-keyword">uint32_t</span> <span class="hljs-title">reverseBits</span><span class="hljs-params">(<span class="hljs-keyword">uint32_t</span> n)</span> </span>&#123;    <span class="hljs-keyword">uint32_t</span> res = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">32</span>; i++)&#123;        <span class="hljs-keyword">bool</span> bit = (<span class="hljs-number">1</span> &lt;&lt; i) &amp; n; <span class="hljs-comment">// get</span>        res |= (bit &lt;&lt; (<span class="hljs-number">32</span> - i - <span class="hljs-number">1</span>)); <span class="hljs-comment">// set</span>    &#125;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><p><strong>[137] <a href="https://leetcode.com/problems/single-number-ii/">https://leetcode.com/problems/single-number-ii/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">singleNumber</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;    <span class="hljs-keyword">int</span> res = <span class="hljs-number">0</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">32</span>; i++)&#123;        <span class="hljs-keyword">int</span> cnt = <span class="hljs-number">0</span>;        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> n:nums) <span class="hljs-keyword">if</span>((<span class="hljs-number">1</span> &lt;&lt; i) &amp; n) cnt+=<span class="hljs-number">1</span>;        <span class="hljs-keyword">if</span>(cnt % <span class="hljs-number">3</span> == <span class="hljs-number">1</span>) res += (<span class="hljs-number">1</span> &lt;&lt; i);    &#125;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><h5 id="5-其他"><a href="#5-其他" class="headerlink" title="(5) 其他"></a>(5) 其他</h5><p><strong>[89] <a href="https://leetcode-cn.com/problems/gray-code/">https://leetcode-cn.com/problems/gray-code/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">grayCode</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n)</span> </span>&#123;    <span class="hljs-keyword">int</span> max = <span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>, n);    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; res;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; max; i++)&#123;        res.push_back((i &gt;&gt; <span class="hljs-number">1</span>) ^ i); <span class="hljs-comment">// 格雷码运算</span>    &#125;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><p><strong>[201] <a href="https://leetcode-cn.com/problems/bitwise-and-of-numbers-range/">https://leetcode-cn.com/problems/bitwise-and-of-numbers-range/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">rangeBitwiseAnd</span><span class="hljs-params">(<span class="hljs-keyword">int</span> m, <span class="hljs-keyword">int</span> n)</span> </span>&#123;    <span class="hljs-keyword">int</span> res = n;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = m; i &lt; n &amp; res != <span class="hljs-number">0</span>; i++)&#123;  <span class="hljs-comment">// 添加一个 res != 0 防止超时</span>        res &amp;= i;    &#125;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><h5 id="TBD"><a href="#TBD" class="headerlink" title="TBD"></a>TBD</h5><div class="table-container"><table><thead><tr><th>题号</th><th>题目链接</th></tr></thead><tbody><tr><td>318</td><td><a href="https://leetcode.com/problems/maximum-product-of-word-lengths/description/">Maximum Product of Word Lengths</a></td></tr><tr><td>393</td><td><a href="https://leetcode.com/problems/utf-8-validation/description/">UTF-8 Validation</a></td></tr></tbody></table></div><h3 id="3-链表"><a href="#3-链表" class="headerlink" title="3. 链表"></a>3. 链表</h3><p><img src="/2020/09/13/leetcode/linklist.png" alt></p><h5 id="常用技巧总结"><a href="#常用技巧总结" class="headerlink" title="常用技巧总结"></a>常用技巧总结</h5><ul><li><p>添加哑节点</p><pre><code class="hljs haxe">ListNode * <span class="hljs-keyword">new</span><span class="hljs-type">head</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">ListNode</span>(<span class="hljs-number">0</span>);<span class="hljs-keyword">new</span><span class="hljs-type">head</span> -&gt; next = head;</code></pre></li><li><p>头指针一般不可以随意改变， 可以申请一个指向头指针的指针</p><pre><code class="hljs abnf">ListNode * p = head<span class="hljs-comment">;</span></code></pre></li><li><p>不要让链表断开了， 适当的在纸上模拟一下对应的链表指向操作。</p></li><li><p>注意一下常见的边界条件：头结点 和 尾结点， 链表为空， 只有一个节点。</p></li><li><p>使用快慢指针</p></li><li><p>注意些小细节：删除节点的释放，末尾指针不要乱指</p></li></ul><h5 id="1-性质判定"><a href="#1-性质判定" class="headerlink" title="(1) 性质判定"></a>(1) 性质判定</h5><p><strong>[141] <a href="https://leetcode.com/problems/linked-list-cycle/">https://leetcode.com/problems/linked-list-cycle/</a></strong>  </p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">hasCycle</span><span class="hljs-params">(ListNode *head)</span> </span>&#123;    <span class="hljs-keyword">if</span>(head == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;    ListNode * fast = head -&gt; next;    ListNode * slow = head;    <span class="hljs-keyword">while</span>(fast != <span class="hljs-literal">NULL</span> &amp;&amp; fast -&gt; next != <span class="hljs-literal">NULL</span>)&#123;        <span class="hljs-keyword">if</span>(fast == slow) <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;        fast = fast -&gt; next -&gt; next;        slow = slow -&gt; next;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;  &#125;</code></pre><p><strong>[142] <a href="https://leetcode-cn.com/problems/linked-list-cycle-ii/">https://leetcode-cn.com/problems/linked-list-cycle-ii/</a></strong>    🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function">ListNode *<span class="hljs-title">detectCycle</span><span class="hljs-params">(ListNode *head)</span> </span>&#123;    <span class="hljs-keyword">if</span>(head == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> head;    ListNode * slow = head;    ListNode * fast = head; <span class="hljs-comment">// fast 等于 head </span>    <span class="hljs-keyword">bool</span> hasCycle = <span class="hljs-literal">false</span>;    <span class="hljs-keyword">while</span>(fast &amp;&amp; fast -&gt; next)&#123;        <span class="hljs-comment">// 先移动指针，再判断</span>        fast = fast -&gt; next -&gt; next;        slow = slow -&gt; next;        <span class="hljs-keyword">if</span>(fast == slow)&#123;            hasCycle = <span class="hljs-literal">true</span>;            <span class="hljs-keyword">break</span>;        &#125;     &#125;      <span class="hljs-comment">// 一个指针从相遇节点开始， 另一个指针从头开始，两者如果相遇</span>    <span class="hljs-comment">// 则该节点即为入环节点</span>    <span class="hljs-keyword">if</span>(!hasCycle) <span class="hljs-keyword">return</span> <span class="hljs-literal">NULL</span>;    slow = head;    <span class="hljs-keyword">while</span>(fast != slow)&#123;        fast = fast -&gt; next;        slow = slow -&gt; next;    &#125;    <span class="hljs-keyword">return</span> fast;    &#125;</code></pre><p><strong>[234] <a href="https://leetcode-cn.com/problems/palindrome-linked-list/">https://leetcode-cn.com/problems/palindrome-linked-list/</a></strong>   🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function">ListNode * <span class="hljs-title">reverseLinkList</span><span class="hljs-params">(ListNode * head)</span></span>&#123;    <span class="hljs-keyword">if</span>(head == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> head;    ListNode * prev = <span class="hljs-literal">NULL</span>;    ListNode * cur = head;    <span class="hljs-keyword">while</span>(cur)&#123;        ListNode * next = cur -&gt; next;        cur -&gt; next = prev;        prev = cur;        cur = next;    &#125;    <span class="hljs-keyword">return</span> prev;&#125;<span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">isPalindrome</span><span class="hljs-params">(ListNode* head)</span> </span>&#123;    ListNode * p = head; <span class="hljs-comment">// 找到中间节点</span>    <span class="hljs-keyword">int</span> cnt = <span class="hljs-number">0</span>;    <span class="hljs-keyword">while</span>(p)&#123;        cnt += <span class="hljs-number">1</span>;        p = p -&gt; next;    &#125;    cnt /= <span class="hljs-number">2</span>;    p = head;    <span class="hljs-keyword">while</span>(cnt--)  p = p -&gt; next;         ListNode * q = reverseLinkList(p); <span class="hljs-comment">// 翻转后半部分</span>      <span class="hljs-keyword">while</span>(q &amp;&amp; head)&#123; <span class="hljs-comment">// 判断是否相等</span>        <span class="hljs-keyword">if</span>(head -&gt; val != q -&gt; val) <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;        q = q -&gt; next;        head = head -&gt; next;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;&#125;</code></pre><h5 id="2-获取指定节点"><a href="#2-获取指定节点" class="headerlink" title="(2) 获取指定节点"></a>(2) 获取指定节点</h5><p><strong>[19] <a href="https://leetcode.com/problems/remove-nth-node-from-end-of-list/">https://leetcode.com/problems/remove-nth-node-from-end-of-list/</a></strong>   🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function">ListNode* <span class="hljs-title">removeNthFromEnd</span><span class="hljs-params">(ListNode* head, <span class="hljs-keyword">int</span> n)</span> </span>&#123;    ListNode * newhead = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">0</span>);    newhead -&gt; next = head;    ListNode * fast = newhead;    ListNode * slow = newhead;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n+<span class="hljs-number">1</span>; i++)&#123;        fast = fast -&gt; next;        &#125;    <span class="hljs-keyword">while</span>(fast)&#123;        fast = fast -&gt; next;        slow = slow -&gt; next;    &#125;    slow -&gt; next = slow -&gt; next -&gt; next;    <span class="hljs-keyword">return</span> newhead -&gt; next;&#125;</code></pre><p><strong>[160] <a href="https://leetcode-cn.com/problems/intersection-of-two-linked-lists/">https://leetcode-cn.com/problems/intersection-of-two-linked-lists/</a></strong>   🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">getLinkedListLength</span><span class="hljs-params">(ListNode *head)</span></span>&#123;    <span class="hljs-keyword">int</span> cnt = <span class="hljs-number">0</span>;    ListNode * p = head;    <span class="hljs-keyword">while</span>(p)&#123;        p = p -&gt; next;        cnt++;    &#125;    <span class="hljs-keyword">return</span> cnt;&#125;<span class="hljs-function">ListNode *<span class="hljs-title">getIntersectionNode</span><span class="hljs-params">(ListNode *headA, ListNode *headB)</span> </span>&#123;    <span class="hljs-keyword">int</span> length_A = getLinkedListLength(headA);    <span class="hljs-keyword">int</span> length_B = getLinkedListLength(headB);    <span class="hljs-keyword">int</span> diff = <span class="hljs-built_in">abs</span>(length_A - length_B);    <span class="hljs-keyword">if</span>(length_A &gt; length_B)         <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; diff; i++)  headA = headA -&gt; next;    <span class="hljs-keyword">else</span>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>;  i &lt; diff; i++)  headB = headB -&gt; next;    <span class="hljs-keyword">while</span>(headA != headB)&#123;        headA  = headA -&gt; next;        headB = headB -&gt; next;    &#125;    <span class="hljs-keyword">return</span> headA;&#125;</code></pre><h5 id="3-链表节点顺序的调整"><a href="#3-链表节点顺序的调整" class="headerlink" title="(3) 链表节点顺序的调整"></a>(3) 链表节点顺序的调整</h5><p><strong>[206] <a href="https://leetcode.com/problems/reverse-linked-list">https://leetcode.com/problems/reverse-linked-list</a></strong>   🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function">ListNode* <span class="hljs-title">reverseList</span><span class="hljs-params">(ListNode* head)</span> </span>&#123;    ListNode * prev = <span class="hljs-literal">NULL</span>;    ListNode * cur = head;    <span class="hljs-keyword">while</span>(cur)&#123;        ListNode * next = cur -&gt; next;        cur -&gt; next = prev;        prev = cur;        cur = next;    &#125;    <span class="hljs-keyword">return</span> prev;&#125;</code></pre><p><strong>[328] <a href="https://leetcode.com/problems/odd-even-linked-list/">https://leetcode.com/problems/odd-even-linked-list/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function">ListNode* <span class="hljs-title">oddEvenList</span><span class="hljs-params">(ListNode* head)</span> </span>&#123;    <span class="hljs-keyword">if</span>(head == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> head;    ListNode * oddhead = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">0</span>);    ListNode * evenhead = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">0</span>);    ListNode * p_odd = oddhead;    ListNode * p_even = evenhead;    ListNode * p = head;    <span class="hljs-keyword">int</span> index = <span class="hljs-number">0</span>;    <span class="hljs-keyword">while</span>(p)&#123;        <span class="hljs-keyword">if</span>(index % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>)&#123;            p_even -&gt; next = p;            p_even = p_even -&gt; next;        &#125;<span class="hljs-keyword">else</span>&#123;            p_odd -&gt; next = p;            p_odd = p_odd -&gt; next;        &#125;        p = p -&gt; next;          index += <span class="hljs-number">1</span>;       &#125;    p_even -&gt; next = oddhead -&gt; next;    p_odd -&gt; next = <span class="hljs-literal">NULL</span>;  <span class="hljs-comment">// 不要忘记将这个链表置零 &lt;--- 否则会产生死循环! 计算超时</span>    <span class="hljs-keyword">return</span> evenhead -&gt; next; &#125;</code></pre><p><strong>[86] <a href="https://leetcode-cn.com/problems/partition-list/">https://leetcode-cn.com/problems/partition-list/</a></strong>  🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function">ListNode* <span class="hljs-title">partition</span><span class="hljs-params">(ListNode* head, <span class="hljs-keyword">int</span> x)</span> </span>&#123;    <span class="hljs-keyword">if</span>(head == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> head;    ListNode * before = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">0</span>);    ListNode * p1 = before;    ListNode * after = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">0</span>);    ListNode * p2 = after;    ListNode * p = head;    <span class="hljs-keyword">while</span>(p)&#123;        <span class="hljs-keyword">if</span>(p -&gt; val &lt; x)&#123;            p1 -&gt; next = p;            p1 = p1 -&gt; next;        &#125;<span class="hljs-keyword">else</span>&#123;            p2 -&gt; next = p;            p2 = p2 -&gt; next;        &#125;            p = p -&gt; next;    &#125;    p1 -&gt; next = after -&gt; next;    p2 -&gt; next = <span class="hljs-literal">NULL</span>;    <span class="hljs-keyword">return</span> before -&gt; next;&#125;</code></pre><p><strong>[143] <a href="https://leetcode-cn.com/problems/reorder-list/">https://leetcode-cn.com/problems/reorder-list/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">reorderList</span><span class="hljs-params">(ListNode* head)</span> </span>&#123;    <span class="hljs-keyword">if</span>(head == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span>;    <span class="hljs-comment">// 获取链表长度</span>    ListNode * p = head;    <span class="hljs-keyword">int</span> len = <span class="hljs-number">0</span>;    <span class="hljs-keyword">while</span>(p)&#123;        len++;        p = p -&gt; next;    &#125;    <span class="hljs-comment">// 翻转后半部分</span>    <span class="hljs-keyword">int</span> mid = len / <span class="hljs-number">2</span>;    p = head;    <span class="hljs-keyword">while</span>(mid--)  p = p -&gt; next;    ListNode * rhead = reverseList(p-&gt;next);    p -&gt; next = <span class="hljs-literal">NULL</span>;  <span class="hljs-comment">// !!! </span>    <span class="hljs-comment">// 合并两个链表</span>    ListNode * dummy = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">0</span>);    ListNode * ptr = dummy;    <span class="hljs-keyword">while</span>(rhead &amp;&amp; head)&#123;        ptr -&gt; next = head;        head = head -&gt; next;        ptr = ptr -&gt; next;        ptr -&gt; next = rhead;        rhead = rhead -&gt; next;        ptr = ptr -&gt; next;    &#125;    <span class="hljs-keyword">if</span>(rhead != <span class="hljs-literal">NULL</span>) ptr -&gt; next = rhead;    <span class="hljs-keyword">if</span>(head != <span class="hljs-literal">NULL</span>) ptr -&gt; next = head;    head = dummy -&gt; next;&#125;</code></pre><p><strong>[61] <a href="https://leetcode-cn.com/problems/rotate-list/">https://leetcode-cn.com/problems/rotate-list/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function">ListNode* <span class="hljs-title">rotateRight</span><span class="hljs-params">(ListNode* head, <span class="hljs-keyword">int</span> k)</span> </span>&#123;    <span class="hljs-keyword">if</span>(head == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> head;        <span class="hljs-keyword">int</span> cnt = <span class="hljs-number">1</span>;    ListNode * ptr = head;    <span class="hljs-keyword">while</span>(ptr -&gt; next)&#123;        ptr = ptr -&gt; next;        ++cnt;    &#125;    <span class="hljs-comment">// 做环</span>    ptr -&gt; next = head;    <span class="hljs-comment">// 移动</span>    k = cnt - k % cnt;    <span class="hljs-keyword">while</span>(k<span class="hljs-number">-1</span>)&#123;        k--;        head = head -&gt; next;    &#125;    <span class="hljs-comment">// 修改指针</span>    ptr = head;    head = head -&gt; next;    ptr -&gt; next = <span class="hljs-literal">NULL</span>;    <span class="hljs-keyword">return</span> head; &#125;</code></pre><h5 id="4-删除链表中的节点"><a href="#4-删除链表中的节点" class="headerlink" title="(4) 删除链表中的节点"></a>(4) 删除链表中的节点</h5><p><strong>[237] <a href="https://leetcode.com/problems/delete-node-in-a-linked-list/">https://leetcode.com/problems/delete-node-in-a-linked-list/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">deleteNode</span><span class="hljs-params">(ListNode* node)</span> </span>&#123;    node -&gt; val = node -&gt; next -&gt; val;    node -&gt; next = node -&gt; next -&gt; next;&#125;</code></pre><p><strong>[203] <a href="https://leetcode.com/problems/remove-linked-list-elements/">https://leetcode.com/problems/remove-linked-list-elements/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function">ListNode* <span class="hljs-title">deleteNode</span><span class="hljs-params">(ListNode* head, <span class="hljs-keyword">int</span> val)</span> </span>&#123;    <span class="hljs-keyword">if</span>(head == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> head;    <span class="hljs-comment">// 添加哑节点</span>    ListNode * newhead = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">0</span>);    newhead -&gt; next = head;    ListNode * p = newhead;    <span class="hljs-keyword">while</span>(p -&gt; next)&#123;        <span class="hljs-keyword">if</span>(p -&gt; next -&gt; val == val)&#123;            ListNode * tmp = p -&gt; next;            p -&gt; next = p -&gt; next -&gt; next;            <span class="hljs-keyword">delete</span> tmp;  <span class="hljs-comment">// 删除无用节点</span>        &#125;         <span class="hljs-keyword">else</span>            p = p -&gt; next;    &#125;    <span class="hljs-keyword">return</span> newhead -&gt; next;&#125;</code></pre><p><strong>[83] <a href="https://leetcode.com/problems/remove-duplicates-from-sorted-list/">https://leetcode.com/problems/remove-duplicates-from-sorted-list/</a></strong>  🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function">ListNode* <span class="hljs-title">deleteDuplicates</span><span class="hljs-params">(ListNode* head)</span> </span>&#123;    <span class="hljs-keyword">if</span>(head == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">NULL</span>;    ListNode * dummy = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">0</span>);    dummy -&gt; next = head;    ListNode * ptr = head;    <span class="hljs-keyword">while</span>(ptr &amp;&amp; ptr -&gt; next)&#123;        <span class="hljs-keyword">if</span>(ptr-&gt;val == ptr-&gt;next-&gt;val)&#123;            ListNode * tmp = ptr -&gt; next;            ptr -&gt; next = ptr -&gt; next -&gt; next;            <span class="hljs-keyword">delete</span> tmp;        &#125;<span class="hljs-keyword">else</span>&#123;            ptr = ptr -&gt; next;        &#125;    &#125;    <span class="hljs-keyword">return</span> dummy -&gt; next;&#125;</code></pre><h5 id="5-合并有序链表"><a href="#5-合并有序链表" class="headerlink" title="(5) 合并有序链表"></a>(5) 合并有序链表</h5><p><strong>[21] <a href="https://leetcode-cn.com/problems/merge-two-sorted-lists/">https://leetcode-cn.com/problems/merge-two-sorted-lists/</a></strong>  🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function">ListNode* <span class="hljs-title">mergeTwoLists</span><span class="hljs-params">(ListNode* l1, ListNode* l2)</span> </span>&#123;    ListNode * newhead = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">0</span>);    ListNode * p = newhead;    ListNode * p_l1 = l1, * p_l2 = l2;    <span class="hljs-keyword">while</span>(p_l1 &amp;&amp; p_l2)&#123;        <span class="hljs-keyword">if</span>(p_l1 -&gt; val &gt; p_l2 -&gt; val)&#123;            p -&gt; next = p_l2;            p_l2 = p_l2 -&gt; next;            p = p -&gt; next;        &#125;<span class="hljs-keyword">else</span>&#123;            p -&gt; next = p_l1;            p_l1 = p_l1 -&gt; next;            p = p -&gt; next;         &#125;    &#125;    <span class="hljs-keyword">if</span>(p_l1 == <span class="hljs-literal">NULL</span>) p-&gt;next = p_l2;    <span class="hljs-keyword">else</span> p-&gt; next = p_l1;    <span class="hljs-keyword">return</span> newhead -&gt; next;&#125;</code></pre><p><strong>[23] <a href="https://leetcode-cn.com/problems/merge-k-sorted-lists/">https://leetcode-cn.com/problems/merge-k-sorted-lists/</a></strong>  🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-function">ListNode* <span class="hljs-title">mergeKLists</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;ListNode*&gt;&amp; lists)</span> </span>&#123;    <span class="hljs-keyword">if</span>(lists.size() == <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">NULL</span>;    <span class="hljs-keyword">if</span>(lists.size() == <span class="hljs-number">1</span>) <span class="hljs-keyword">return</span> lists[<span class="hljs-number">0</span>];    <span class="hljs-keyword">if</span>(lists.size() == <span class="hljs-number">2</span>) <span class="hljs-keyword">return</span> mergeTwoLists(lists[<span class="hljs-number">0</span>], lists[<span class="hljs-number">1</span>]);    <span class="hljs-keyword">int</span> len = lists.size();    <span class="hljs-keyword">int</span> mid = len / <span class="hljs-number">2</span>;    <span class="hljs-built_in">vector</span>&lt;ListNode* &gt; lists1;    <span class="hljs-built_in">vector</span>&lt;ListNode* &gt; lists2;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; mid; i++)  lists1.push_back(lists[i]);      <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = mid; i &lt; len; i++)  lists2.push_back(lists[i]);    ListNode * p1 = mergeKLists(lists1);    ListNode * p2 = mergeKLists(lists2);    <span class="hljs-keyword">return</span> mergeTwoLists(p1, p2);  &#125;</code></pre><h5 id="6-模拟数学运算-两数相加、链表-1"><a href="#6-模拟数学运算-两数相加、链表-1" class="headerlink" title="(6) 模拟数学运算(两数相加、链表+1)"></a>(6) 模拟数学运算(两数相加、链表+1)</h5><p><strong>[2] <a href="https://leetcode.com/problems/add-two-numbers/">https://leetcode.com/problems/add-two-numbers/</a></strong>  🌟🌟</p><pre><code class="hljs cpp"><span class="hljs-comment">// 模拟常见的加法操作: 需要注意，当一条链表为空时候，仍然在此框架下相加，最后还需要添加 c_in</span><span class="hljs-function">ListNode* <span class="hljs-title">addTwoNumbers</span><span class="hljs-params">(ListNode* l1, ListNode* l2)</span> </span>&#123;    ListNode * dummy = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">0</span>);    ListNode * p = dummy;    <span class="hljs-keyword">int</span> c_in = <span class="hljs-number">0</span>;    <span class="hljs-keyword">while</span>(l1 != <span class="hljs-literal">NULL</span> || l2 != <span class="hljs-literal">NULL</span>)&#123;        <span class="hljs-keyword">int</span> val1 = l1 != <span class="hljs-literal">NULL</span> ? l1 -&gt; val : <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> val2 = l2 != <span class="hljs-literal">NULL</span> ? l2 -&gt; val : <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> res = val1 + val2 + c_in;        ListNode * newNode =  <span class="hljs-keyword">new</span> ListNode(res % <span class="hljs-number">10</span>);        c_in = res / <span class="hljs-number">10</span>;        p -&gt; next = newNode;        p = p -&gt; next;        <span class="hljs-keyword">if</span>(l1 != <span class="hljs-literal">NULL</span>) l1 = l1 -&gt; next;        <span class="hljs-keyword">if</span>(l2 != <span class="hljs-literal">NULL</span>) l2 = l2 -&gt; next;                                                 &#125;    <span class="hljs-keyword">if</span>(c_in)&#123;        ListNode * newNode = <span class="hljs-keyword">new</span> ListNode(c_in);        p -&gt; next = newNode;        p = p -&gt; next;    &#125;    <span class="hljs-keyword">return</span> dummy -&gt; next;  &#125;</code></pre><p><strong>[369] <a href="https://leetcode.com/problems/plus-one-linked-list/">https://leetcode.com/problems/plus-one-linked-list/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> &#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-function">ListNode * <span class="hljs-title">reverseList</span><span class="hljs-params">(ListNode * head)</span></span>&#123;        <span class="hljs-keyword">if</span>(head == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> head;        ListNode * prev = <span class="hljs-literal">NULL</span>;        <span class="hljs-keyword">while</span>(head)&#123;            ListNode * next = head -&gt; next;            head -&gt; next = prev;            prev = head;            head = next;        &#125;        <span class="hljs-keyword">return</span> prev;            &#125;        <span class="hljs-function">ListNode* <span class="hljs-title">plusOne</span><span class="hljs-params">(ListNode* head)</span> </span>&#123;        <span class="hljs-keyword">if</span>(head == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">NULL</span>;        head = reverseList(head);                <span class="hljs-keyword">int</span> c_in = <span class="hljs-number">1</span>;        ListNode * ptr = head;        ListNode * prev = head;        <span class="hljs-keyword">while</span>(ptr)&#123;            <span class="hljs-keyword">int</span> res = ptr -&gt; val + c_in;            c_in = res / <span class="hljs-number">10</span>;            ptr -&gt; val = res % <span class="hljs-number">10</span>;            prev = ptr;            ptr = ptr -&gt; next;        &#125;                <span class="hljs-keyword">if</span>(c_in)  prev -&gt; next = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">1</span>);                <span class="hljs-keyword">return</span> reverseList(head);            &#125;&#125;;</code></pre><h5 id="7-复杂链表的复制"><a href="#7-复杂链表的复制" class="headerlink" title="(7) 复杂链表的复制"></a>(7) 复杂链表的复制</h5><h5 id="8-链表与二叉树"><a href="#8-链表与二叉树" class="headerlink" title="(8) 链表与二叉树"></a>(8) 链表与二叉树</h5><h5 id="TBD-1"><a href="#TBD-1" class="headerlink" title="TBD"></a>TBD</h5><div class="table-container"><table><thead><tr><th>题号</th><th>题目链接</th></tr></thead><tbody><tr><td>82</td><td><a href="https://leetcode.com/problems/remove-duplicates-from-sorted-list-ii/description/">Remove Duplicates from Sorted List II</a>  🌟🌟</td></tr><tr><td>148</td><td><a href="https://leetcode.com/problems/sort-list/description/">Sort List</a></td></tr><tr><td>147</td><td><a href="https://leetcode.com/problems/insertion-sort-list/description/">Insertion Sort List</a></td></tr><tr><td>24</td><td><a href="https://leetcode.com/problems/swap-nodes-in-pairs/description/">Swap Nodes in Pairs</a>  🌟🌟</td></tr><tr><td>25</td><td><a href="https://leetcode.com/problems/reverse-nodes-in-k-group/description/">Reverse Nodes in k-Group</a>  🌟🌟</td></tr><tr><td>92</td><td><a href="https://leetcode.com/problems/reverse-linked-list-ii/description/">Reverse Linked List II</a>    🌟🌟</td></tr></tbody></table></div><h3 id="10-其他-leetcode-题目"><a href="#10-其他-leetcode-题目" class="headerlink" title="10. 其他 leetcode 题目"></a>10. 其他 leetcode 题目</h3><p>数组中重复的数字</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">findRepeatNumber</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;    <span class="hljs-built_in">set</span>&lt;<span class="hljs-keyword">int</span>&gt; m_set;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> n:nums)&#123;        <span class="hljs-keyword">if</span>(m_set.find(n) == m_set.end())            m_set.insert(n);        <span class="hljs-keyword">else</span>            <span class="hljs-keyword">return</span> n;    &#125;     <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>矩阵搜索</p><p>替换空格</p><p>斐波那契</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">fib</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n)</span> </span>&#123;    <span class="hljs-keyword">if</span>(n == <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    <span class="hljs-keyword">if</span>(n == <span class="hljs-number">1</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;    <span class="hljs-keyword">int</span> first = <span class="hljs-number">0</span>, second = <span class="hljs-number">1</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">2</span>; i &lt;= n; i++)&#123;        <span class="hljs-keyword">int</span> third = (first + second) % <span class="hljs-number">1000000007</span>;        first = second;        second = third;    &#125;    <span class="hljs-keyword">return</span> second;&#125;</code></pre><p><a href="https://leetcode-cn.com/problems/climbing-stairs/">https://leetcode-cn.com/problems/climbing-stairs/</a></p><pre><code class="hljs angelscript"><span class="hljs-built_in">int</span> numWays(<span class="hljs-built_in">int</span> n) &#123;    <span class="hljs-keyword">if</span>(n &lt;= <span class="hljs-number">1</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>;    <span class="hljs-keyword">if</span>(n == <span class="hljs-number">2</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">2</span>;    <span class="hljs-built_in">int</span> first = <span class="hljs-number">1</span>, second = <span class="hljs-number">2</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-built_in">int</span> i = <span class="hljs-number">3</span>; i &lt;= n; i++)&#123;        <span class="hljs-built_in">int</span> third = (first + second) % <span class="hljs-number">1000000007</span>;        first = second;        second = third;    &#125;    <span class="hljs-keyword">return</span> second;&#125;</code></pre><p><strong>[53] <a href="https://leetcode-cn.com/problems/maximum-subarray/">https://leetcode-cn.com/problems/maximum-subarray/</a></strong></p><pre><code class="hljs arduino"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">maxSubArray</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;    <span class="hljs-comment">// dp 数组的第 i 个数字表示以该数结尾的 连续数组的最大值</span>    <span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">dp</span><span class="hljs-params">(nums.<span class="hljs-built_in">size</span>(), nums[<span class="hljs-number">0</span>])</span></span>;    <span class="hljs-keyword">int</span> res = nums[<span class="hljs-number">0</span>];    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; nums.<span class="hljs-built_in">size</span>(); i++)&#123;        dp[i] = <span class="hljs-built_in">max</span>(nums[i], dp[i<span class="hljs-number">-1</span>] + nums[i]);        res = <span class="hljs-built_in">max</span>(dp[i], res);    &#125;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><pre><code class="hljs cpp"><span class="hljs-comment">// 不要使用 map, 直接使用 数组 作为 哈希表即可</span><span class="hljs-function"><span class="hljs-keyword">char</span> <span class="hljs-title">firstUniqChar</span><span class="hljs-params">(<span class="hljs-built_in">string</span> s)</span> </span>&#123;    <span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">dic</span><span class="hljs-params">(<span class="hljs-number">26</span>, <span class="hljs-number">0</span>)</span></span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> c:s)   dic[ c - <span class="hljs-string">&#x27;a&#x27;</span>] += <span class="hljs-number">1</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> c:s)   <span class="hljs-keyword">if</span>(dic[c - <span class="hljs-string">&#x27;a&#x27;</span>] == <span class="hljs-number">1</span>) <span class="hljs-keyword">return</span> c;    <span class="hljs-keyword">return</span> <span class="hljs-string">&#x27; &#x27;</span>; <span class="hljs-comment">// 可以直接 return  &#x27; &#x27; 的</span>&#125;</code></pre><h4 id="剑指-Offer-57-和为s的两个数字"><a href="#剑指-Offer-57-和为s的两个数字" class="headerlink" title="剑指 Offer 57. 和为s的两个数字"></a><a href="https://leetcode-cn.com/problems/he-wei-sde-liang-ge-shu-zi-lcof/">剑指 Offer 57. 和为s的两个数字</a></h4><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">twoSum</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> target)</span> </span>&#123;    <span class="hljs-built_in">set</span>&lt;<span class="hljs-keyword">int</span>&gt; m_set;    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; res;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> n : nums)&#123;        <span class="hljs-keyword">if</span>(m_set.find(target - n) == m_set.end())&#123;            m_set.insert(n);        &#125;<span class="hljs-keyword">else</span>&#123;            res.push_back(n);            res.push_back(target - n);            <span class="hljs-keyword">break</span>;        &#125;    &#125;    <span class="hljs-keyword">return</span> res; &#125;</code></pre><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">maxDepth</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;    <span class="hljs-keyword">if</span>(root == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> + max(maxDepth(root-&gt;left), maxDepth(root-&gt;right));&#125;</code></pre><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">printNumbers</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n)</span> </span>&#123;    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; res;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; <span class="hljs-built_in">pow</span>(<span class="hljs-number">10</span>, n); i++)&#123;        res.push_back(i);    &#125;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><p><strong><a href="https://leetcode-cn.com/problems/diao-zheng-shu-zu-shun-xu-shi-qi-shu-wei-yu-ou-shu-qian-mian-lcof/">剑指 Offer 21. 调整数组顺序使奇数位于偶数前面</a></strong></p><pre><code class="hljs matlab">vector&lt;int&gt; exchange(vector&lt;int&gt;&amp; nums) &#123;    int <span class="hljs-built_in">i</span> = <span class="hljs-number">0</span>, <span class="hljs-built_in">j</span> = nums.<span class="hljs-built_in">size</span>() - <span class="hljs-number">1</span>;    <span class="hljs-keyword">while</span>(<span class="hljs-built_in">i</span> &lt; <span class="hljs-built_in">j</span>)&#123;        <span class="hljs-keyword">while</span>(<span class="hljs-built_in">i</span> &lt;<span class="hljs-built_in">j</span> &amp;&amp; nums[<span class="hljs-built_in">i</span>] <span class="hljs-comment">% 2 == 1) i++;</span>        <span class="hljs-keyword">while</span>(<span class="hljs-built_in">i</span> &lt; <span class="hljs-built_in">j</span> &amp;&amp; nums[<span class="hljs-built_in">j</span>] <span class="hljs-comment">% 2 == 0) j--;</span>        swap(nums[<span class="hljs-built_in">i</span>++], nums[<span class="hljs-built_in">j</span>--]);    &#125;    <span class="hljs-keyword">return</span> nums;&#125;</code></pre><h4 id="剑指-Offer-58-II-左旋转字符串"><a href="#剑指-Offer-58-II-左旋转字符串" class="headerlink" title="剑指 Offer 58 - II. 左旋转字符串"></a><a href="https://leetcode-cn.com/problems/zuo-xuan-zhuan-zi-fu-chuan-lcof/">剑指 Offer 58 - II. 左旋转字符串</a></h4><pre><code class="hljs angelscript"><span class="hljs-built_in">void</span> reverse(<span class="hljs-built_in">string</span>&amp; s, <span class="hljs-built_in">int</span> i, <span class="hljs-built_in">int</span> j)&#123;    <span class="hljs-keyword">while</span>(i &lt; j)&#123;        swap(s[i++], s[j--]);    &#125;&#125;<span class="hljs-built_in">string</span> reverseLeftWords(<span class="hljs-built_in">string</span> s, <span class="hljs-built_in">int</span> n) &#123;    reverse(s, <span class="hljs-number">0</span>, n<span class="hljs-number">-1</span>);    reverse(s, n, s.size() - <span class="hljs-number">1</span>);    reverse(s, <span class="hljs-number">0</span>, s.size() - <span class="hljs-number">1</span>);    <span class="hljs-keyword">return</span> s;&#125;</code></pre><h4 id="剑指-Offer-27-二叉树的镜像"><a href="#剑指-Offer-27-二叉树的镜像" class="headerlink" title="剑指 Offer 27. 二叉树的镜像"></a><a href="https://leetcode-cn.com/problems/er-cha-shu-de-jing-xiang-lcof/">剑指 Offer 27. 二叉树的镜像</a></h4><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">exchangeLeaf</span><span class="hljs-params">(TreeNode * node)</span></span>&#123;    <span class="hljs-comment">// 注意返回条件</span>    <span class="hljs-keyword">if</span>(node == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span>;    <span class="hljs-keyword">if</span>(node -&gt; left == <span class="hljs-literal">NULL</span> &amp;&amp; node -&gt; right == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span>;    TreeNode * tmp = node -&gt; left;    node -&gt; left = node -&gt; right;    node -&gt; right = tmp;    <span class="hljs-keyword">if</span>(node -&gt; left) exchangeLeaf(node -&gt; left);    <span class="hljs-keyword">if</span>(node -&gt; right) exchangeLeft(node -&gt; right);&#125;<span class="hljs-function">TreeNode* <span class="hljs-title">mirrorTree</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;    exchangeLeft(root);    <span class="hljs-keyword">return</span> root;&#125;</code></pre><h4 id="剑指-Offer-32-I-从上到下打印二叉树"><a href="#剑指-Offer-32-I-从上到下打印二叉树" class="headerlink" title="剑指 Offer 32 - I. 从上到下打印二叉树"></a><a href="https://leetcode-cn.com/problems/cong-shang-dao-xia-da-yin-er-cha-shu-lcof/">剑指 Offer 32 - I. 从上到下打印二叉树</a></h4><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">levelOrder</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; res;    <span class="hljs-keyword">if</span>(root == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> res;    <span class="hljs-built_in">queue</span>&lt;TreeNode *&gt; m_queue;    m_queue.push(root);    <span class="hljs-keyword">while</span>(!m_queue.empty())&#123;        res.push_back(m_queue.front() -&gt; val);        <span class="hljs-keyword">if</span>(m_queue.front() -&gt; left) m_queue.push(m_queue.front() -&gt; left);        <span class="hljs-keyword">if</span>(m_queue.front() -&gt; right) m_queue.push(m_queue.front() -&gt; right);        m_queue.pop();    &#125;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><p><strong>[102]<a href="https://leetcode-cn.com/problems/binary-tree-level-order-traversal/">https://leetcode-cn.com/problems/binary-tree-level-order-traversal/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&gt; <span class="hljs-title">levelOrder</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; &gt; res;    <span class="hljs-keyword">if</span>(root == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> res;    <span class="hljs-built_in">queue</span>&lt;TreeNode *&gt; m_queue;       <span class="hljs-comment">// queue 的几个 方法: empty(), size(), front(), pop(), push()</span>    m_queue.push(root);    <span class="hljs-keyword">while</span>(!m_queue.empty())&#123;        <span class="hljs-keyword">int</span> n = m_queue.size();        <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; raw;        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++)&#123;            raw.push_back(m_queue.front() -&gt; val);            <span class="hljs-keyword">if</span>(m_queue.front() -&gt;left) m_queue.push(m_queue.front() -&gt; left);            <span class="hljs-keyword">if</span>(m_queue.front() -&gt; right) m_queue.push(m_queue.front() -&gt; right);            m_queue.pop();        &#125;        res.push_back(raw);    &#125;     <span class="hljs-keyword">return</span> res;&#125;</code></pre><p><a href="https://leetcode-cn.com/problems/li-wu-de-zui-da-jie-zhi-lcof/">剑指 Offer 47. 礼物的最大价值</a></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">maxValue</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&gt;&amp; grid)</span> </span>&#123;    <span class="hljs-keyword">int</span> m = grid.size();    <span class="hljs-keyword">int</span> n = grid[<span class="hljs-number">0</span>].size();    <span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; &gt; <span class="hljs-title">dp</span><span class="hljs-params">(m, <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;(n))</span></span>;   <span class="hljs-comment">// 这个语法使用错了  vector</span>    dp[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] = grid[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>];    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; m; i++) dp[i][<span class="hljs-number">0</span>] = dp[i<span class="hljs-number">-1</span>][<span class="hljs-number">0</span>] + grid[i][<span class="hljs-number">0</span>];    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">1</span>; j &lt; n; j++) dp[<span class="hljs-number">0</span>][j] = dp[<span class="hljs-number">0</span>][j<span class="hljs-number">-1</span>] + grid[<span class="hljs-number">0</span>][j];    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">1</span>; i &lt; m; i++)&#123;        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> j = <span class="hljs-number">1</span>;  j &lt; n; j++)&#123;            dp[i][j] = max(dp[i<span class="hljs-number">-1</span>][j], dp[i][j<span class="hljs-number">-1</span>]) + grid[i][j];        &#125;    &#125;    <span class="hljs-keyword">return</span> dp[m<span class="hljs-number">-1</span>][n<span class="hljs-number">-1</span>];&#125;</code></pre><p><strong>[121] <a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/">https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">maxProfit</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; prices)</span> </span>&#123;    <span class="hljs-keyword">int</span> max_profit = <span class="hljs-number">0</span>;    <span class="hljs-keyword">int</span> min_val = INT_MAX;        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; prices.size(); i++)&#123;        min_val = min(min_val, prices[i]); <span class="hljs-comment">// 保存之前的最小值</span>        max_profit = max(max_profit, prices[i] - min_val);  <span class="hljs-comment">// 当前与最小值的差</span>    &#125;    <span class="hljs-keyword">return</span> max_profit;&#125;</code></pre><p><a href="https://leetcode-cn.com/problems/gou-jian-cheng-ji-shu-zu-lcof/">剑指 Offer 66. 构建乘积数组</a></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">constructArr</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; a)</span> </span>&#123;    <span class="hljs-keyword">if</span>(a.size() == <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> &#123;&#125;;    <span class="hljs-keyword">int</span> len = a.size();    <span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">res</span><span class="hljs-params">(a.size(), a[len<span class="hljs-number">-1</span>])</span></span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = len - <span class="hljs-number">2</span>; i &gt;= <span class="hljs-number">0</span>; i--)        res[i] = res[i+<span class="hljs-number">1</span>] * a[i];    <span class="hljs-keyword">int</span> m = <span class="hljs-number">1</span>;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>;  i &lt; a.size()<span class="hljs-number">-1</span>; i++)&#123;        res[i] = m * res[i+<span class="hljs-number">1</span>];        m *= a[i];    &#125;    res[a.size()<span class="hljs-number">-1</span>] = m;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><p><strong>[110] <a href="https://leetcode-cn.com/problems/balanced-binary-tree/">https://leetcode-cn.com/problems/balanced-binary-tree/</a></strong></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">getDepth</span><span class="hljs-params">(TreeNode * root)</span></span>&#123;    <span class="hljs-keyword">if</span>(root == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;    <span class="hljs-keyword">return</span> max(getDepth(root-&gt;left), getDepth(root-&gt;right)) + <span class="hljs-number">1</span>;&#125;<span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">isBalanced</span><span class="hljs-params">(TreeNode* root)</span> </span>&#123;    <span class="hljs-comment">// 递归的解决方案， 先看退出条件比较好!</span>    <span class="hljs-keyword">if</span>(root == <span class="hljs-literal">NULL</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;    <span class="hljs-keyword">int</span> left_depth = getDepth(root-&gt;left);    <span class="hljs-keyword">int</span> right_depth = getDepth(root-&gt;right);    <span class="hljs-keyword">return</span> <span class="hljs-built_in">abs</span>(left_depth - right_depth) &lt;= <span class="hljs-number">1</span> &amp;&amp; isBalanced(root -&gt; left) &amp;&amp; isBalanced(root -&gt; right);</code></pre><p><a href="https://leetcode-cn.com/problems/xuan-zhuan-shu-zu-de-zui-xiao-shu-zi-lcof/">剑指 Offer 11. 旋转数组的最小数字</a></p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">minArray</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; numbers)</span> </span>&#123;    <span class="hljs-keyword">int</span> left = <span class="hljs-number">0</span>, right = numbers.size() <span class="hljs-number">-1</span>;    <span class="hljs-keyword">while</span>(left &lt; right)&#123; <span class="hljs-comment">// 这里没有等号</span>        <span class="hljs-keyword">int</span> mid = left + (right - left) / <span class="hljs-number">2</span>;        <span class="hljs-keyword">if</span>(numbers[mid] &gt; numbers[right])&#123;            left = mid + <span class="hljs-number">1</span>;        &#125;<span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(numbers[mid] &lt; numbers[right])&#123;            right = mid;        &#125;<span class="hljs-keyword">else</span>&#123;            right -= <span class="hljs-number">1</span>;  <span class="hljs-comment">// 这里要分三种情况的， 等于的时候 right -= 1 即可</span>        &#125;    &#125;    <span class="hljs-keyword">return</span> numbers[left];&#125;</code></pre><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">spiralOrder</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&gt;&amp; matrix)</span> </span>&#123;    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; res;    <span class="hljs-keyword">if</span>(matrix.size() == <span class="hljs-number">0</span> || matrix[<span class="hljs-number">0</span>].size() == <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> res;    <span class="hljs-keyword">int</span> start_r = <span class="hljs-number">0</span>,  end_r = matrix.size() - <span class="hljs-number">1</span>;    <span class="hljs-keyword">int</span> start_c = <span class="hljs-number">0</span>, end_c = matrix[<span class="hljs-number">0</span>].size() - <span class="hljs-number">1</span>;    <span class="hljs-keyword">while</span>(<span class="hljs-literal">true</span>)&#123; <span class="hljs-comment">// 这里退出的条件放在每个内层循环里面</span>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = start_c; i &lt;= end_c; i++) res.push_back(matrix[start_r][i]);        <span class="hljs-keyword">if</span>(++start_r &gt; end_r) <span class="hljs-keyword">break</span>;        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = start_r; i &lt;= end_r; i++) res.push_back(matrix[i][end_c]);         <span class="hljs-keyword">if</span>(--end_c &lt; start_c) <span class="hljs-keyword">break</span>;        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = end_c; i &gt;= start_c; i--) res.push_back(matrix[end_r][i]);        <span class="hljs-keyword">if</span>(--end_r &lt; start_r) <span class="hljs-keyword">break</span>;        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = end_r; i &gt;= start_r; i--) res.push_back(matrix[i][start_c]);        <span class="hljs-keyword">if</span>(++start_c &gt; end_c)  <span class="hljs-keyword">break</span>;    &#125;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; <span class="hljs-title">getLeastNumbers</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; arr, <span class="hljs-keyword">int</span> k)</span> </span>&#123;    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; res;    <span class="hljs-keyword">if</span>(arr.size() == <span class="hljs-number">0</span> || k == <span class="hljs-number">0</span>) <span class="hljs-keyword">return</span> res;    <span class="hljs-built_in">priority_queue</span>&lt;<span class="hljs-keyword">int</span>&gt; max_heap; <span class="hljs-comment">// 最大堆</span>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; arr.size(); i++)&#123;        <span class="hljs-keyword">if</span>(i &lt; k) max_heap.push(arr[i]);        <span class="hljs-keyword">else</span>&#123;            <span class="hljs-keyword">if</span>(arr[i] &lt; max_heap.top())&#123;                max_heap.pop();                max_heap.push(arr[i]);            &#125;        &#125;    &#125;    <span class="hljs-keyword">while</span>(!max_heap.empty())&#123;        res.push_back(max_heap.top());        max_heap.pop();    &#125;    <span class="hljs-keyword">return</span> res;&#125;</code></pre><h4 id="剑指-Offer-58-I-翻转单词顺序"><a href="#剑指-Offer-58-I-翻转单词顺序" class="headerlink" title="剑指 Offer 58 - I. 翻转单词顺序"></a><a href="https://leetcode-cn.com/problems/fan-zhuan-dan-ci-shun-xu-lcof/">剑指 Offer 58 - I. 翻转单词顺序</a></h4><pre><code class="hljs cpp"><span class="hljs-comment">// 全手写: 有点繁琐</span><span class="hljs-function"><span class="hljs-built_in">string</span> <span class="hljs-title">reverseWords</span><span class="hljs-params">(<span class="hljs-built_in">string</span> s)</span> </span>&#123;    <span class="hljs-comment">// 预处理: 去除首尾字符</span>    s.erase(<span class="hljs-number">0</span>, s.find_first_not_of(<span class="hljs-string">&quot; &quot;</span>));      s.erase(s.find_last_not_of(<span class="hljs-string">&quot; &quot;</span>) + <span class="hljs-number">1</span>);    <span class="hljs-keyword">if</span>(s.empty()) <span class="hljs-keyword">return</span> s; <span class="hljs-comment">// 可以防止多空格的情况</span>    <span class="hljs-comment">// 单词拆分</span>    <span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">string</span>&gt; vec_word;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; s.size();)&#123;        <span class="hljs-keyword">int</span> j = i + <span class="hljs-number">1</span>;        <span class="hljs-keyword">while</span>(j &lt; s.size() &amp;&amp; s[j] != <span class="hljs-string">&#x27; &#x27;</span>) j++;        vec_word.push_back(s.substr(i, j-i));        <span class="hljs-keyword">while</span>(j &lt; s.size() &amp;&amp; s[j] == <span class="hljs-string">&#x27; &#x27;</span>) j++;        i = j;    &#125;    <span class="hljs-comment">// 连接</span>    <span class="hljs-built_in">string</span> res;    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> i = vec_word.size()<span class="hljs-number">-1</span>; i &gt; <span class="hljs-number">0</span>; i--)&#123;        res += vec_word[i];        res += <span class="hljs-string">&quot; &quot;</span>;    &#125;    res += vec_word[<span class="hljs-number">0</span>];    <span class="hljs-keyword">return</span> res;&#125;</code></pre><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://leetcode.com/">https://leetcode.com/</a></li><li>《剑指 offer》</li><li>《Cracking the Coding Interview: 150 Programming Interview Questions and Solutions》</li><li><a href="https://cspiration.com/leetcodeClassification">https://cspiration.com/leetcodeClassification</a></li><li><a href="https://greyireland.gitbook.io/algorithm-pattern/">https://greyireland.gitbook.io/algorithm-pattern/</a></li><li>leetcode 刷题班 <a href="https://www.bilibili.com/video/BV1GW411Q77S">https://www.bilibili.com/video/BV1GW411Q77S</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>Interview</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>int8_quantize</title>
    <link href="/2020/02/07/int8-quantize/"/>
    <url>/2020/02/07/int8-quantize/</url>
    
    <content type="html"><![CDATA[<h3 id="1-FP32-vs-int8"><a href="#1-FP32-vs-int8" class="headerlink" title="1. FP32 vs int8"></a>1. FP32 vs int8</h3><p>​        首先来看一下 <strong>FP32、FP16 和 int8</strong>之间的动态范围和精度的对比， 可以看到float32 的取值范围几乎是无穷的，而int8只有<strong>-128~127</strong>. 因此需要建立映射关系将float32类型的浮点数映射到指定范围的int8类型。 </p><p><img src="/2020/02/07/int8-quantize/fpvsint8.png" alt></p><p>​        目前最常见的两种<strong>int8量化实现方式</strong>是 <strong>Nvidia的TensorRT方案</strong> 和 <strong>google的方案</strong>，其中前者直接量化，无需retrain，实现简单；而后者需要重新训练，稍显复杂。 大部分的开源方案是基于前者的，<strong>本文基于ncnn框架，主要讲述第一种解决方案。</strong></p><h3 id="2-TensorRT-int8-量化方案"><a href="#2-TensorRT-int8-量化方案" class="headerlink" title="2. TensorRT int8 量化方案"></a>2. TensorRT int8 量化方案</h3><p>​    Nvidia 的 TensorRT提供了一种量化方案，但是它仅仅提供相应的SDK和解决方案， 没有公布对应的源代码， 诸多第三方厂家则根据该解决方案自己造轮子，产生了对应的解决方案。该量化方案的最重要的两份参考资料如下所示:</p><ul><li><p>TensorRT Develop guide: <a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#work-with-qat-networks">https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#work-with-qat-networks</a></p></li><li><p>PDF 链接： <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf</a></p></li></ul><p><strong>(1)</strong> <strong>max-max 映射</strong>： 最简单粗暴的方式如下左图所示</p><p>​    首先求出一个laye 的激活值范围， 然后按照绝对值的最大值作为阈值， 然后把这个范围按照比例映射到-127到128的范围内, 其fp32和int8的转换公式为:</p><pre><code class="hljs lisp">FP32 Tensor (<span class="hljs-name">T</span>) = scale_factor(<span class="hljs-name">sf</span>) * <span class="hljs-number">8</span>-bit Tensor(<span class="hljs-name">t</span>) + FP32_bias (<span class="hljs-name">b</span>)</code></pre><p>​    通过实验得知，bias值去掉对精度的影响不是很大，因此我们直接去掉, 所以该公式可以简化为:</p><pre><code class="hljs excel"><span class="hljs-built_in">T</span> = sf * <span class="hljs-built_in">t</span></code></pre><p><img src="/2020/02/07/int8-quantize/tensorrt_qua.png" alt></p><p><strong>(2) 饱和映射</strong></p><p>​    如上方法会有一个问题：<strong>不饱和，即通常在正负上会有一些量化值未被利用，且会产生的精度损失较大。</strong>针对 max-max 映射存在的问题， TensorRT提出了如上右图的饱和映射。 <strong>选取一个阈值T，然后将 -|T|~|T| 之间的值映射到 -127 到 128 这个范围内。这样确定了阈值T之后，其实也能确定Scale，一个简单的线性公式是: Scale = T/127。 所以要计算Scale，只要找到合适的阈值T就可以了。那么问题来了，T应该取何值? 其基本流程如下:</strong></p><p>​    <strong>(a) 选取不同的 T 阈值进行量化, 将 P(fp32) 映射到 Q(int8)。</strong></p><p>​    <strong>(b) 将 Q(int8) 反量化到 P(fp32) 一样长度，得到分布 Q_expand；</strong></p><p>​    <strong>(c) 计算P和Q_expand 的相对熵(KL散度)，然后选择相对熵最少的一个，也就是跟原分布最像的一个,</strong> <strong>从而确定Scale</strong>。</p><p><strong>(3) KL 散度</strong></p><p>​    KL散度可以用来<strong>描述P、Q两个分布的差异</strong>。<strong>散度越小，两个分布的差异越小，概率密度函数形状和数值越接近</strong>。这里的所有分布、计算，都是离散形式的。分布是以统计直方图的方式存在，KL散度公式也是离散公式：</p><p><img src="/2020/02/07/int8-quantize/kl.png" alt="img"></p><p>​        从上式中我们还发现一个问题：KL散度计算公式要求P、Q两个统计直方图长度一样（也就是bins的数量一样）。Q一直都是-127～127；可是P的数量会随着T的变化而变化。那这怎么做KL散度呢？</p><p>ncnn 的做法是将 Q扩展到和P一样的长度，下面举个例子(NVIDIA PPT中的例子)：</p><pre><code class="hljs python">P = [<span class="hljs-number">1</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">5</span> <span class="hljs-number">3</span> <span class="hljs-number">1</span> <span class="hljs-number">7</span>]     // fp32的统计直方图，T=<span class="hljs-number">8</span> // 假设只量化到两个bins，即量化后的值只有<span class="hljs-number">-1</span>/<span class="hljs-number">0</span>/+<span class="hljs-number">1</span>三种 Q=[<span class="hljs-number">1</span>+<span class="hljs-number">0</span>+<span class="hljs-number">2</span>+<span class="hljs-number">3</span>, <span class="hljs-number">5</span>+<span class="hljs-number">3</span>+<span class="hljs-number">1</span>+<span class="hljs-number">7</span>] = [<span class="hljs-number">6</span>, <span class="hljs-number">16</span>]  // P和Q现在没法做KL散度，所以要将Q扩展到和P一样的长度 Q_expand = [<span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">6</span>/<span class="hljs-number">3</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>, <span class="hljs-number">16</span>/<span class="hljs-number">4</span>]=[<span class="hljs-number">2</span> <span class="hljs-number">0</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span> <span class="hljs-number">4</span>]  // P中有<span class="hljs-number">0</span>时不算在内 D = KL(P||Q_expand)  // 这样就可以做KL散度计算了</code></pre><p>​    这个扩展的操作，就像图像的上采样一样，将低精度的统计直方图(Q)，上采样的高精度的统计直方图上去(Q_expand)。由于Q中一个bin对应P中的4个bin，因此在Q上采样的Q_expand的过程中，所有的数据要除以4。另外，在计算fp32的分布P时，被T截断的数据，是要算在最后一个bin里面的。</p><h3 id="3-ncnn的conv量化计算流程"><a href="#3-ncnn的conv量化计算流程" class="headerlink" title="3. ncnn的conv量化计算流程"></a>3. ncnn的conv量化计算流程</h3><p>正常的 fp32 计算中， 一个conv 的计算流程如下所示， 所有的数据均是 fp32， 没什么特殊的</p><p><img src="/2020/02/07/int8-quantize/fpconv.png" alt></p><p>在 ncnn conv 进行Int8计算时， 计算流程如下所示，ncnn首先<strong>将输入(bottom_blob)和权重量化成Int8，在Int8下计算卷积，然后反量化到 fp32，再和未量化的bias相加，得到输出 top_blob</strong>(ncnn并没有对bias做量化)</p><p><img src="/2020/02/07/int8-quantize/int8conv.png" alt></p><p>输入和权重的<strong>量化公式</strong>为:</p><script type="math/tex; mode=display">bottom\_blob[int8] = bottom\_blob\_in8t\_scale * bottom[fp32] \\weight\_blob[int8] = weight\_data\_int8\_scale * weight[fp32]</script><p><strong>反量化</strong>的目的是将int8映射回到原来的fp32,范围保持要一致, 由于 weight_blob(int8) 和 bottom_blob(int8) 相乘， 所以此处的量化反量化的 scale 应该为:</p><script type="math/tex; mode=display">dequantize\_scale = 1/(bottom\_blob\_int8\_scale * weight\_data\_int8\_scale) \\innner\_blob[fp32] = dequantize\_scale * inner\_blob</script><p>! 值得注意的是， 权重是在网络初始化时候就进行量化了， 而输入则是在前向推导时进行量化。</p><h3 id="4-ncnn-量化工具的使用"><a href="#4-ncnn-量化工具的使用" class="headerlink" title="4. ncnn 量化工具的使用"></a>4. ncnn 量化工具的使用</h3><p>(1)  Optimization graphic</p><pre><code class="hljs smali"><span class="hljs-keyword">.</span>/ncnnoptimize mobilenet-fp32.param mobilenet-fp32.bin mobilenet-nobn-fp32.param mobilenet-nobn-fp32.bin</code></pre><p>(2) Create the calibration table file</p><pre><code class="hljs angelscript">./ncnn2table --param mobilenet-nobn-fp32.param --bin mobilenet-nobn-fp32.bin --images images/ --output mobilenet-nobn.table --mean <span class="hljs-number">104</span>,<span class="hljs-number">117</span>,<span class="hljs-number">123</span> --norm <span class="hljs-number">0.017</span>,<span class="hljs-number">0.017</span>,<span class="hljs-number">0.017</span> --size <span class="hljs-number">224</span>,<span class="hljs-number">224</span> --thread <span class="hljs-number">2</span></code></pre><p>(3) Quantization</p><pre><code class="hljs stylus">./ncnn2int8 mobilenet-nobn-fp32<span class="hljs-selector-class">.param</span> mobilenet-nobn-fp32<span class="hljs-selector-class">.bin</span> mobilenet-int8<span class="hljs-selector-class">.param</span> mobilenet-int8<span class="hljs-selector-class">.bin</span> mobilenet-nobn.table</code></pre><h3 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h3><p>[1] <a href="https://me.csdn.net/sinat_31425585">https://me.csdn.net/sinat_31425585</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/c_1064124187198705664">https://zhuanlan.zhihu.com/c_1064124187198705664</a></p><p>[3] <a href="https://github.com/BUG1989/caffe-int8-convert-tools">https://github.com/BUG1989/caffe-int8-convert-tools</a></p><p>[4] <a href="https://github.com/Tencent/ncnn/wiki/quantized-int8-inference">Tencent/ncnn</a></p><p>[5] QNNPACK</p><p>[6] Nvidia solution： Szymon Migacz. 8-bit Inference with TensorRT</p><p>[7] Google solution：Quantizing deep convolutional networks for efficient inference: A whitepaper</p>]]></content>
    
    
    <categories>
      
      <category>DL_Deploy</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>RNN</title>
    <link href="/2020/02/06/RNN/"/>
    <url>/2020/02/06/RNN/</url>
    
    <content type="html"><![CDATA[<hr><p>title: RNN<br>date: 2019-03-05 11:49:20<br>tags:</p><h2 id="mathjax-true"><a href="#mathjax-true" class="headerlink" title="mathjax: true"></a>mathjax: true</h2><p>​        <strong>循环神经网络（Recurrent Neural Network）</strong>是用来建模序列化数据的一种主流深度学习模型。我们知道，传统的前馈神经网络一般的输入都是一个定长的向量，无法处理变长的序列信息，即使通过一些方法把序列处理成定长的向量，模型也很难捕捉序列中的长距离依赖关系。RNN则通过将神经元串行起来处理序列化的数据。由于每个神经元都能用它的内部变量保存之前输入的序列信息，因此整个序列被浓缩成抽象的表示，并可以据此进行分类或生成新的序列。近年来RNN在很多领域取得突破性进展 —— 机器翻译、序列标注、图像描述、推荐系统、智能聊天机器人、自动作词作曲等。</p><p>下图展示了一个典型的循环神经网络结构：</p><p><img src="/2020/02/06/RNN/rnn.png" alt></p><p>一个长度为T 的序列用循环神经网络建模，展开之后可以看做是一个T曾的前馈神经网络。其中，第 $t$ 层的隐含状态 $h<em>{t}$ 编码了序列中前 $t$ 个输入的信息，可以通过当前的输入$x</em>{t}$和上一层的状态 $h<em>{t-1}$ 计算得到；最后一层的状态 $h</em>{t}$ 编码了整个序列的信息，因此可以作为整个序列的压缩表示。循环神经网络的前向传播公式为:</p><script type="math/tex; mode=display">net_{t} = Ux_{t} + Wh_{t-1}  \\h_{t} = f(net_{t})  \\y = g(Vh_{t})</script><p>​    其中 $f$ 和 $g$  为激活函数，$U$ 为输入层到隐含层的权重矩阵，$W$ 为隐含层从上一时刻到下一时刻状态转移的权重矩阵。$f$ 可以选取 Tanh 函数 或者 ReLU 函数，$g$ 可以采用 Softmax 函数。</p><h5 id="问题1：循环神经网络的梯度消失问题"><a href="#问题1：循环神经网络的梯度消失问题" class="headerlink" title="问题1：循环神经网络的梯度消失问题"></a>问题1：循环神经网络的梯度消失问题</h5><p>循环神经网络模型的求解可以采用 <strong>BPTT</strong> 算法实现， BPTT 实际上是反向传播算法的变种。这一现象主要源于深度神经网络中的梯度消失。传统的循环神经网络梯度可以写成连乘的形式：</p><script type="math/tex; mode=display">\frac{\partial{net_t}}{\partial{net_1}} = \frac{\partial{net_t}}{\partial{net_{t-1}}}.\frac{\partial{net_{t-1}}}{\partial{net_{t-2}}} ...\frac{\partial{net_{2}}}{\partial{net_{1}}}</script><p>其中，</p><script type="math/tex; mode=display">net_{t} = Ux_{t} + Wh_{t-1} \\h_{t} = f(net_{t}) \\y = g(Vh_{t})</script><script type="math/tex; mode=display">\frac{\partial{net_t}}{\partial{net_{t-1}}} = \frac{\partial{net_t}}{\partial{h_{t-1}}}\frac{\partial{h_{t-1}}}{\partial{net_{t-1}}}= W \cdot diag[f'(net_{t-1})] \\=   \begin{bmatrix}W_{11}f'(net_{t-1}^1)      &  \dots  & W_{1n}f'(net_{t-1}^n) \\\vdots  &  \ddots & \vdots  \\W_{n1}f'(net_{t-1}^1)       &  \dots  & W_{nn}f'(net_{t-1}^n) \end{bmatrix}</script><p>其中 $n$ 为 隐含层 $h<em>{t-1}$  的维度（即隐含单元的个数）， $\frac{\partial{net_1}}{\partial{net</em>{t-1}}}$ 对应 $n*n$ 维矩阵，又被称为雅各比矩阵。</p><p>由于预测误差是沿着神经网络的每一层反向传播的，因此当雅各比矩阵的最大特征值大于1时， 随着离输出越来越远，每层梯度大小会呈指数增长，导致梯度爆炸；反之，若雅各比矩阵的最大特征值小于1， 梯度的大小会呈指数减小，产生梯度消失。</p><p>梯度爆炸的问题可以通过梯度裁剪来缓解，即当梯度的范式大于某个某个给定值时， 对梯度进行等比收缩。而对于梯度消失问题， 可以通过加入门口机制来弥补梯度消失带来的损失。</p><h5 id="问题-2：-在循环神经网络中，是否可以使用-ReLU-作为激活函数？"><a href="#问题-2：-在循环神经网络中，是否可以使用-ReLU-作为激活函数？" class="headerlink" title="问题 2： 在循环神经网络中，是否可以使用 ReLU 作为激活函数？"></a>问题 2： 在循环神经网络中，是否可以使用 ReLU 作为激活函数？</h5><p>可以，但是需要将$W$初始化为单位矩阵。如果不然会引发严重的数值问题和梯度消失或爆炸问题。</p><p><strong>数值问题：</strong></p><p>考虑前向传播公式：</p><script type="math/tex; mode=display">net_{t} = Ux_{t} + Wh_{t-1}  \\h_{t} = f(net_{t})  \\</script><p>根据前向传播公式向前传递一层，可以得到</p><script type="math/tex; mode=display">net_{t} = Ux_{t} + Wh_{t-1} + Wf(Ux_{t-1} + Wh_{t-2})</script><p>如果使用 $ReLU$ 替代公式中的激活函数 $f$， 并且假设 $ReLU$ 处于激活区域（即输入大于0），则有 $f(x)=x, net<em>{t}=Ux</em>{t}+W(Ux<em>{t-1}+Wh</em>{t-2})$, 继续将其展开，$net_{t}$ 的表达式中最终将包含$t$ 个 $W$ 连乘。如果 $W$ 不是单位矩阵，最终的结果将趋于 $0$ 或者无穷，引发严重的数值问题。</p><p><strong>梯度爆炸或梯度消失：</strong></p><p>考虑循环神经网络的梯度计算公式：</p><script type="math/tex; mode=display">\frac{\partial{net_t}}{\partial{net_{t-1}}} = W \cdot diag[f'(net_{t-1})] \\=   \begin{bmatrix}W_{11}f'(net_{t-1}^1)      &  \dots  & W_{1n}f'(net_{t-1}^n) \\\vdots  &  \ddots & \vdots  \\W_{n1}f'(net_{t-1}^1)       &  \dots  & W_{nn}f'(net_{t-1}^n) \end{bmatrix}</script><p>假设采用 $ReLU$ 激活函数，且一开始所有的神经元都处于激活中， 则 $diag[f’(net<em>{t-1})]$ 为单位矩阵， 有 $\frac{\partial{net_t}}{\partial{net</em>{t-1}}}=W$。在梯度经过了$n$ 层之后， $\frac{\partial{net<em>{t}}}{\partial{net</em>{1}}} = W^n$。可以看到即使采用了 $ReLU$ 激活函数，只要$W$ 不是单位矩阵，还是会出现梯度消失或者爆炸的现象。</p><h5 id="问题-3：-LSTM-是如何实现长短期记忆功能的？"><a href="#问题-3：-LSTM-是如何实现长短期记忆功能的？" class="headerlink" title="问题 3： LSTM 是如何实现长短期记忆功能的？"></a>问题 3： LSTM 是如何实现长短期记忆功能的？</h5><p>​    与传统的循环神经网络相比，LSTM 仍然是基于$x<em>t$ 和 $h</em>{t-1}$ 来计算 $h_t$, 只不过对内部的结构进行了更加精心的设计，加入了输入门 $i_t$ 、遗忘门 $f_t$ 以及输出门 $o_t$ 三个门和一个内部记忆单元 $c_t$。</p><p>​    输入门控制当前计算的新状态以多大程度更新到记忆单元中；遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉；输出门控制当前的输出有多大程度取决于当前的记忆单元。</p><p>经典的LSTM中， 第 $t$ 步的更新计算公式为：</p><script type="math/tex; mode=display">i_t = \sigma(W_{i}x_{t}+U_{i}h_{t-1}+b_{i}) \\f_t = \sigma(W_{f}x_{t}+U_{f}h_{t-1}+b_{f}) \\o_t = \sigma(W_{o}x_{t}+U_{o}h_{t-1}+b_{o}) \\\widetilde{c_{t}} = Tanh(W_{c}x_{t}+U_{c}h_{t-1}) \\c_{t} = f_{t} \odot c_{t-1} + i_{t} \odot \widetilde{c_{t}} \\h_{t} = o_{t} \odot Tanh(c_t)</script><p>其中<strong>输入门  $i<em>{t}$ 是通过输入 $x_t$和上一步的隐含层输出 $h</em>{t-1}$ 来进行线性变换，再经过激活函数 $\sigma$ 得到的</strong>。输入门 $i<em>t$  的结果是向量，其中每个元素是 0 到 1 之间的实数， 用于控制各维度流过阀门的信息量；$W</em>{i}$、$U<em>i$ 两个矩阵核向量 $b_i$ 为输入门的参数，是在训练过程中得到的。**遗忘门 $f</em>{t}$ 和输出门 $o<em>{t}$ 的计算方式与输入门类似，它们有各自的参数 $W$、$U$ 和 $b$ **。 与传统的循环神经网络不通的是，从上一个记忆单元的状态 $c</em>{t-1}$ 到 当前的状态 $c_{t} ​$ 的转移不一定完全取决于激活函数计算得到的状态，还由输入门和遗忘门来共同控制。</p><p>​    在一个循环好的网络中，当输入的序列中没有重要信息时， $LSTM$ 的遗忘门的值会接近于 1， 输入门的值会接近于0， 此时过去的信息回本保存，从而实现长期记忆功能；当输入序列中出现重要信息时，$LSTM$ 应当将其存入记忆中，此时其输入门的值会接近于1； 当输入的序列出现了重要信息，且该信息意味着之前的记忆不在重要时，输入门的值会接近于1， 而遗忘门的值接近于0， 这样旧的记忆被遗忘，新的重要信息被记忆。经过这样的设计，整个网络更容易学到序列之间的长期依赖。</p><p><img src="/2020/02/06/RNN/lstm_1.png" alt></p><h5 id="问题4：LSTM-里各个模块分别使用什么激活函数，可以使用别的激活函数吗？"><a href="#问题4：LSTM-里各个模块分别使用什么激活函数，可以使用别的激活函数吗？" class="headerlink" title="问题4：LSTM 里各个模块分别使用什么激活函数，可以使用别的激活函数吗？"></a>问题4：LSTM 里各个模块分别使用什么激活函数，可以使用别的激活函数吗？</h5><p>关于激活函数的选取， <strong>在 $LSTM​$ 中，遗忘门、输入门和输出门使用 $Sigmoid​$ 函数作为激活函数；在生成候选记忆时，使用双曲正切函数 $Tanh​$ 作为激活函数。</strong><br> （1）  这两个激活函数都是饱和的，也就是说在输入达到一定值得情况下，输出就不会发生明显变化了。如果使用非饱和的激活函数，例如 $ReLU​$，那么将很难实现门控的效果。</p><p>（2） $Sigmoid$ 函数的输出在 0 ~ 1 之间，<strong>符合门控的物理定义</strong>。当输入较大或者较小时， 其输出会非常接近1或 0， 从而保证该门的开关。</p><p>（3）在生成候选记忆时，使用 $Tanh$ 函数，时因为其输出在-1~1 之间，这<strong>与大多数场景下特征分布是0中心吻合</strong>。此外，Tanh 函数在输入为 $0$ 附近相比 ​$Sigmoid$ 含有有更大的梯度，通常使<strong>模型收敛更快</strong>。</p><p>此外，<strong>在一些对计算能力有限制的设备，诸如可穿戴设备中，由于 $Sigmoid​$ 函数求指数需要一定的计算量，此时会使用0/1门$（hard gard）​$让门控输出为0或者1的离散值，即当输入小于阈值时，门控输出为0；当输入大于阈值时，输出为1</strong>。从而在性能下降不明显的情况下，减少计算量。</p><h5 id="问题-5：-什么是-Seq2Seq模型？Seq2Seq模型有哪些优点？"><a href="#问题-5：-什么是-Seq2Seq模型？Seq2Seq模型有哪些优点？" class="headerlink" title="问题 5： 什么是 Seq2Seq模型？Seq2Seq模型有哪些优点？"></a>问题 5： 什么是 Seq2Seq模型？Seq2Seq模型有哪些优点？</h5><p>$Seq2Seq$ 模型的核心思想是，<strong>通过深度神经网络将一个作为输入的序列映射为一个作为输出的序列，这一过程由编码输入与解码输出两个环节构成</strong>。在经典的实现中，编码器和解码器各由一个循环神经网络构成，既可以选择传统的循环神经网络结构，也可以使用长短期记忆模型。门控循环单元等。在 $Seq2Seq$ 模型中，两个循环神经网络是共同训练的。典型的循环神经网络编解码结构图如下所示:</p><p><img src="/2020/02/06/RNN/seq2seq_ts.png" alt></p><h5 id="问题-6：-Seq2Seq-模型在解码时，-有哪些常用的方法？"><a href="#问题-6：-Seq2Seq-模型在解码时，-有哪些常用的方法？" class="headerlink" title="问题 6： Seq2Seq 模型在解码时， 有哪些常用的方法？"></a>问题 6： Seq2Seq 模型在解码时， 有哪些常用的方法？</h5><p>$Seq2Seq$ 模型最基础的解码方法是<strong>贪心法</strong>， 即选取一种度量标准后，每次都在当前状态下选择最佳的一个结果， 直到结束。很显然，贪心法获得的是一个局部最优解，由于实际问题的复杂性，该方法往往不能取得更好的效果。<strong>集束搜索(beam search) </strong> 是一种常见的改进算法。该方法会<strong>保存 beam size 个当前的较佳选择，然后解码时每一步根据保存的选择进行下一步扩展和排序，接着选择前 beam size 个进行保存，循环迭代，直到结束时选择最佳的一个作为解码的结果。</strong> 随着 beam size 的增大，其搜索的空间增大，最终效果会有所提升，但需要的计算量也相应增大。在实际应用中，b往往会选择一个适中的范围，以8 ~ 12 为佳。下图是 beam size 为2时的集束搜索示例。</p><p><img src="/2020/02/06/RNN/beam search.png" alt></p><p>解码时使用<strong>堆叠的RNN</strong>、<strong>增加 Dropout机制</strong>、<strong>与编码器之间建立残差连接</strong>、<strong>增加Attention机制</strong>等，均是常见的改进措施。在实际的研究工作中，可以根据不同使用场景，有针对性的选择和实践。</p><h5 id="问题7：Seq2Seq-模型引入注意力机制是为了解决什么问题？-为什么选用双向的循环神经网络模型？"><a href="#问题7：Seq2Seq-模型引入注意力机制是为了解决什么问题？-为什么选用双向的循环神经网络模型？" class="headerlink" title="问题7：Seq2Seq 模型引入注意力机制是为了解决什么问题？ 为什么选用双向的循环神经网络模型？"></a>问题7：Seq2Seq 模型引入注意力机制是为了解决什么问题？ 为什么选用双向的循环神经网络模型？</h5><p>在 Seq2Seq 模型中，当前隐状态以及上一个输出词决定了当前的输出词， 即：</p><script type="math/tex; mode=display">s_{i} = f(y_{i-1}, s_{i-1}) \\p(y_{i} | y_{1}, y_{2}, ..., y_{i-1}) = g(y_{i-1}, s_{i})</script><p><strong>在实际应用中会发现随着序列的增长，模型的性能发生显著下降。这是因为编码时输入序列的全部信息被压缩到一个向量表示中。随着序列增长，句子约前面的信息就丢失的越严重。</strong>$Seq2Seq$ 模型中引入注意力机制就是为了解决这个问题。</p><p><img src="/2020/02/06/RNN/seq2seq_attn.png" alt></p><p>(1)  在编码过程中，我们仍然使用普通的循环神经网络对输入序列进行编码，得到隐状态 $h1, h2, …, h<em>T$。使用一个神经网络 $align$ 上一个输入序列的隐状态 $s</em>{i-1}$ 和输入序列隐状态  $h<em>{j}$ 作为输入，计算出一个 $x</em>{j}, y<em>{i}$ 对齐的值 $e</em>{ij}$， 再归一化得到权重 $\alpha_{ij}$。</p><script type="math/tex; mode=display">e_{ij} = align(s_{i-1}, h_{j})  \\\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T}{exp(e_{ik})}}</script><p>(2) 计算语境向量 $c_{i}​$ , 即是输入序列全部隐状态 $h1, h2, …, h_T​$的一个加权和。</p><script type="math/tex; mode=display">c_{i} = \sum_{j=1}^{T}{\alpha_{ij}h_{j}}</script><p>(3) 在解码过程中，每一个输出词都依赖于前一个隐状态以及输入序列每一个对应的隐状态。</p><script type="math/tex; mode=display">s_{i} = f(y_{i-1}, s_{i-1}, x_{i}) \\p(y_{i} | y_{1}, y_{2}, ..., y_{i-1}) = g(y_{i-1}, s_{i}, c_{i})</script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>camera-calibration</title>
    <link href="/2019/06/09/camera-calibration/"/>
    <url>/2019/06/09/camera-calibration/</url>
    
    <content type="html"><![CDATA[<p>相机标定相关总结</p><a id="more"></a><h4 id="1-为什么需要相机标定？"><a href="#1-为什么需要相机标定？" class="headerlink" title="1. 为什么需要相机标定？"></a>1. 为什么需要相机标定？</h4><p>相机机标定的目的：建立相机成像几何模型并矫正透镜畸变。</p><p><strong>建立相机成像集合模型</strong>：计算机视觉的首要任务就是要通过拍摄到的图像信息获取到物体在真实三维世界里相对应的信息，于是，建立物体从三维世界映射到相机成像平面这一过程中的几何模型就显得尤为重要，而这一过程最关键的部分就是要得到相机的<strong>内参和外参</strong>。</p><p><strong>矫正透镜畸变</strong>：我们最开始接触到的成像方面的知识应该是有关小孔成像的，但是由于这种成像方式只有小孔部分能透过光线就会导致物体的成像亮度很低，于是聪明的人类发明了透镜。虽然亮度问题解决了，但是新的问题又来了：由于透镜的制造工艺，会使成像产生多种形式的畸变，于是为了去除畸变（使成像后的图像与真实世界的景象保持一致），人们计算并利用<strong>畸变系数</strong>来矫正这种像差。虽然理论上可以设计出不产生畸变的透镜，但其制造工艺相对于球面透镜会复杂很多，所以相对于复杂且高成本的制造工艺，人们更喜欢用数学来解决问题。</p><p>常见术语：</p><ul><li>内参矩阵: Intrinsic Matrix </li><li>焦距: Focal Length </li><li>主点: Principal Point </li><li>径向畸变: Radial Distortion </li><li>切向畸变: Tangential Distortion </li><li>旋转矩阵: Rotation Matrices </li><li>平移向量: Translation Vectors </li><li>平均重投影误差: Mean Reprojection Error </li><li>重投影误差: Reprojection Errors </li><li>重投影点: Reprojected Points</li></ul><h4 id="2-基础知识"><a href="#2-基础知识" class="headerlink" title="2. 基础知识"></a>2. 基础知识</h4><p>相机标定的目的之一是为了建立物体从三维世界到成像平面上各坐标点的对应关系，所以首先我们需要定义这样几个坐标系来为整个过程做好铺垫：</p><p><strong>世界坐标系(world coordinate system)</strong>：用户定义的三维世界的坐标系，为了描述目标物在真实世界里的位置而被引入，单位为m。</p><p><strong>相机坐标系(camera coordinate system)</strong>：在相机上建立的坐标系，为了从相机的角度描述物体位置而定义，作为沟通世界坐标系和图像/像素坐标系的中间一环。单位为m。</p><p><strong>图像坐标系(image coordinate system)</strong>：为了描述成像过程中物体从相机坐标系到图像坐标系的投影透射关系而引入，方便进一步得到像素坐标系下的坐标。 单位为m。</p><p><strong>像素坐标系(pixel coordinate system)</strong>：为了描述物体成像后的像点在数字图像上（相片）的坐标而引入，是我们真正从相机内读取到的信息所在的坐标系。单位为个（像素数目）。</p><p>下图可以更清晰地表达这四个坐标系之间的关系：</p><p><img src="/2019/06/09/camera-calibration/1.jpg" alt="img"></p><p>世界坐标系：$X_w$ 、$Y_w$、$Z_w$</p><p>相机坐标系： $X_c$、$Y_c$、$Z_c$</p><p>图像坐标系：$x$、$y$</p><p>像素坐标系：$u$、$v$。</p><p>相机坐标系的 $Z$ 轴与光轴重合，且垂直于图像坐标系平面并通过图像坐标系的原点，相机坐标系与图像坐标系之间的距离为焦距$f$（也即图像坐标系原点与焦点重合）。像素坐标系平面$u-v$和图像坐标系平面$x-y$重合，但像素坐标系原点位于图中左上角（之所以这么定义，目的是从存储信息的首地址开始读写）。</p><p><strong>棋盘</strong>是一块由黑白方块间隔组成的标定板，我们用它来作为相机标定的<strong>标定物</strong>（从真实世界映射到数字图像内的对象）。之所以我们用棋盘作为标定物是因为平面棋盘模式更容易处理（相对于复杂的三维物体），但与此同时，二维物体相对于三维物体会缺少一部分信息，于是<strong>我们会多次改变棋盘的方位来捕捉图像，以求获得更丰富的坐标信息</strong>。如下图所示，是相机在不同方向下拍摄的同一个棋盘图像。</p><p><img src="/2019/06/09/camera-calibration/2.png" alt></p><p>下面将依次对刚体进行一系列变换，使之<strong>从世界坐标系进行仿射变换、投影透射，最终得到像素坐标系下的离散图像点，过程中会逐步引入各参数矩阵</strong>。</p><h5 id="1-从世界坐标系到相机坐标系"><a href="#1-从世界坐标系到相机坐标系" class="headerlink" title="(1) 从世界坐标系到相机坐标系"></a>(1) 从世界坐标系到相机坐标系</h5><p>刚体从世界坐标系转换到相机坐标系的过程，可以通过旋转和平移来得到，我们将其变换矩阵由一个旋转矩阵和平移向量组合成的齐次坐标矩阵来表示：</p><p><img src="/2019/06/09/camera-calibration/4.png" alt></p><p>其中，$R$为旋转矩阵，$t$为平移向量。其中变换矩阵</p><p><img src="/2019/06/09/camera-calibration/7.png" alt></p><p>即为前文提到的外参矩阵，之所称之为外参矩阵可以理解为只与相机外部参数有关，且外参矩阵随刚体位置的变化而变化。下图表示了用R，t将上述世界坐标系转换到相机坐标系的过程。</p><p><img src="/2019/06/09/camera-calibration/8.png" alt></p><h5 id="2-从相机坐标系到理想图像坐标系"><a href="#2-从相机坐标系到理想图像坐标系" class="headerlink" title="(2) 从相机坐标系到理想图像坐标系"></a>(2) 从相机坐标系到理想图像坐标系</h5><p>这一过程进行了从三维坐标到二维坐标的转换，也即投影透视过程（用中心投影法将物体投射到投影面上，从而获得的一种较为接近视觉效果的单面投影图，也就是使我们人眼看到景物近大远小的一种成像方式）。我们还是拿针孔成像来说明（除了成像亮度低外，成像效果和透镜成像是一样的，但是光路更简单）。成像过程如图二所示：针孔面（相机坐标系）在图像平面（图像坐标系）和物点平面（棋盘平面）之间，所成图像为倒立实像。</p><p><img src="/2019/06/09/camera-calibration/9.png" alt></p><p>但是为了在数学上更方便描述，我们将相机坐标系和图像坐标系位置对调，变成图三所示的布置方式（没有实际的物理意义，只是方便计算）：</p><p><img src="/2019/06/09/camera-calibration/10.png" alt></p><p>此时，假设相机坐标系中有一点M，则在理想图像坐标系下（无畸变）的成像点P的坐标为（可由相似三角形原则得出）：</p><p><img src="/2019/06/09/camera-calibration/11.png" alt="img"></p><p>将上式化为齐次坐标表示形式为：</p><p><img src="/2019/06/09/camera-calibration/12.png" alt="img"></p><h5 id="3-从理想图像坐标系到实际图像坐标系（考虑畸变）"><a href="#3-从理想图像坐标系到实际图像坐标系（考虑畸变）" class="headerlink" title="(3) 从理想图像坐标系到实际图像坐标系（考虑畸变）"></a>(3) 从理想图像坐标系到实际图像坐标系（考虑畸变）</h5><p>​        透镜的畸变主要分为径向畸变和切向畸变，还有薄透镜畸变等等，但都没有径向和切向畸变影响显著，所以我们在这里只考虑径向和切向畸变。</p><ul><li><strong>径向畸变</strong>是由于透镜形状的制造工艺导致。且越向透镜边缘移动径向畸变越严重。下图所示是径向畸变的两种类型：桶形畸变和枕形畸变。</li></ul><p><img src="/2019/06/09/camera-calibration/13.png" alt="img"></p><p>实际情况中我们常用r=0处的泰勒级数展开的前几项来近似描述径向畸变。矫正径向畸变前后的坐标关系为：</p><p><img src="/2019/06/09/camera-calibration/14.png" alt="img"></p><p>由此可知对于径向畸变，我们有3个畸变参数需要求解。</p><ul><li><strong>切向畸变</strong>是由于透镜和CMOS或者CCD的安装位置误差导致。因此，如果存在切向畸变，一个矩形被投影到成像平面上时，很可能会变成一个梯形。切向畸变需要两个额外的畸变参数来描述，矫正前后的坐标关系为：</li></ul><p><img src="/2019/06/09/camera-calibration/15.png" alt="img"></p><p>由此可知对于切向畸变，我们有2个畸变参数需要求解。</p><p>综上，我们一共需要5个畸变参数（k1、k2、k3、p1和p2 ）来描述透镜畸变。</p><h5 id="4-从实际图像坐标系到像素坐标系"><a href="#4-从实际图像坐标系到像素坐标系" class="headerlink" title="(4) 从实际图像坐标系到像素坐标系"></a>(4) 从实际图像坐标系到像素坐标系</h5><p>由于定义的像素坐标系原点与图像坐标系原点不重合，假设图像坐标系原点在像素坐标系下的坐标为（u0，v0），每个像素点在图像坐标系x轴、y轴方向的尺寸为：dx、dy，且像点在实际图像坐标系下的坐标为（xc，yc），于是可得到像点在像素坐标系下的坐标为：</p><p><img src="/2019/06/09/camera-calibration/16.png" alt="img"></p><p>化为齐次坐标表示形式可得：</p><p><img src="/2019/06/09/camera-calibration/17.png" alt="img"></p><p>公式2中(xp, yp)与公式5中(xc, yc)相同，都是图像坐标系下的坐标。</p><p>若暂不考虑透镜畸变，则将式2与式5的转换矩阵相乘即为内参矩阵M：</p><p><img src="/2019/06/09/camera-calibration/17.png" alt="img"></p><p>之所以称之为内参矩阵可以理解为矩阵内各值只与相机内部参数有关，且不随物体位置变化而变化。</p><p>最后用一幅图来总结从世界坐标系到像素坐标系（不考虑畸变）的转换关系：</p><p><img src="/2019/06/09/camera-calibration/3.png" alt></p><h4 id="3-标定方法"><a href="#3-标定方法" class="headerlink" title="3. 标定方法"></a>3. 标定方法</h4><h5 id="1-、传统相机标定"><a href="#1-、传统相机标定" class="headerlink" title="(1)、传统相机标定"></a>(1)、传统相机标定</h5><p>最简单的相机标定为线性标定，即不考虑相机的畸变而只考虑空间坐标转换。<br>每个坐标点有X,Y两个变量，可列两个方程，相机内参有5个未知数，外参平移和旋转各3个，共有11个变量，因此至少需要6个特征点来求解。</p><h5 id="2-、非线性标定"><a href="#2-、非线性标定" class="headerlink" title="(2)、非线性标定"></a>(2)、非线性标定</h5><p>当镜头畸变明显时必须考虑畸变，一般较为便宜的网络摄像头畸变特别大，而价格较贵的工业摄像头则畸变很小，因为其中已经嵌入了许多消除畸变的程序。这时线性模型转化为非线性模型，需要通过非线性标定方法求解。有最速下降法，遗传算法，高斯牛顿法和神经网络算法等。</p><h5 id="3-、张正友标定"><a href="#3-、张正友标定" class="headerlink" title="(3)、张正友标定"></a>(3)、张正友标定</h5><h4 id="4-相机标定步骤："><a href="#4-相机标定步骤：" class="headerlink" title="4.  相机标定步骤："></a>4.  相机标定步骤：</h4><blockquote><p>(1)、打印一张棋盘格，把它贴在一个平面上，作为标定物。<br>(2)、通过调整标定物或摄像机的方向，为标定物拍摄一些不同方向的照片。<br>(3)、从照片中提取棋盘格角点。<br>(4)、估算理想无畸变的情况下，五个内参和六个外参。<br>(5)、应用最小二乘法估算实际存在径向畸变下的畸变系数。<br>(6)、极大似然法，优化估计，提升估计精度。</p></blockquote><h4 id="5-参考资料："><a href="#5-参考资料：" class="headerlink" title="5. 参考资料："></a>5. 参考资料：</h4><ul><li>从零开始学习张氏相机标定发： <a href="https://zhuanlan.zhihu.com/p/35223115">https://zhuanlan.zhihu.com/p/35223115</a></li><li>相机标定（Camera calibration）原理、步骤<a href="https://blog.csdn.net/qq_37791134/article/details/80942171">https://blog.csdn.net/qq_37791134/article/details/80942171</a></li><li>谭平：从相机标定到SLAM，极简三维视觉六小时课程视频 <a href="http://www.sohu.com/a/317611305_100007727">http://www.sohu.com/a/317611305_100007727</a></li><li>SLAM book： <a href="https://github.com/gaoxiang12/slambook">https://github.com/gaoxiang12/slambook</a></li><li>双目视觉之相机标定：<a href="https://www.cnblogs.com/zyly/p/9366080.html">https://www.cnblogs.com/zyly/p/9366080.html</a></li><li>最详细、最完整的相机标定讲解<a href="https://blog.csdn.net/lxy_2011/article/details/80675803">https://blog.csdn.net/lxy_2011/article/details/80675803</a></li><li>OPENCV 相机参数标定 <a href="https://www.jianshu.com/p/967a35dbb56a">https://www.jianshu.com/p/967a35dbb56a</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>探索</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>pytorch深入理解</title>
    <link href="/2019/05/17/pytorch%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
    <url>/2019/05/17/pytorch%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<p>挖个坑：主要想学习一下 Pytorch 的内部实现机制</p><a id="more"></a><p>参考资料</p><p><a href="https://minitorch.github.io/index.html">https://minitorch.github.io/index.html</a></p>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>pytorch分布式训练</title>
    <link href="/2019/05/17/pytorch%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    <url>/2019/05/17/pytorch%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<p>pytorch 分布式解决方案尝试: nn.Dataparallel、torch.distributed 和 horovod 等解决方案</p><a id="more"></a><h4 id="os-environ"><a href="#os-environ" class="headerlink" title="os.environ"></a>os.environ</h4><pre><code class="hljs python">os.environ[<span class="hljs-string">&#x27;NCCL_SOCKET_IFNAME&#x27;</span>] = <span class="hljs-string">&#x27;enp2s0&#x27;</span>os.environ[<span class="hljs-string">&#x27;GLOO_SOCKET_IFNAME&#x27;</span>] = <span class="hljs-string">&#x27;enp2s0&#x27;</span></code></pre><h4 id="nn-DataParallel"><a href="#nn-DataParallel" class="headerlink" title="nn.DataParallel"></a>nn.DataParallel</h4><pre><code class="hljs python">gpus = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]torch.cuda.set_device(<span class="hljs-string">&#x27;cuda:&#123;&#125;&#x27;</span>.format(gpus[<span class="hljs-number">0</span>]))model = nn.DataParallel(model.to(device), device_ids=gpus, output_device=gpus[<span class="hljs-number">0</span>])</code></pre><h4 id="torch-distributed"><a href="#torch-distributed" class="headerlink" title="torch.distributed"></a>torch.distributed</h4><pre><code class="hljs python">parser = argparse.ArgumentParser()parser.add_argument(<span class="hljs-string">&#x27;--backend&#x27;</span>, type=str, default=<span class="hljs-string">&#x27;nccl&#x27;</span>, help=<span class="hljs-string">&#x27;Name of the backend to use.&#x27;</span>)parser.add_argument(<span class="hljs-string">&#x27;-i&#x27;</span>,                    <span class="hljs-string">&#x27;--init-method&#x27;</span>,                    type=str,                    default=<span class="hljs-string">&#x27;env://&#x27;</span>,                    help=<span class="hljs-string">&#x27;URL specifying how to initialize the package.&#x27;</span>)parser.add_argument(<span class="hljs-string">&#x27;-ws&#x27;</span>, <span class="hljs-string">&#x27;--world-size&#x27;</span>, type=int, default=<span class="hljs-number">1</span>, help=<span class="hljs-string">&#x27;Number of processes participating in the job.&#x27;</span>)parser.add_argument(<span class="hljs-string">&#x27;-r&#x27;</span>, <span class="hljs-string">&#x27;--rank&#x27;</span>, type=int, default=<span class="hljs-number">0</span>, help=<span class="hljs-string">&#x27;Rank of the current process.&#x27;</span>)args = parser.parse_args()</code></pre><pre><code class="hljs python">distributed.init_process_group(    backend=args.backend,    init_method=args.init_method,    world_size=args.world_size,    rank=args.rank,)</code></pre><pre><code class="hljs python">train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)</code></pre><pre><code class="hljs python">model = nn.parallel.DistributedDataParallel(model)</code></pre><h4 id="torch-multiprocessing"><a href="#torch-multiprocessing" class="headerlink" title="　torch.multiprocessing"></a>　torch.multiprocessing</h4><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.multiprocessing <span class="hljs-keyword">as</span> mpmp.spawn(main_worker, nprocs=<span class="hljs-number">4</span>, args=(<span class="hljs-number">4</span>, myargs))</code></pre><h4 id="APEX"><a href="#APEX" class="headerlink" title="APEX"></a>APEX</h4><pre><code class="hljs python"><span class="hljs-keyword">from</span> apex <span class="hljs-keyword">import</span> amp<span class="hljs-keyword">from</span> apex.parallel <span class="hljs-keyword">import</span> DistributedDataParallel</code></pre><pre><code class="hljs python">model, optimizer = amp.initialize(model, optimizer)model = DistributedDataParallel(model)<span class="hljs-keyword">with</span> amp.scale_loss(loss, optimizer) <span class="hljs-keyword">as</span> scaled_loss:   scaled_loss.backward()</code></pre><h4 id="Horovod"><a href="#Horovod" class="headerlink" title="Horovod"></a>Horovod</h4><pre><code class="hljs python"><span class="hljs-keyword">import</span> horovod.torch <span class="hljs-keyword">as</span> hvdhvd.local_rank()</code></pre><pre><code class="hljs python">hvd.init()</code></pre><pre><code class="hljs python">train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)</code></pre><pre><code class="hljs reasonml">hvd.broadcast<span class="hljs-constructor">_parameters(<span class="hljs-params">model</span>.<span class="hljs-params">state_dict</span>()</span>, root_rank=<span class="hljs-number">0</span>)</code></pre><pre><code class="hljs python">hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters(), compression=hvd.Compression.fp16)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>pytorch加速</title>
    <link href="/2019/05/17/pytorch%E5%8A%A0%E9%80%9F/"/>
    <url>/2019/05/17/pytorch%E5%8A%A0%E9%80%9F/</url>
    
    <content type="html"><![CDATA[<p>Pytorch 加速方案</p><a id="more"></a><h3 id="1-数据"><a href="#1-数据" class="headerlink" title="1. 数据"></a>1. 数据</h3><h5 id="0-dataloader"><a href="#0-dataloader" class="headerlink" title="(0) dataloader"></a>(0) dataloader</h5><ul><li>num_workers与batch_size调到合适值，并非越大越快（注意后者也影响模型性能）(需要在实验中找到最快的取值)</li><li>eval/test时shuffle=False</li><li>内存够大的情况下，dataloader的<strong>pin_memory</strong>设为True。对特别小的数据集如 MNIST 设置 <code>pin_memory=False</code>  反而更快一些。</li></ul><h5 id="1-预处理提速"><a href="#1-预处理提速" class="headerlink" title="(1) 预处理提速"></a>(1) 预处理提速</h5><ul><li>尽量减少每次读取数据时的预处理操作，可以考虑把一些固定的操作，例如 resize ，事先处理好保存下来，训练的时候直接拿来用</li><li><p>Linux上将预处理搬到GPU上加速：</p></li><li><ul><li><strong>NVIDIA/DALI</strong> ：<a href="https://github.com/NVIDIA/DALI">https://github.com/NVIDIA/DALI</a></li><li><a href="https://github.com/tanglang96/DataLoaders_DALI">https://github.com/tanglang96/DataLoaders_DALI</a></li></ul></li><li><p>数据预取：prefetch_generator（<a href="https://zhuanlan.zhihu.com/p/80695364">方法</a>）让读数据的worker能在运算时预读数据，而默认是数据清空时才读</p></li></ul><h5 id="2-IO-提速"><a href="#2-IO-提速" class="headerlink" title="(2) IO 提速"></a>(2) IO 提速</h5><ul><li><p>使用更快的图片处理：</p></li><li><ul><li><strong>opencv 一般要比 PIL 要快</strong></li><li>对于jpeg读取，可以尝试 <strong>jpeg4py</strong></li><li>存 <strong>bmp</strong> 图（降低解码时间）</li></ul></li><li><p><strong>小图拼起来存放（降低读取次数）：对于大规模的小文件读取，建议转成单独的文件，可以选择的格式可以考虑</strong>：TFRecord（Tensorflow）、recordIO(recordIO)、hdf5、 pth、n5、lmdb 等等（<a href="https://github.com/Lyken17/Efficient-PyTorch#data-loader）">https://github.com/Lyken17/Efficient-PyTorch#data-loader）</a></p></li><li><ul><li><strong>TFRecord</strong>：<a href="https://github.com/vahidk/tfrecord">https://github.com/vahidk/tfrecord</a></li><li>借助 <strong>lmdb 数据库格式</strong>：</li></ul></li><li><ul><li><ul><li><a href="https://github.com/Fangyh09/Image2LMDB">https://github.com/Fangyh09/Image2LMDB</a></li><li><a href="https://blog.csdn.net/P_LarT/article/details/103208405">https://blog.csdn.net/P_LarT/article/details/103208405</a></li><li><a href="https://github.com/lartpang/PySODToolBox/blob/master/ForBigDataset/ImageFolder2LMDB.py">https://github.com/lartpang/PySODToolBox/blob/master/ForBigDataset/ImageFolder2LMDB.py</a></li><li><a href="https://github.com/Lyken17/Efficient-PyTorch">https://github.com/Lyken17/Efficient-PyTorch</a></li></ul></li></ul></li></ul><h5 id="3-借助硬件"><a href="#3-借助硬件" class="headerlink" title="(3)　借助硬件"></a>(3)　借助硬件</h5><ul><li>借助内存：<strong>直接载到内存里面，或者把把内存映射成磁盘好了</strong></li><li>借助固态：把读取速度慢的机械硬盘换成 <strong>NVME 固态</strong>吧～</li></ul><h5 id="4-训练策略"><a href="#4-训练策略" class="headerlink" title="(4) 训练策略"></a>(4) 训练策略</h5><ul><li><p>在训练中使用<strong>低精度（FP16 甚至 INT8 、二值网络、三值网络）表示取代原有精度（FP32）表示</strong></p></li><li><ul><li>NVIDIA/Apex：</li></ul></li><li><ul><li><ul><li><a href="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/100135729">https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/100135729</a></li><li><a href="https://github.com/nvidia/apex">https://github.com/nvidia/apex</a></li></ul></li></ul></li><li><p>使用分布式训练　DDP 或者 horovod</p></li></ul><h5 id="5-代码层面"><a href="#5-代码层面" class="headerlink" title="(5) 代码层面"></a>(5) 代码层面</h5><ul><li><code>torch.backends.cudnn.benchmark = True</code></li><li>Do numpy-like operations on the GPU wherever you can</li><li>Free up memory using<code>del</code>     用<code>del</code>及时删除不用的中间变量，节约GPU存储。</li><li>Avoid unnecessary transfer of data from the GPU</li><li>Use pinned memory, and use non_blocking=False to parallelize data transfer and GPU number crunching</li></ul><h3 id="2-model"><a href="#2-model" class="headerlink" title="2. model"></a>2. model</h3><ol><li><p>用<strong>float16</strong>代替默认的float32运算（<a href="https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers/blob/dad3c7a485b7ffc6fd2766f349e6ee845ecc2eee/examples/run_classifier.py">方法参考</a>，搜索”fp16”可以看到需要修改之处，包括model、optimizer、backward、learning rate）</p></li><li><p><strong>优化器</strong>以及对应参数的选择，如learning rate，不过它对性能的影响似乎更重要【占坑】</p></li><li><p>少用循环，多用<strong>向量化</strong>操作</p></li><li><p>经典操作尽量用别人优化好的<strong>库</strong>，别自己写</p></li><li><p>数据很多时少用append，虽然使用很方便，不过它每次都会重新分配空间？所以数据很大的话，光一次append就要几秒（测过），可以先分配好整个容器大小，每次用索引去修改内容，这样一步只要0.0x秒</p></li><li><p>固定对模型影响不大的部分参数，还能节约显存，可以用 detach() 切断反向传播，注意若仅仅给变量设置 required_grad=False 还是会计算梯度的</p></li><li><p>eval/test 的时候，加上 model.eval() 和 torch.no_grad()，前者固定 batch-normalization 和 dropout 但是会影响性能，后者关闭 autograd</p></li><li><p>提高程序<strong>并行度</strong>，例如 我想 train 时对每个 epoch 都能 test 一下以追踪模型性能变化，但是 test 时间成本太高要一个小时，所以写了个 socket，设一个127.0.0.1 的端口，每次 train 完一个 epoch 就发个UDP过去，那个进程就可以自己 test，同时原进程可以继续 train 下一个 epoch（对 这是自己想的诡异方法hhh）</p></li><li><p>torch.backends.cudnn.benchmark设为True，可以让cudnn根据当前训练各项config寻找优化算法，但这本身需要时间，所以input size在训练时会频繁变化的话，建议设为False</p></li><li><p>使用<code>inplace</code>操作可节约 GPU 存储，如</p><pre><code class="hljs ini"><span class="hljs-attr">x</span> = torch.nn.functional.relu(x, inplace=<span class="hljs-literal">True</span>)</code></pre></li><li><p>减少CPU和GPU之间的数据传输。例如， 如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</p></li><li><p>使用半精度浮点数<code>half()</code>会有一定的速度提升，具体效率依赖于GPU型号。需要小心数值精度过低带来的稳定性问题。时常使用 <code>assert tensor.size() == (N, D, H, W)</code>作为调试手段，确保张量维度和你设想中一致。</p></li><li><p>除了标记 y 外，尽量少使用一维张量，使用n*1的二维张量代替，可以避免一些意想不到的一维张量计算结果。</p></li><li><p>统计代码各部分耗时</p></li></ol><pre><code class="hljs python"><span class="hljs-keyword">with</span> torch.autograd.profiler.profile(enabled=<span class="hljs-literal">True</span>, use_cuda=<span class="hljs-literal">False</span>) <span class="hljs-keyword">as</span> profile:    ...    print(profile)</code></pre><p>或者在命令行运行：</p><pre><code class="hljs css"><span class="hljs-selector-tag">python</span> <span class="hljs-selector-tag">-m</span> <span class="hljs-selector-tag">torch</span><span class="hljs-selector-class">.utils</span><span class="hljs-selector-class">.bottleneck</span> <span class="hljs-selector-tag">main</span><span class="hljs-selector-class">.py</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>pytorch项目搭建</title>
    <link href="/2019/05/17/pytorch%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA/"/>
    <url>/2019/05/17/pytorch%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<p>pytorch 搭建项目的基本流程</p><a id="more"></a><h3 id="基本工作流程"><a href="#基本工作流程" class="headerlink" title="基本工作流程"></a>基本工作流程</h3><ol><li><p>相关工作调研:  <strong>评价指标、数据集、经典解决方案、待解决问题和已有方案的不同、精度和速度预估、相关难点 ! </strong></p></li><li><p>数据探索和方案确定</p></li><li>依次编写模型 models.py、数据集读取接口 datasets.py 、损失函数 losses.py 、评价指标 criterion.py</li><li>编写训练脚本(train.py)和测试脚本(test.py)</li><li>训练、调试和测评</li><li>模型的部署</li></ol><p>注意，不要将所有层和模型放在同一个文件中。最佳做法是将最终网络分离为单独的文件（networks.py），并将层、损耗和 ops 保存在各自的文件（layers.py、losses.py、ops.py）中。完成的模型（由一个或多个网络组成）应在一个文件中引用，文件名为 yolov3.py、dcgan.py 这样。</p><h5 id="1-构建神经网络"><a href="#1-构建神经网络" class="headerlink" title="(1) 构建神经网络"></a>(1) 构建神经网络</h5><p>​    自定义的网络继承自一般继承自　<code>nn.Module</code> 类，　必须有一个 <code>forward</code> 方法来实现各个层或操作的 forward 传递，　</p><p>对于具有<strong>单个输入</strong>和<strong>单个输出</strong>的简单网络，请使用以下模式：</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ConvBlock</span>(<span class="hljs-params">nn.Module</span>):</span>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span>    super(ConvBlock, self).__init__()    self.block = nn.Squential(       nn.Conv2d(...),       nn.ReLU(),       nn.BatchNorm2d(...)    )     <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>    <span class="hljs-keyword">return</span> self.block(x)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SimpleNetwork</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, num_of_layers = <span class="hljs-number">15</span></span>):</span>        super(SimpleNetwork, self).__init__()        layers = list()        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_of_layers):            layers.append(..)        self.conv0 = nn.Sequential(*layers)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        out = self.conv0(x)        <span class="hljs-keyword">return</span> out</code></pre><p>我们建议将网络拆分为更小的<strong>可重用部分</strong>。网络由操作或其它网络模块组成。损失函数也是神经网络的模块，因此可以直接集成到网络中。</p><h5 id="2-自定义数据集"><a href="#2-自定义数据集" class="headerlink" title="(2) 自定义数据集"></a>(2) 自定义数据集</h5><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CustomDataset</span>(<span class="hljs-params">Dataset</span>):</span>    <span class="hljs-string">&quot;&quot;&quot; CustomDataset. &quot;&quot;&quot;</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, root_dir=<span class="hljs-string">&#x27;./data&#x27;</span>, transform=None</span>):</span>        <span class="hljs-string">&quot;&quot;&quot;</span><span class="hljs-string">        &quot;&quot;&quot;</span>        self.root_dir = root_dir        self.transform = transform        self.train_data = ...        self.train_target = ...    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span>        <span class="hljs-keyword">return</span> len(self.train_data)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span>(<span class="hljs-params">self, idx</span>):</span>        <span class="hljs-keyword">if</span> torch.is_tensor(idx):            idx = idx.tolist()        data = Image.open(self.train_data[idx])        target = Image.open(self.train_target[idx])        <span class="hljs-keyword">if</span> self.transform:            data, target = self.transform(data, target)        sample = &#123;<span class="hljs-string">&#x27;data&#x27;</span>: data, <span class="hljs-string">&#x27;high_img&#x27;</span>: target&#125;        <span class="hljs-keyword">return</span> sample</code></pre><h5 id="3-自定义损失"><a href="#3-自定义损失" class="headerlink" title="(3) 自定义损失"></a>(3) 自定义损失</h5><p>​        虽然 PyTorch 已经有很多标准的损失函数，但有时也可能需要创建自己的损失函数。为此，请创建单独的文件 <code>losses.py</code> 并扩展 <code>nn.module</code> 类以创建自定义的损失函数：</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CustomLoss</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span>        <span class="hljs-string">&quot;&quot;&quot; CustomLoss&quot;&quot;&quot;</span>        super(CustomLoss, self).__init__()    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x, y</span>):</span>        <span class="hljs-keyword">return</span> torch.mean(torch.square(x  - y))</code></pre><h5 id="4-推荐可以参考的用于训练模型的代码结构"><a href="#4-推荐可以参考的用于训练模型的代码结构" class="headerlink" title="(4) 推荐可以参考的用于训练模型的代码结构"></a>(4) 推荐可以参考的用于训练模型的代码结构</h5><pre><code class="hljs python"><span class="hljs-comment"># import statements</span><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> data...<span class="hljs-comment"># set flags / seeds</span>torch.backends.cudnn.benchmark = <span class="hljs-literal">True</span>np.random.seed(<span class="hljs-number">1</span>)torch.manual_seed(<span class="hljs-number">1</span>)torch.cuda.manual_seed(<span class="hljs-number">1</span>)...  <span class="hljs-comment"># dataset</span>transform_train = ...trainform_text = ...train_dataset = CustomDataset(args.train_dataset, is_trainval = <span class="hljs-literal">True</span>, transform = transform_train) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size,                                           shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>, drop_last=<span class="hljs-literal">False</span>) valid_dataset = CustomDataset(args.valid_dataset, is_trainval = <span class="hljs-literal">True</span>, transform = transform_test)  valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=args.val_batch_size,                                            shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>) <span class="hljs-comment"># model &amp; loss</span>net = CustomNet().to(device) criterion = ...  <span class="hljs-comment"># lr &amp; optimizer</span>optim = optim.SGD(model.parameters(), lr=args.init_lr, momentum=args.momentum, weight_decay=args.weight_decay)scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class="hljs-number">50</span>, <span class="hljs-number">70</span>], gamma=<span class="hljs-number">0.1</span>)<span class="hljs-comment"># load resume</span><span class="hljs-keyword">if</span> args.resume:    <span class="hljs-keyword">if</span> os.path.isfile(args.resume):        print(<span class="hljs-string">&quot;=&gt; loading checkpoint &#x27;&#123;&#125;&#x27;&quot;</span>.format(args.resume))        checkpoint = torch.load(args.resume)        args.start_epoch = checkpoint[<span class="hljs-string">&#x27;epoch&#x27;</span>]        best_prec = checkpoint[<span class="hljs-string">&#x27;best_prec&#x27;</span>]        model.load_state_dict(checkpoint[<span class="hljs-string">&#x27;state_dict&#x27;</span>])        optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;optimizer&#x27;</span>])        print(<span class="hljs-string">&quot;=&gt; loaded checkpoint &#x27;&#123;&#125;&#x27; (epoch &#123;&#125;) Prec: &#123;:f&#125;&quot;</span>              .format(args.resume, checkpoint[<span class="hljs-string">&#x27;epoch&#x27;</span>], best_prec1))    <span class="hljs-keyword">else</span>:        print(<span class="hljs-string">&quot;=&gt; no checkpoint found at &#x27;&#123;&#125;&#x27;&quot;</span>.format(args.resume))<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">epoch</span>):</span>    model.train()　<span class="hljs-comment"># 在　model(x)　前需要添加　model.eval()　或者　model.eval()</span>    avg_loss = <span class="hljs-number">0.0</span>    train_acc = <span class="hljs-number">0.0</span>    <span class="hljs-keyword">for</span> batch_idx, batchdata <span class="hljs-keyword">in</span> enumerate(train_loader):        data, target = batchdata[<span class="hljs-string">&quot;data&quot;</span>], batchdata[<span class="hljs-string">&quot;target&quot;</span>] <span class="hljs-comment">#</span>        data, target = data.to(device), target.to(device)  <span class="hljs-comment">#</span>        <span class="hljs-comment"># 在 loss.backward()　前用　optimizer.zero_grad()　清除累积梯度</span>        optimizer.zero_grad() <span class="hljs-comment"># optimizer.zero_grad　与　model.zero_grad效果一样</span>        predict = model(data) <span class="hljs-comment"># </span>        loss = criterion(predict, target) <span class="hljs-comment">#</span>        avg_loss += loss.item() <span class="hljs-comment">#</span>        loss.backward()        optimizer.step()        print(<span class="hljs-string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.1f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.format(                epoch, batch_idx * len(data), len(train_loader.dataset),                <span class="hljs-number">100.</span> * batch_idx / len(train_loader), loss.item()))    <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) %  args.save_interval == <span class="hljs-number">0</span>:        state = &#123; <span class="hljs-string">&#x27;epoch&#x27;</span>: epoch + <span class="hljs-number">1</span>,                   <span class="hljs-string">&#x27;state_dict&#x27;</span>: model.state_dict(),                   <span class="hljs-string">&#x27;best_prec&#x27;</span>: <span class="hljs-number">0.0</span>,                   <span class="hljs-string">&#x27;optimizer&#x27;</span>: optimizer.state_dict()&#125;        model_path = os.path.join(args.checkpoint_dir, <span class="hljs-string">&#x27;model_&#x27;</span> + str(epoch) + <span class="hljs-string">&#x27;.pth&#x27;</span>)        torch.save(state, model_path)<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span>():</span>    model.eval()    test_loss = <span class="hljs-number">0</span>    <span class="hljs-keyword">for</span> batch_idx, batchdata <span class="hljs-keyword">in</span> enumerate(valid_loader):        data, target = batchdata[<span class="hljs-string">&quot;data&quot;</span>], batchdata[<span class="hljs-string">&quot;target&quot;</span>] <span class="hljs-comment">#</span>        data, target = data.to(device), target.to(device) <span class="hljs-comment">#</span>        predict = model(data) <span class="hljs-comment"># </span>        test_loss += criterion(predict, target) <span class="hljs-comment">#</span>        psnr = criterion(predict * <span class="hljs-number">255</span>, target * <span class="hljs-number">255</span>) <span class="hljs-comment">#</span>    test_loss /= len(valid_loader.dataset)    print(<span class="hljs-string">&#x27;\nTest set: Average loss: &#123;:.4f&#125;, loss:&#123;&#125;, PSNR: (&#123;:.1f&#125;)\n&#x27;</span>.format(        test_loss, test_loss / len(valid_loader.dataset), psnr / len(valid_loader.dataset)))    <span class="hljs-keyword">return</span> psnr / float(len(valid_loader.dataset))best_prec = <span class="hljs-number">0.0</span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(args.start_epoch, args.epochs):    train(epoch)    scheduler.step()    print(print(optimizer.state_dict()[<span class="hljs-string">&#x27;param_groups&#x27;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;lr&#x27;</span>]))    current_prec = test()     is_best = current_prec &gt; best_prec <span class="hljs-comment">#　更改大小写 !</span>    best_prec = max(best_prec, best_prec) <span class="hljs-comment">#  max or min</span>    save_checkpoint(&#123;        <span class="hljs-string">&#x27;epoch&#x27;</span>: epoch + <span class="hljs-number">1</span>,        <span class="hljs-string">&#x27;state_dict&#x27;</span>: model.state_dict(),        <span class="hljs-string">&#x27;best_prec&#x27;</span>: best_prec,        <span class="hljs-string">&#x27;optimizer&#x27;</span>: optimizer.state_dict(),    &#125;, is_best, args.checkpoint_dir)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>面试相关整理</title>
    <link href="/2019/04/29/Interview-2019/"/>
    <url>/2019/04/29/Interview-2019/</url>
    
    <content type="html"><![CDATA[<p>面试一些相关的感受经历整理</p><a id="more"></a><p>​    投了大概有十家公司的样子，回复并过去面试的公司有五家。应该会在这几家中决定吧。面试挺耗费时间的，心也累，得准备好多东西。这五家公司做的东西都不大相同。两家做AI+教育，一家AI+安防，一家遥感图像处理，还有一家不明确主要业务。主要面试的岗位为<strong>计算机视觉/图像处理岗位</strong>。除了一家没有给出意向，其他公司面的都还算可以。其实并不喜欢AI、人工智能这些 Title， 感觉有点虚无缥缈，不着边际(可能融资比较容易吧)，感觉计算机视觉/机器视觉/图像处理更加贴地气，也知道方向和目标。</p><p>​    每个公司的主要流程不尽相同，面试的内容也不尽相同。（其实感觉应届生和有工作经验的人的面试内容还挺不一样的。前者基础比较重要，后者更看重项目经历）。</p><p>​    先说一下大体的面试流程：<strong>投简历</strong> -&gt; <strong>HR 发送相应的面试通知</strong>  -&gt; <strong>找到相应的面试地点</strong> -&gt; <strong>填写面试应聘表</strong>  —&gt; <strong>技术面试(一面/二面)</strong> -&gt; <strong>HR 面试</strong> -&gt; <strong>后续沟通以及offer事宜</strong></p><h4 id="1-一些基本的礼仪和细节问题："><a href="#1-一些基本的礼仪和细节问题：" class="headerlink" title="1. 一些基本的礼仪和细节问题："></a>1. 一些基本的礼仪和细节问题：</h4><p>（1）着装：技术性面试，也没必要正装，平常着装就可以。然后干净，利落就行。面试前可以洗个头吧，一是看着舒爽，另外醒醒盹🤣</p><p>（2）我面试的时候携带的东西：背包 + mac + 一只中性笔 + 几张A4白纸 + 简历（一些证书可带可不带）</p><p>（3）关于到达的时间：我一般提前30分钟到达面试地点，提前20分钟上楼。一是 预留 buffer 给堵车/找路等事情。二是填个表什么的。(这个看个人习惯，也不晓得对不对。)</p><p>（4）诚实， 不会的就说不会。都是搞技术的，也不会为难大家。另外像我有段考研的经历，我也都直接说出来了。不管他们怎么想。</p><h4 id="2-面试的体验："><a href="#2-面试的体验：" class="headerlink" title="2. 面试的体验："></a>2. 面试的体验：</h4><p>A 岗位：给了一道实际工作中应用场景写代码。然后问项目、问的挺细致的。【写出代码很重要， 这不仅决定你能不能接下来二面，甚至影响二面的印象】</p><p>B 岗位：问项目，挺细致的。 还问了一些基础的CPP和linux的内容。（该岗位面试体验很差，一方面有点为难人，我都说不会了，还一直问。另外一方面问了好多和岗位无关的知识）</p><p>C 岗位：做题，可能题目[linux基础+深度学习+Python/CPP]做的比较好。然后项目问了十分钟就差不多过了。【做出题目很关键】</p><p>D 岗位：问项目，之后过了三天打电话技术面（感觉问的挺粗,可能面试官不是做这个的，对我的项目也不是很清楚）。</p><p>E 岗位：直接问项目 + 上机写代码。【上机写代码很关键】</p><h4 id="3-面试的收获以及需要努力的地方："><a href="#3-面试的收获以及需要努力的地方：" class="headerlink" title="3. 面试的收获以及需要努力的地方："></a>3. 面试的收获以及需要努力的地方：</h4><p>（0）简历/自我介绍还是要准备好的， 感觉挺重要的。其实我的并不大好，毕竟gap了一年考研，还没考上(sad)</p><p>（1）写代码很关键。Python和C++都要熟悉，C++用的少了有点生疏了。导致E岗位的翻转列表都忘了怎么写了。(⊙﹏⊙)b 准备时间比较短，也没有去刷题，直接硬着头皮就上了。。</p><p>（2）项目要记得非常仔细，每一个细节，甚至之前的代码都要记住</p><p>（3）常见的面试范围：</p><ul><li>项目</li><li>(刷题)剑指 offer + leetcode + codewars(E面试官用的，哈哈，偷偷学来的)</li><li>机器学习 + 深度学习 基础</li><li>图像处理基础知识</li></ul><p>（4）有两三家公司对 高性能计算很感兴趣。我猜测是应为部署的时候DL太慢导致的。需要进行优化。其中有一家还问了caffe 中卷积的实现方案：im2col + 矩阵乘法。还好当年看看百度的MDL 时候去看了这个源码。</p><p>​     其实这方面也是我挺感兴趣的一个方面：常见并行方案有 MPI、OPENMP、SIMD、neon、CUDA —&gt; 需要好好学学 ncnn， 看看人家怎么写的DL interface。</p><p>（5）blog 和 github 还是要好好整整的，树立个人品牌很关键(Kaggle比赛、blog、github)，有几篇blog是想着好好学习一下的(心有余而力不足呀)：</p><ul><li><p>openmp</p></li><li><p>最近的anchor free的目标检测算法(膜一下 densebox)</p></li><li><p>ncnn 的优化方案</p></li></ul><p>（6） 有些面试官感觉的出来也不大懂你做的，因此项目中的共同的东西就要非常熟悉，比如：</p><p>​      网络中的backbone、一些衡量指标、loss设计、参数调整的过程、一些检测的方案（SSD/YOLO/R-CNN、anchor free or anchor based）要熟记。</p><p>（7）有些公司还是挺看重实践能力的，比如问了 CMakeLists 怎么写？ ncnn中怎么定义层？ 怎么优化代码？最后怎么提交成绩的(在共有数据集上，比如lfw上)？</p><h4 id="4-写一下其他的吧-吐槽-："><a href="#4-写一下其他的吧-吐槽-：" class="headerlink" title="4. 写一下其他的吧(吐槽)："></a>4. 写一下其他的吧(吐槽)：</h4><p>（1）有些面试通知中都没有写明地点? 那栋楼? 怎么走? —&gt; [某公司去到没找到(ABCDE栋/楼层都没写)，打了三遍电话才接通]</p><p>（2）有一家公司说了两个多小时，一口水都没给喝(我那个渴呀，以后可以自己带一瓶水吧)</p><p>（3）有家公司扔了一张试卷就走了，我半个小时就做完了，一个半小时之后他才回来(其实也不是他，另一个面试官面的)，期间出去找面试官两次没找到，不是发现该公司同事在玩手机，就是在走廊聊天， 甚者还在我做题的那个屋里办理银行卡，我真是见识了。对该公司印象极其差。</p><p>​    【有一家创业公司给我印象挺好，早上还给我发了面试提醒，HR还下楼去接的我，虽然没碰到】</p><p>（4）面试时不要害怕(面多了就好了，现在面试心情很轻松的)，进公司之后就是同事或者leader，面试官也一般不会为难面试者的。有一家公司面试官在我做题期间很细心的指导别人，给我留下很深刻的印象。</p><p>（5）面试也是一个双选的过程，感觉不好的公司尽量也不要去，免得最后不欢而散、</p><p>（6）也不要害怕谈薪资，按自己能力要即可。好吧，这方面我一直不在行。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>YOLO</title>
    <link href="/2019/04/23/YOLO/"/>
    <url>/2019/04/23/YOLO/</url>
    
    <content type="html"><![CDATA[<p>YOLO series: YOLO v1. YOLO v2 and. YOLO v3</p><a id="more"></a><h3 id="1-YOLO-v1"><a href="#1-YOLO-v1" class="headerlink" title="1. YOLO v1"></a>1. YOLO v1</h3><h4 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h4><p>直接在输出层回归 bounding box 的位置和 bounding box所属的类别(整张图作为网络的输入，把 Object Detection 的问题转化成一个 Regression 问题)</p><h4 id="YOLO-v1-流程"><a href="#YOLO-v1-流程" class="headerlink" title="YOLO v1 流程"></a>YOLO v1 流程</h4><p><img src="/2019/04/23/YOLO/2.png" alt></p><ol><li><p>将图片Resize成448x448，分成7x7个网格(grid cell)，某个物体的中心落在这个网格中此网格就负责预测这个物体。</p></li><li><p>提取特征和预测，卷积部分负责提取特征。全连接部分负责预测：</p><ul><li>两个 【bounding box(bbox) + conﬁdence】 (2x5)</li><li>20个物体的概率（20）</li></ul><p>总的输出为 (7X7)X30 的维度。</p></li><li><p>过滤bbox（通过nms）</p></li></ol><h4 id="YOLO-v2-细节"><a href="#YOLO-v2-细节" class="headerlink" title="YOLO v2 细节"></a>YOLO v2 细节</h4><ol><li><p>class-specific confidence score：</p><p>class-specific confidence score 是三项的成绩。第一项是 bbox每个网格预测的类别信息即20个物体的概率， 第二项是每个 bbox 的置信度， 第三项是 ground truth 和 预测框的 IOU。得到每个bbox的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果</p></li><li><p>损失函数的设计：<br><img src="/2019/04/23/YOLO/1.png" alt></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>分类、检测和分割</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>recent convolutions</title>
    <link href="/2019/04/21/recent-convolutions/"/>
    <url>/2019/04/21/recent-convolutions/</url>
    
    <content type="html"><![CDATA[<p>octave conv(octconv)、hetconv、res2net</p><a id="more"></a><h3 id="1-octave-conv"><a href="#1-octave-conv" class="headerlink" title="1. octave conv"></a>1. octave conv</h3><h5 id="Paper"><a href="#Paper" class="headerlink" title="Paper:"></a>Paper:</h5><p>Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution</p><h5 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation:"></a>Motivation:</h5><p>如下图所示：（a）自然图像可以被分解为高频部分和低频部分。高频分量表达细节，低频分量表达整体。很显然，低频分量是存在冗余的，在编码过程中可以节省。(b) 卷积层的输出通道也是如此，可以被分解为低频分量和高频分量并进行重组。（c）作者将低频分量的通道大小设置为高频分量通道大小的一半，用来减少冗余。(d) 低频部分和高频部分可以各自更新，并进行通道之间的交流。</p><p><img src="/2019/04/21/recent-convolutions/1.png" alt></p><h5 id="mothods"><a href="#mothods" class="headerlink" title="mothods"></a>mothods</h5><p>作者提出了 conv 的改进版 OctConv 用来降低低频部分的冗余度。 Octconv 的结构如下所示：</p><p><img src="/2019/04/21/recent-convolutions/2.png" alt></p><p>当在第一个 OctConv 是 $\alpha_{in} = 0$，此时执行 ①② 两个操作。</p><pre><code class="hljs python"><span class="hljs-keyword">if</span> self.type == <span class="hljs-string">&#x27;first&#x27;</span>:    <span class="hljs-keyword">return</span> self.H2H(x), self.H2L(self.avg_pool(x))<span class="hljs-comment"># H2H、L2H 均为传统卷积</span></code></pre><p>当在最后一个 OctConv 中 $\alpha_{out} = 0$， 此时执行 ①③ 两个操作。</p><pre><code class="hljs python"><span class="hljs-keyword">if</span> self.type == <span class="hljs-string">&#x27;last&#x27;</span>:    hf, lf = x    <span class="hljs-keyword">return</span> self.H2H(hf) + self.L2H(self.upsample(lf))<span class="hljs-comment"># H2H、L2H 均为传统卷积</span></code></pre><p>当其他情况时，执行 ①②③④四个操作</p><pre><code class="hljs python"><span class="hljs-keyword">else</span>：    hf, lf = x    <span class="hljs-keyword">return</span> self.H2H(hf) + self.upsample(self.L2H(lf)),            self.L2L(lf) + self.H2L(self.avg_pool(hf))<span class="hljs-comment"># L2H、L2L、H2H、H2L 均为传统卷积</span></code></pre><p>octconv 整体的实现代码如下所示：</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">OctConv</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, kernel_size, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, </span></span><span class="hljs-function"><span class="hljs-params">             alpha_in=<span class="hljs-number">0.25</span>, alpha_out=<span class="hljs-number">0.25</span>, type=<span class="hljs-string">&#x27;normal&#x27;</span></span>):</span>        super(OctConv, self).__init__()        self.kernel_size = kernel_size        self.stride = stride        self.type = type        hf_in = int(in_channels * (<span class="hljs-number">1</span> - alpha_in))        hf_out = int(out_channels * (<span class="hljs-number">1</span> - alpha_out))        lf_in = in_channels - hf_in        lf_out = out_channels - hf_out        <span class="hljs-keyword">if</span> stride == <span class="hljs-number">2</span>:            self.downsample = nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=stride)        <span class="hljs-keyword">if</span> type == <span class="hljs-string">&#x27;first&#x27;</span>:            self.H2H = nn.Conv2d(in_channels, hf_out, kernel_size=kernel_size, stride=<span class="hljs-number">1</span>, padding=padding)            self.H2L = nn.Conv2d(in_channels, lf_out, kernel_size=kernel_size, stride=<span class="hljs-number">1</span>, padding=padding)            self.avg_pool = nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)        <span class="hljs-keyword">elif</span> type == <span class="hljs-string">&#x27;last&#x27;</span>:            self.H2H = nn.Conv2d(hf_in, out_channels, kernel_size=kernel_size, padding=padding)            self.L2H = nn.Conv2d(lf_in, out_channels, kernel_size=kernel_size, padding=padding)            self.upsample = partial(F.interpolate, scale_factor=<span class="hljs-number">2</span>, mode=<span class="hljs-string">&quot;nearest&quot;</span>)        <span class="hljs-keyword">else</span>:            self.L2L = nn.Conv2d(lf_in, lf_out, kernel_size=kernel_size, stride=<span class="hljs-number">1</span>, padding=padding)            self.L2H = nn.Conv2d(lf_in, hf_out, kernel_size=kernel_size, stride=<span class="hljs-number">1</span>, padding=padding)            self.H2L = nn.Conv2d(hf_in, lf_out, kernel_size=kernel_size, stride=<span class="hljs-number">1</span>, padding=padding)            self.H2H = nn.Conv2d(hf_in, hf_out, kernel_size=kernel_size, stride=<span class="hljs-number">1</span>, padding=padding)            self.upsample = partial(F.interpolate, scale_factor=<span class="hljs-number">2</span>, mode=<span class="hljs-string">&quot;nearest&quot;</span>)            self.avg_pool = partial(F.avg_pool2d, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        <span class="hljs-keyword">if</span> self.type == <span class="hljs-string">&#x27;first&#x27;</span>:            <span class="hljs-keyword">if</span> self.stride == <span class="hljs-number">2</span>:                x = self.downsample(x)            <span class="hljs-keyword">return</span> self.H2H(x), self.H2L(self.avg_pool(x))        <span class="hljs-keyword">elif</span> self.type == <span class="hljs-string">&#x27;last&#x27;</span>:            hf, lf = x            <span class="hljs-keyword">if</span> self.stride == <span class="hljs-number">2</span>:                hf = self.downsample(hf)                <span class="hljs-keyword">return</span> self.H2H(hf) + self.L2H(lf)            <span class="hljs-keyword">else</span>:                <span class="hljs-keyword">return</span> self.H2H(hf) + self.L2H(self.upsample(lf))        <span class="hljs-keyword">else</span>:            hf, lf = x            <span class="hljs-keyword">if</span> self.stride == <span class="hljs-number">2</span>:                hf = self.downsample(hf)                <span class="hljs-keyword">return</span> self.H2H(hf) + self.L2H(lf), self.L2L(self.avg_pool(lf)) + self.H2L(self.avg_pool(hf))            <span class="hljs-keyword">else</span>:                <span class="hljs-keyword">return</span> self.H2H(hf) + self.upsample(self.L2H(lf)), self.L2L(lf) + self.H2L(self.avg_pool(hf))</code></pre><h3 id="2-res2net"><a href="#2-res2net" class="headerlink" title="2. res2net"></a>2. res2net</h3><h5 id="Paper-1"><a href="#Paper-1" class="headerlink" title="Paper:"></a>Paper:</h5><p>Res2Net: A New Multi-scale Backbone Architecture</p><h5 id="Methods"><a href="#Methods" class="headerlink" title="Methods:"></a>Methods:</h5><p>作者提出了一种在更加细粒度(卷积层)的层面提升多尺度表达能力。其基本结构如下图(b) 所示：</p><p><img src="/2019/04/21/recent-convolutions/3.png" alt></p><p>传统的resnet结构如上图所示，作者在其基础上进行改进，在不增加计算量的同时，使其具备更强的多尺度提取能力。如上图（b）所示，作者采用了更小的卷积组来替代 bottleneck block 里面的 3x3 卷积。具体操作为：</p><ul><li>首先将 1x1 卷积后的特征图均分为 s 个特征图子集。每个特征图子集的大小相同，但是通道数是输入特征图的 1/s。</li><li>对每一个特征图子集 $X<em>i$，有一个对应的 3x3 卷积 $K_i$ , 假设 $K_i$的输出是 $y_i$。接下来每个特征图子集 $X_i $会加上 $K</em>{i-1}$ 的输出，然后一起输入进 $K_i$。为了在增大 s 的值时减少参数量，作者省去了 $X_1$ 的 3x3 网络。因此，输出 $y_i$ 可以用如下公式表示：</li></ul><script type="math/tex; mode=display">y_i = \begin{equation}\begin{cases}x_i && i=1;\\K_i(x_i+y_{i-1}) && 1 < i \leq s.\end{cases}\end{equation}</script><p>根据图(b)，可以发现每一个 $X_j (j&lt;=i)$ 下的 3x3 卷积可以利用之前所有的特性信息，它的输出会有比 $X_j$ 更大的感受野。因此这样的组合可以使 Res2Net 的输出有更多样的感受野信息。为了更好的融合不同尺度的信息，作者将它们的输出拼接起来，然后再送入 1x1 卷积，如上图（b）所示。</p><p>res2Net module 的实现代码如下所示：</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Res2NetBottleneck</span>(<span class="hljs-params">nn.Module</span>):</span>   expansion = <span class="hljs-number">4</span>   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, downsample=None, stride=<span class="hljs-number">1</span>, scales=<span class="hljs-number">4</span>, groups=<span class="hljs-number">1</span>, se=False, norm_layer=None</span>):</span>       super(Res2NetBottleneck, self).__init__()       <span class="hljs-keyword">if</span> out_channels % scales != <span class="hljs-number">0</span>:           <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Planes must be divisible by scales&#x27;</span>)       <span class="hljs-keyword">if</span> norm_layer <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:           norm_layer = nn.BatchNorm2d                  <span class="hljs-comment"># Both self.conv2 and self.downsample layers downsample the input when stride != 1</span>       self.conv1 = conv1x1(in_channels, out_channels, stride)       self.bn1 = norm_layer(out_channels)       self.conv2 = nn.ModuleList([conv3x3(out_channels // scales, out_channels // scales, groups=groups) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(scales<span class="hljs-number">-1</span>)])       self.bn2 = nn.ModuleList([norm_layer(out_channels // scales) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(scales<span class="hljs-number">-1</span>)])       self.conv3 = conv1x1(out_channels, out_channels * self.expansion)       self.bn3 = norm_layer(out_channels * self.expansion)       self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)       self.downsample = downsample       self.stride = stride       self.scales = scales   <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>       identity = x              out = self.relu(self.bn1(self.conv1(x)))       xs = torch.chunk(out, self.scales, <span class="hljs-number">1</span>)       ys = []       <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> range(self.scales):           <span class="hljs-keyword">if</span> s == <span class="hljs-number">0</span>:               ys.append(xs[s])           <span class="hljs-keyword">elif</span> s == <span class="hljs-number">1</span>:               ys.append(self.relu(self.bn2[s<span class="hljs-number">-1</span>](self.conv2[s<span class="hljs-number">-1</span>](xs[s]))))           <span class="hljs-keyword">else</span>:               ys.append(self.relu(self.bn2[s<span class="hljs-number">-1</span>](self.conv2[s<span class="hljs-number">-1</span>](xs[s] + ys[<span class="hljs-number">-1</span>]))))       out = torch.cat(ys, <span class="hljs-number">1</span>)       out = self.bn3(self.conv3(out))              <span class="hljs-keyword">if</span> self.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:           identity = self.downsample(identity)       <span class="hljs-keyword">return</span> self.relu(out + identity)</code></pre><h3 id="3-Hetconv"><a href="#3-Hetconv" class="headerlink" title="3. Hetconv"></a>3. Hetconv</h3><h5 id="Paper："><a href="#Paper：" class="headerlink" title="Paper："></a>Paper：</h5><p>HetConv: Heterogeneous Kernel-Based Convolutions for Deep CNNs</p><h5 id="Methods-1"><a href="#Methods-1" class="headerlink" title="Methods:"></a>Methods:</h5><p>本文提出了一种高效的异构卷积过滤器(一些核的大小是 3x3， 其余的是1x1)，相较于 mobilenet的原始的 depthwise conv，能在不牺牲准确度的同时提升这些架构的效率。实现的方案很简单， 将传统卷积进行改进，对于某一个卷积，只有1/p个通道 使用 3x3 的卷积，其余均使用 1x1卷积，然后将所有通道相加，作为一个输出通道。</p><p>Hexconv module 的实现代码如下所示</p><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HetConv</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, p=<span class="hljs-number">2</span></span>):</span>        super(HetConv, self).__init__()        <span class="hljs-keyword">if</span> in_channels % groups != <span class="hljs-number">0</span>:            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;in_channels must be divisible by groups&#x27;</span>)        self.in_channels = in_channels        self.out_channels = out_channels        self.blocks = nn.ModuleList()        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(out_channels):            self.blocks.append(self._make_hetconv_layer(i, p))    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_make_hetconv_layer</span>(<span class="hljs-params">self, n, p</span>):</span>        layers = nn.ModuleList()        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.in_channels):            <span class="hljs-keyword">if</span> ((i - n) % (p)) == <span class="hljs-number">0</span>:                layers.append(nn.Conv2d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">1</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>))            <span class="hljs-keyword">else</span>:                layers.append(nn.Conv2d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">1</span>, kernel_size=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>))        <span class="hljs-keyword">return</span> layers    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>        out = []        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, self.out_channels):            out_ = self.blocks[i][<span class="hljs-number">0</span>](x[:, <span class="hljs-number">0</span>: <span class="hljs-number">1</span>, :, :])            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, self.in_channels):               out_ += self.blocks[i][j](x[:, j:j + <span class="hljs-number">1</span>, :, :])            out.append(out_)        <span class="hljs-keyword">return</span> torch.cat(out, <span class="hljs-number">1</span>)</code></pre><h3 id="三种卷积的构造方式总结："><a href="#三种卷积的构造方式总结：" class="headerlink" title="三种卷积的构造方式总结："></a>三种卷积的构造方式总结：</h3><p><img src="/2019/04/21/recent-convolutions/4.png" alt></p><ol><li>三种卷积均在输入通道进行改进，前两种方案(res2net、octconv)都进行了特征通道的融合。但是第一种方案对硬件并不友好(没有进行试验的验证)。第三种方案可以看做是传统卷积的改善，将其中的某些通道设置为3x3，其他通道设置为1x1卷积。</li><li>常见的多尺度的获取方式：<ul><li>细粒度卷积(res2net方式)</li><li>NIN(例如 Inception 样式的卷积也可以)</li><li>多尺度图像输出(图像金字塔)</li><li>特征多尺度(融合多个尺度的特征图：FPN网络)</li><li>空间金字塔池化(Spatial Pyramid Pooling)</li></ul></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>receptive_size_and_anchor_design</title>
    <link href="/2019/04/21/receptive-size-and-anchor-design/"/>
    <url>/2019/04/21/receptive-size-and-anchor-design/</url>
    
    <content type="html"><![CDATA[<p>octave conv(octconv)、hetconv、res2net</p><a id="more"></a><h3 id="1-感受野的计算："><a href="#1-感受野的计算：" class="headerlink" title="1. 感受野的计算："></a>1. 感受野的计算：</h3><pre><code class="hljs python"><span class="hljs-comment"># [kernel_size, stride, padding]</span>convnet =   [[<span class="hljs-number">11</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>], [<span class="hljs-number">5</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>]]layer_names = [<span class="hljs-string">&#x27;conv1&#x27;</span>,<span class="hljs-string">&#x27;pool1&#x27;</span>,<span class="hljs-string">&#x27;conv2&#x27;</span>,<span class="hljs-string">&#x27;pool2&#x27;</span>,<span class="hljs-string">&#x27;conv3&#x27;</span>,<span class="hljs-string">&#x27;conv4&#x27;</span>,<span class="hljs-string">&#x27;conv5&#x27;</span>,<span class="hljs-string">&#x27;pool5&#x27;</span>]input image: receptive size: <span class="hljs-number">1</span> conv1: receptive size: <span class="hljs-number">11</span>  pool1: receptive size: <span class="hljs-number">19</span>  conv2: receptive size: <span class="hljs-number">51</span>pool2: receptive size: <span class="hljs-number">67</span>conv3: receptive size: <span class="hljs-number">99</span>conv4: receptive size: <span class="hljs-number">131</span>conv5: receptive size: <span class="hljs-number">163</span>pool5: receptive size: <span class="hljs-number">195</span></code></pre><h5 id="1-反向推导："><a href="#1-反向推导：" class="headerlink" title="(1) 反向推导："></a>(1) <strong>反向推导</strong>：</h5><ul><li>初始(最后) $feature map$ 层的感受野是1。</li><li>每经过一层 kernel<em>size为 k, 步长为 s的卷积/池化层，感受野 $r</em>{l} = r \times s + (k-s) = (r_{l+1} -1) \times s + k$ 。</li><li>经过多分支的路径，按照感受野最大支路计算。</li><li>不会改变感受野的情况: conv1x1 s1、ReLU、BN、dropout、shotcut等元素级操作。</li><li>经过FC层和Gobal Ave Pooling 层，感受野就是整个输入图像。</li></ul><pre><code class="hljs angelscript">以 pool5 为例：Pool5: (<span class="hljs-number">1</span><span class="hljs-number">-1</span>)*<span class="hljs-number">2</span> + <span class="hljs-number">3</span> = <span class="hljs-number">3</span>Conv5: (<span class="hljs-number">3</span><span class="hljs-number">-1</span>)*<span class="hljs-number">1</span> + <span class="hljs-number">3</span> = <span class="hljs-number">5</span>Conv4: (<span class="hljs-number">5</span><span class="hljs-number">-1</span>)*<span class="hljs-number">1</span> + <span class="hljs-number">3</span> =  <span class="hljs-number">7</span>Conv3:(<span class="hljs-number">7</span><span class="hljs-number">-1</span>)*<span class="hljs-number">1</span> + <span class="hljs-number">3</span> = <span class="hljs-number">9</span>pool2: (<span class="hljs-number">9</span><span class="hljs-number">-1</span>)*<span class="hljs-number">2</span> + <span class="hljs-number">3</span> = <span class="hljs-number">19</span>Conv2: (<span class="hljs-number">19</span><span class="hljs-number">-1</span>)*<span class="hljs-number">1</span> + <span class="hljs-number">5</span>  = <span class="hljs-number">23</span>Pool1： (<span class="hljs-number">23</span><span class="hljs-number">-1</span>)*<span class="hljs-number">2</span> + <span class="hljs-number">3</span> = <span class="hljs-number">47</span>Conv1: (<span class="hljs-number">47</span><span class="hljs-number">-1</span>)*<span class="hljs-number">4</span> + <span class="hljs-number">11</span> = <span class="hljs-number">195</span></code></pre><h5 id="2-正向推导"><a href="#2-正向推导" class="headerlink" title="(2) 正向推导"></a>(2) 正向推导</h5><p>和反向推导相似， $s_0 = 1$,  $feature_map$ 的感受野为1。</p><script type="math/tex; mode=display">\begin{eqnarray}&& r_{l+1} = r_l + (k-1) * s_l \\&& s_{l+1} = s_{l} * s \\\end{eqnarray}</script><pre><code class="hljs python">以 pool5 为例：Conv1: <span class="hljs-number">1</span> + (<span class="hljs-number">11</span><span class="hljs-number">-1</span>)*<span class="hljs-number">1</span> =  <span class="hljs-number">11</span>, s1=<span class="hljs-number">1</span>*<span class="hljs-number">4</span>=<span class="hljs-number">4</span>   <span class="hljs-comment"># [11,4,0]</span>Pool1: <span class="hljs-number">11</span> + (<span class="hljs-number">3</span><span class="hljs-number">-1</span>)*<span class="hljs-number">4</span> = <span class="hljs-number">19</span>, s2=<span class="hljs-number">4</span>*<span class="hljs-number">2</span>=<span class="hljs-number">8</span>   <span class="hljs-comment"># [3,2,0]</span>Conv2: <span class="hljs-number">19</span> + (<span class="hljs-number">5</span><span class="hljs-number">-1</span>)*<span class="hljs-number">8</span> = <span class="hljs-number">51</span>, s3=<span class="hljs-number">8</span>*<span class="hljs-number">1</span>=<span class="hljs-number">8</span>  <span class="hljs-comment"># [5,1,2]</span>pool2: <span class="hljs-number">51</span> + (<span class="hljs-number">3</span><span class="hljs-number">-1</span>)*<span class="hljs-number">8</span> = <span class="hljs-number">67</span>, s4=<span class="hljs-number">8</span>*<span class="hljs-number">2</span>=<span class="hljs-number">16</span> <span class="hljs-comment"># [3,2,0]</span>Conv3: <span class="hljs-number">67</span> + (<span class="hljs-number">3</span><span class="hljs-number">-1</span>)*<span class="hljs-number">16</span> = <span class="hljs-number">99</span>, s5=<span class="hljs-number">16</span>*<span class="hljs-number">1</span>=<span class="hljs-number">16</span>  <span class="hljs-comment"># [3，1，1]</span>Conv4: <span class="hljs-number">99</span> + (<span class="hljs-number">3</span><span class="hljs-number">-1</span>)*<span class="hljs-number">16</span> = <span class="hljs-number">131</span>， s6=<span class="hljs-number">16</span>*<span class="hljs-number">1</span>=<span class="hljs-number">16</span> <span class="hljs-comment"># [3，1，1]</span>Conv5: <span class="hljs-number">131</span> + (<span class="hljs-number">3</span><span class="hljs-number">-1</span>)*<span class="hljs-number">16</span> = <span class="hljs-number">163</span>, s7=<span class="hljs-number">16</span>*<span class="hljs-number">1</span>=<span class="hljs-number">16</span> <span class="hljs-comment"># [3, 1, 1]</span>Pool5: <span class="hljs-number">163</span> + (<span class="hljs-number">3</span><span class="hljs-number">-1</span>)*<span class="hljs-number">16</span> = <span class="hljs-number">195</span>, s8=<span class="hljs-number">16</span>*<span class="hljs-number">2</span>=<span class="hljs-number">32</span> <span class="hljs-comment"># [3, 2, 0]</span></code></pre><p>[<a href="https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807">https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807</a>]</p><p>[<a href="https://fomoro.com/projects/project/receptive-field-calculator">https://fomoro.com/projects/project/receptive-field-calculator</a>]</p><h3 id="2-卷积的有效感受野"><a href="#2-卷积的有效感受野" class="headerlink" title="2. 卷积的有效感受野"></a>2. 卷积的有效感受野</h3><p>上文所述的是理论感受野，而特征的有效感受野（实际起作用的感受野）实际上是远小于理论感受野的，如下图所示。具体数学分析比较复杂，不再赘述，感兴趣的话可以参考论文 [Understanding the Effective Receptive Field in Deep Convolutional Neural Networks]。</p><p><img src="/2019/04/21/receptive-size-and-anchor-design/effective_receptive.jpg" alt="有效感受野示例"></p><p>​    下面我从直观上解释一下有效感受野背后的原因。以一个两层 <img src="https://www.zhihu.com/equation?tex=kernel%5C_size%3D3" alt="kernel\_size=3">， <img src="https://www.zhihu.com/equation?tex=stride%3D1" alt="stride=1">的网络为例，该网络的理论感受野为5，计算流程可以参加下图。其中 <img src="https://www.zhihu.com/equation?tex=x" alt="x"> 为输入， <img src="https://www.zhihu.com/equation?tex=w" alt="w"> 为卷积权重， <img src="https://www.zhihu.com/equation?tex=o" alt="o"> 为经过卷积后的输出特征。</p><p>​    很容易可以发现， <img src="https://www.zhihu.com/equation?tex=x_%7B1%2C1%7D" alt="x_{1,1}"> 只影响第一层feature map中的 <img src="https://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E1" alt="o_{1,1}^1"> ；而 <img src="https://www.zhihu.com/equation?tex=x_%7B3%2C3%7D" alt="x_{3,3}"> 会影响第一层feature map中的所有特征，即 <img src="https://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E1%2Co_%7B1%2C2%7D%5E1%2Co_%7B1%2C3%7D%5E1%2Co_%7B2%2C1%7D%5E1%2Co_%7B2%2C2%7D%5E1%2Co_%7B2%2C3%7D%5E1%2Co_%7B3%2C1%7D%5E1%2Co_%7B3%2C2%7D%5E1%2Co_%7B3%2C3%7D%5E1" alt="o_{1,1}^1,o_{1,2}^1,o_{1,3}^1,o_{2,1}^1,o_{2,2}^1,o_{2,3}^1,o_{3,1}^1,o_{3,2}^1,o_{3,3}^1"> 。第一层的输出全部会影响第二层的 <img src="https://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E2" alt="o_{1,1}^2"> 。于是 <img src="https://www.zhihu.com/equation?tex=x_%7B1%2C1%7D" alt="x_{1,1}"> 只能通过 <img src="https://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E1" alt="o_{1,1}^1"> 来影响 <img src="https://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E2" alt="o_{1,1}^2"> ；而 <img src="https://www.zhihu.com/equation?tex=x_%7B3%2C3%7D" alt="x_{3,3}"> 能通过 <img src="https://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E1%2Co_%7B1%2C2%7D%5E1%2Co_%7B1%2C3%7D%5E1%2Co_%7B2%2C1%7D%5E1%2Co_%7B2%2C2%7D%5E1%2Co_%7B2%2C3%7D%5E1%2Co_%7B3%2C1%7D%5E1%2Co_%7B3%2C2%7D%5E1%2Co_%7B3%2C3%7D%5E1" alt="o_{1,1}^1,o_{1,2}^1,o_{1,3}^1,o_{2,1}^1,o_{2,2}^1,o_{2,3}^1,o_{3,1}^1,o_{3,2}^1,o_{3,3}^1"> 来影响 <img src="https://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E2" alt="o_{1,1}^2"> 。显而易见，虽然 <img src="https://www.zhihu.com/equation?tex=x_%7B1%2C1%7D" alt="x_{1,1}"> 和 <img src="https://www.zhihu.com/equation?tex=x_%7B3%2C3%7D" alt="x_{3,3}"> 都位于第二层特征感受野内，但是二者对最后的特征 <img src="https://www.zhihu.com/equation?tex=o_%7B1%2C1%7D%5E2" alt="o_{1,1}^2"> 的影响却大不相同，输入中越靠感受野中间的元素对特征的贡献越大。</p><p><img src="/2019/04/21/receptive-size-and-anchor-design/123.png" alt></p><h3 id="3-anchor-设计原则："><a href="#3-anchor-设计原则：" class="headerlink" title="3. anchor 设计原则："></a>3. anchor 设计原则：</h3><p>anchor的本质是特征层的map，这和传统cv中的滑动窗口并无二致。我们的目标是生成更好的窗口去匹配bbox。faster rcnn 是通过CNN 来自动生成 anchor，利用了 CNN 自动提取特征的能力。</p><p>anchor 设计原则：</p><p>(1) anchor 的尺寸、长宽比、位置都应该 match 源数据中的bbox。一种方法是针对特定数据集设计anchor，如YOLOv2中的聚类，和近期有论文CNN训练anchor的设置，这些方法或许更适合某一数据集，但也可能影响模型的泛化能力，换一个库是否依然够用。</p><p>(2) anchor 的 size 必须小于感受野</p><p>(3) 不同size的anchor应当具有相同的空间密度分布。密度一致的话，要求 anchor/stride 为一个定值。</p><p>下面以人脸检测为例，来分析 anchor 的设计：</p><p>（1）MSFD 中通过分析 WIDER Face 中人脸的范围，进而将 anchor 的尺寸设定为 </p><p>16/32/64/128/256/512 这些范围，从而覆盖不同尺寸的人脸。anchors’ aspect ratio 则设定为1: 1.5 。这是考虑到人脸的形状宽高比为 1.5 这一事实。</p><p>(2) S3FD 中提出的两条设计原则。我们看到 anchor 小于感受野，大约为感受野的 1/4， stride 则均为 anchor 的 1/4。这保证了anchor 的密度一致。</p><p><img src="/2019/04/21/receptive-size-and-anchor-design/rf.png" alt></p><p>我们来看Faceboxes中的anchor 设计：</p><p><img src="/2019/04/21/receptive-size-and-anchor-design/anchor.png" alt></p><p>这里 32 和 64 的anchor 密度为1和2，小于其他的密度4，所以通过对 32 复制4次，对64 复制两次来使anchor 密度一致。</p><p>[1]SSD:Single Shot MultiBox Detector</p><p>[2] YOLOv3: An Incremental Improvement</p><p>[3]FPN: feature pyramid networks for object detection </p><p>[4] A practical theory for designing very deep convolutional neural networks</p><p>[5] S3FD: Single Shot Scale-invariant Face Detector</p><p>[6] FaceBoxes: A CPU Real-time Face Detector with High Accuracy</p><p>[7] <a href="https://medium.com/@andersasac/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9">https://medium.com/@andersasac/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9</a></p><p>[8] <a href="https://www.reddit.com/r/MachineLearning/comments/7giwk1/d_is_anchor_necessary_for_object_detection/">https://www.reddit.com/r/MachineLearning/comments/7giwk1/d_is_anchor_necessary_for_object_detection/</a></p><p>[9] <a href="https://zhuanlan.zhihu.com/p/55824651">https://zhuanlan.zhihu.com/p/55824651</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>GCN</title>
    <link href="/2019/04/20/GCN/"/>
    <url>/2019/04/20/GCN/</url>
    
    <content type="html"><![CDATA[<p>GCN tutorial</p><a id="more"></a><h4 id="1-基本的传播规则"><a href="#1-基本的传播规则" class="headerlink" title="1. 基本的传播规则"></a>1. 基本的传播规则</h4><p><img src="/2019/04/20/GCN/1.png" alt></p><p>在开始前先说明几个符号的含义：</p><ol><li>$N = 1,2,3,…,n$ 代表所有节点。</li><li>$X$ 代表所有节点的特征， 其中  $X_i$ 代表节点 $i$ 的特征。</li><li>$A$  代表邻接矩阵，其中 $A_{ij}$ 代表节点 $i$ 和 节点 $j$ 之间的边的权， 如果无权图则为 0 或者 1。</li></ol><h5 id="1-简单传播（平均）："><a href="#1-简单传播（平均）：" class="headerlink" title="1. 简单传播（平均）："></a>1. 简单传播（平均）：</h5><script type="math/tex; mode=display">f(X，A) = AX</script><p>简单传播的问题：</p><ul><li>节点的聚合表征不包含它自己的特征！</li><li>度大的节点在其特征表征中将具有较大的值，度小的节点将具有较小的值。</li></ul><h5 id="2-增加自环"><a href="#2-增加自环" class="headerlink" title="2. 增加自环"></a>2. 增加自环</h5><script type="math/tex; mode=display">f(X，A) = (A+I)X</script><h5 id="3-特征归一化处理"><a href="#3-特征归一化处理" class="headerlink" title="3. 特征归一化处理"></a>3. 特征归一化处理</h5><script type="math/tex; mode=display">f(X，A) =  D^{-1}(A+I)X</script><h5 id="4-对称归一化处理"><a href="#4-对称归一化处理" class="headerlink" title="4. 对称归一化处理"></a>4. 对称归一化处理</h5><script type="math/tex; mode=display">f(X，A) = D^{-0.5}(A+I)D^{-0.5}X</script><h5 id="5-增加权重"><a href="#5-增加权重" class="headerlink" title="5. 增加权重"></a>5. 增加权重</h5><script type="math/tex; mode=display">f(X，A) = D^{-0.5}(A+I)D^{-0.5}X W</script><h5 id="6-添加-激活函数"><a href="#6-添加-激活函数" class="headerlink" title="6. 添加 激活函数"></a>6. 添加 激活函数</h5><script type="math/tex; mode=display">f(X，A) = \sigma (D^{-0.5}(A+I)D^{-0.5}XW)</script><h4 id="2-GCN-可以做什么？"><a href="#2-GCN-可以做什么？" class="headerlink" title="2. GCN 可以做什么？"></a>2. GCN 可以做什么？</h4><h5 id="（1）-结点分类（Node-classification）"><a href="#（1）-结点分类（Node-classification）" class="headerlink" title="（1） 结点分类（Node classification）"></a>（1） 结点分类（Node classification）</h5><h5 id="（2）-图分类（Graph-classification）"><a href="#（2）-图分类（Graph-classification）" class="headerlink" title="（2） 图分类（Graph classification）"></a>（2） 图分类（Graph classification）</h5><h5 id="（3）连接预测（Link-prediction-）"><a href="#（3）连接预测（Link-prediction-）" class="headerlink" title="（3）连接预测（Link prediction ）"></a>（3）连接预测（Link prediction ）</h5><p><img src="/2019/04/20/GCN/2.png" alt></p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol><li><a href="https://tkipf.github.io/graph-convolutional-networks">https://tkipf.github.io/graph-convolutional-networks</a></li><li></li></ol>]]></content>
    
    
    <categories>
      
      <category>探索</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>awesome-face</title>
    <link href="/2019/04/18/awesome-face/"/>
    <url>/2019/04/18/awesome-face/</url>
    
    <content type="html"><![CDATA[<p>my GitHub project awesome-face: face algorithm、source code、datasets、conf/workshop/trans</p><a id="more"></a><h1 id="awesome-face"><a href="#awesome-face" class="headerlink" title="awesome-face"></a>awesome-face</h1><p>🔥  face releated algorithm, datasets and papers  🤔</p><h2 id="📝-Paper-Algorithm"><a href="#📝-Paper-Algorithm" class="headerlink" title="📝 Paper / Algorithm"></a>📝 Paper / Algorithm</h2><h4 id="2D-Face-Recognition"><a href="#2D-Face-Recognition" class="headerlink" title="2D- Face Recognition"></a>2D- Face Recognition</h4><p><img src="/2019/04/18/awesome-face/face_reg.png" alt="2d_face_reg"></p><p><strong>[1] DeepID1</strong>  <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Sun_Deep_Learning_Face_2014_CVPR_paper.pdf"><strong>[paper]</strong></a> </p><p>Deep Learning Face Representation from Predicting 10,000 Classes</p><p><strong>[2] DeepID2</strong>  <a href="https://arxiv.org/abs/1406.4773"><strong>[paper]</strong></a> </p><p>Deep Learning Face Representation by Joint Identification-Verification</p><p><strong>[3] DeepID2+</strong>  <a href="https://arxiv.org/abs/1412.1265"><strong>[paper]</strong></a></p><p>Deeply learned face representations are sparse, selective, and robust</p><p><strong>[4] DeepIDv3</strong>  <a href="https://arxiv.org/abs/1502.00873"><strong>[paper]</strong></a> </p><p>DeepID3: Face Recognition with Very Deep Neural Networks</p><p><strong>[5] Deep Face</strong> <a href="https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf"><strong>[paper]</strong></a> </p><p>Deep Face Recognition</p><p><strong>[6] Center Loss</strong> <a href="http://ydwen.github.io/papers/WenECCV16.pdf"><strong>[paper]</strong></a>    <a href="https://github.com/ydwen/caffe-face"><strong>[code]</strong></a></p><p>A Discriminative Feature Learning Approach for Deep Face Recognition</p><p><strong>[7]Marginal loss</strong> <a href="https://www.computer.org/csdl/proceedings-article/cvprw/2017/0733c006/12OmNzayNCT"><strong>[paper]</strong></a></p><p>Marginal loss for deep face recognition</p><p><strong>[8] Range Loss</strong><a href="https://arxiv.org/abs/1611.08976"><strong>[paper]</strong></a> </p><p>Range Loss for Deep Face Recognition with Long-tail</p><p><strong>[9]Contrastive Loss</strong> <a href="https://arxiv.org/abs/1406.4773"><strong>[paper]</strong></a></p><p>Deep learning face representation by joint identification-verification</p><p><strong>[10] FaceNet</strong>   <a href="https://arxiv.org/abs/1503.03832"><strong>[paper]</strong></a>   <a href="https://github.com/davidsandberg/facenet">[<strong>third-party implemention</strong>]</a></p><p>FaceNet: A Unified Embedding for Face Recognition and Clustering</p><p><strong>[11] NormFace</strong>  <a href="https://arxiv.org/pdf/1704.06369.pdf"><strong>[paper]</strong></a>    <a href="https://github.com/happynear/NormFace"><strong>[code]</strong></a></p><p>NormFace: L2 Hypersphere Embedding for Face Verification</p><p><strong>[12] COCO Loss:</strong>    <a href="https://arxiv.org/pdf/1710.00870.pdf"><strong>[paper]</strong></a>   <a href="https://github.com/sciencefans/coco_loss">[<strong>code</strong>]</a></p><p>Rethinking Feature Discrimination and Polymerization for Large-scale Recognition</p><p><strong>[13] Large-Margin Softmax Loss</strong>  <a href="https://arxiv.org/pdf/1612.02295.pdf"><strong>[paper]</strong></a>  <a href="https://github.com/wy1iu/LargeMargin_Softmax_Loss">[<strong>code</strong>]</a></p><p>Large-Margin Softmax Loss for Convolutional Neural Networks(L-Softmax loss)</p><p><strong>[14]SphereFace：</strong>  <strong>A-Softmax</strong>   <a href="https://arxiv.org/abs/1704.08063"><strong>[paper]</strong></a>  <a href="https://github.com/wy1iu/sphereface">[<strong>code</strong>]</a></p><p>SphereFace: Deep Hypersphere Embedding for Face Recognition</p><p><strong>[15]AM-Softmax/cosFace</strong>     <a href="https://arxiv.org/pdf/1801.05599.pdf"><strong>[paper AM-Softmax]</strong></a>       <a href="https://arxiv.org/pdf/1801.09414.pdf"><strong>[paper cosFace]</strong></a>        <a href="https://github.com/happynear/AMSoftmax">[<strong>AM-softmax code</strong>]</a></p><p>AM : Additive Margin Softmax for Face Verification</p><p>CosFace: Large Margin Cosine Loss for Deep Face Recognition(Tencent AI Lab)</p><p><strong>[16] ArcFace:</strong>  <a href="https://arxiv.org/pdf/1801.07698.pdf"><strong>[paper]</strong></a>  <a href="https://github.com/deepinsight/insightface"><strong>[code]</strong></a></p><p>ArcFace: Additive Angular Margin Loss for Deep Face Recognition</p><p><img src="/2019/04/18/awesome-face/cos_loss.png" alt="cos_loss"></p><h4 id="Face-Detection"><a href="#Face-Detection" class="headerlink" title="Face Detection"></a>Face Detection</h4><p><img src="/2019/04/18/awesome-face/face_detection.png" alt></p><p><strong>[1] Cascade CNN</strong>  <a href="https://ieeexplore.ieee.org/document/7299170/"><strong>[paper]</strong></a>   <a href="https://github.com/anson0910/CNN_face_detection"><strong>[code]</strong></a>    </p><p>A Convolutional Neural Network Cascade for Face Detection</p><p><strong>[2] MTCNN</strong>   <a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/"><strong>[Paper]</strong></a>    <a href="https://github.com/kpzhang93/MTCNN_face_detection_alignment"><strong>[code]</strong></a>  </p><p>Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</p><p><strong>[3] ICC - CNN</strong>  <a href="https://ieeexplore.ieee.org/document/8237606"><strong>[paper]</strong></a></p><p>Detecting Faces Using Inside Cascaded Contextual CNN</p><p><strong>[4] Face R-CNN</strong>  <a href="https://arxiv.org/pdf/1706.01061.pdf"><strong>[Paper]</strong></a></p><p>Face R-CNN</p><p><strong>[5] Deep-IR</strong><a href="https://arxiv.org/abs/1701.08289"><strong>[Paper]</strong></a></p><p>Face Detection using Deep Learning: An Improved Faster RCNN Approach</p><p><strong>[6] SSH</strong>     <a href="https://arxiv.org/pdf/1708.03979.pdf"><strong>[paper]</strong></a>    <a href="https://github.com/mahyarnajibi/SSH"><strong>[code]</strong></a></p><p>SSH: Single Stage Headless Face Detector</p><p><strong>[7] S3FD</strong>   <a href="https://arxiv.org/abs/1708.05237"><strong>[paper]</strong></a></p><p>Single Shot Scale-invariant Face Detector</p><p><strong>[8] FaceBoxes</strong> <a href="https://arxiv.org/pdf/1708.05234.pdf"><strong>[paper]</strong></a>     <a href="https://github.com/sfzhang15/FaceBoxes"><strong>[code]</strong></a></p><p>Faceboxes: A CPU Real-time Face Detector with High Accuracy</p><p><strong>[9] Scaleface</strong>     <a href="http://cn.arxiv.org/abs/1706.02863"><strong>[paper]</strong></a></p><p>Face Detection through Scale-Friendly Deep Convolutional Networks</p><p><strong>[10] HR</strong>  <a href="https://arxiv.org/abs/1612.04402"><strong>[paper]</strong></a>  <a href="https://github.com/peiyunh/tiny"><strong>[code]</strong></a></p><p>Finding Tiny Faces</p><p><strong>[11] FAN</strong>   <a href="https://arxiv.org/abs/1712.00721"><strong>[paper]</strong></a></p><p>Feature Agglomeration Networks for Single Stage Face Detection.</p><p><strong>[12] PyramidBox</strong>    <a href="https://arxiv.org/abs/1803.07737?context=cs"><strong>[paper]</strong></a> <a href="https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/face_detection/README_cn.md"><strong>[code]</strong></a></p><p>PyramidBox: A Context-assisted Single Shot Face Detector</p><p><strong>[13] SRN</strong>     <a href="https://arxiv.org/abs/1809.02693"><strong>[paper]</strong></a> </p><p>Selective Refinement Network for High Performance Face Detection.</p><p><strong>[14] DSFD</strong>  <a href="https://arxiv.org/abs/1810.10220"><strong>[paper]</strong></a></p><p>DSFD: Dual Shot Face Detector</p><p><strong>[15] VIM FD</strong> <a href="https://arxiv.org/abs/1901.02350"><strong>[paper]</strong></a></p><p>Robust and High Performance Face Detector</p><p><strong>[16] ISRN</strong>  <a href="https://arxiv.org/abs/1901.06651"><strong>[paper]</strong></a></p><p>Improved Selective Refinement Network for Face Detection</p><h4 id="Face-Alignment"><a href="#Face-Alignment" class="headerlink" title="Face Alignment"></a>Face Alignment</h4><ul><li><a href="https://arxiv.org/abs/1805.10483">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</a>[Wayne Wu al., 2018]</li><li><a href="https://arxiv.org/pdf/1902.10859.pdf">PFLD: A Practical Facial Landmark Detector</a>[Xiaojie Guo al., 2019]</li></ul><h2 id="⚙-Open-source-lib"><a href="#⚙-Open-source-lib" class="headerlink" title="⚙ Open source lib"></a>⚙ Open source lib</h2><h4 id="face-recognition"><a href="#face-recognition" class="headerlink" title="face recognition"></a>face recognition</h4><ul><li><p><a href="https://github.com/ZhaoJ9014/face.evoLVe.PyTorch">face.evoLVe.</a></p></li><li><p><a href="https://github.com/grib0ed0v/face_recognition.pytorch">face_recognition.pytorch</a></p></li><li><a href="https://github.com/deepinsight/insightface">insightface</a></li></ul><h4 id="face-detection"><a href="#face-detection" class="headerlink" title="face detection"></a>face detection</h4><ul><li><a href="https://github.com/ShiqiYu/libfacedetection">libfaccedetection</a></li></ul><h2 id="📦-Datasets"><a href="#📦-Datasets" class="headerlink" title="📦 Datasets"></a>📦 Datasets</h2><h4 id="2D-Face-Recognition-1"><a href="#2D-Face-Recognition-1" class="headerlink" title="2D Face Recognition"></a>2D Face Recognition</h4><div class="table-container"><table><thead><tr><th>Datasets</th><th>Description</th><th>Links</th><th>Publish Time</th></tr></thead><tbody><tr><td><strong>CASIA-WebFace</strong></td><td><strong>10,575</strong> subjects and <strong>494,414</strong> images</td><td><a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html">Download</a></td><td>2014</td></tr><tr><td><strong>MegaFace</strong>🏅</td><td><strong>1 million</strong> faces, <strong>690K</strong> identities</td><td><a href="http://megaface.cs.washington.edu/">Download</a></td><td>2016</td></tr><tr><td><strong>MS-Celeb-1M</strong>🏅</td><td>about <strong>10M</strong> images for <strong>100K</strong> celebrities   Concrete measurement to evaluate the performance of recognizing one million celebrities</td><td><a href="http://www.msceleb.org">Download</a></td><td>2016</td></tr><tr><td><strong>LFW</strong>🏅</td><td><strong>13,000</strong> images of faces collected from the web. Each face has been labeled with the name of the person pictured.  <strong>1680</strong> of the people pictured have two or more distinct photos in the data set.</td><td><a href="http://vis-www.cs.umass.edu/lfw/">Download</a></td><td>2007</td></tr><tr><td><strong>VGG Face2</strong>🏅</td><td>The dataset contains <strong>3.31 million</strong> images of <strong>9131</strong> subjects (identities), with an average of 362.6 images for each subject.</td><td><a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/">Download</a></td><td>2017</td></tr><tr><td><strong>UMDFaces Dataset-image</strong></td><td><strong>367,888 face annotations</strong> for <strong>8,277 subjects.</strong></td><td><a href="http://www.umdfaces.io">Download</a></td><td>2016</td></tr><tr><td><strong>Trillion Pairs</strong>🏅</td><td>Train: <strong>MS-Celeb-1M-v1c</strong> &amp;  <strong>Asian-Celeb</strong> Test: <strong>ELFW&amp;DELFW</strong></td><td><a href="http://trillionpairs.deepglint.com/overview">Download</a></td><td>2018</td></tr><tr><td><strong>FaceScrub</strong></td><td>It comprises a total of <strong>106,863</strong> face images of male and female <strong>530</strong> celebrities, with about <strong>200 images per person</strong>.</td><td><a href="http://vintage.winklerbros.net/facescrub.html">Download</a></td><td>2014</td></tr><tr><td><strong>Mut1ny</strong>🏅</td><td>head/face segmentation dataset contains over 17.3k labeled images</td><td><a href="http://www.mut1ny.com/face-headsegmentation-dataset">Download</a></td><td>2018</td></tr><tr><td><strong>IMDB-Face</strong></td><td>The dataset contains about 1.7 million faces, 59k identities, which is manually cleaned from 2.0 million raw images.</td><td><a href="https://github.com/fwang91/IMDb-Face">Download</a></td><td>2018</td></tr></tbody></table></div><h4 id="video-face-recognition"><a href="#video-face-recognition" class="headerlink" title="video face recognition"></a>video face recognition</h4><div class="table-container"><table><thead><tr><th>Datasets</th><th>Description</th><th>Links</th><th>Publish Time</th></tr></thead><tbody><tr><td><strong>YouTube Face</strong>🏅</td><td>The data set contains <strong>3,425</strong> videos of <strong>1,595</strong> different people.</td><td><a href="http://www.cs.tau.ac.il/%7Ewolf/ytfaces/">Download</a></td><td>2011</td></tr><tr><td><strong>UMDFaces Dataset-video</strong>🏅</td><td>Over <strong>3.7 million</strong> annotated video frames from over <strong>22,000</strong> videos of <strong>3100 subjects.</strong></td><td><a href="http://www.umdfaces.io">Download</a></td><td>2017</td></tr><tr><td><strong>PaSC</strong></td><td>The challenge includes 9,376 still images and 2,802 videos of 293 people.</td><td><a href="https://www.nist.gov/programs-projects/point-and-shoot-face-recognition-challenge-pasc">Download</a></td><td>2013</td></tr><tr><td><strong>YTC</strong></td><td>The data consists of two parts: video clips (1910 sequences of 47 subjects) and initialization data(initial frame face bounding boxes, manually marked).</td><td><a href="http://seqamlab.com/youtube-celebrities-face-tracking-and-recognition-dataset/">Download</a></td><td>2008</td></tr><tr><td><strong>iQIYI-VID</strong>🏅</td><td>The iQIYI-VID dataset <strong>contains 500,000 videos clips of 5,000 celebrities, adding up to 1000 hours</strong>. This dataset supplies multi-modal cues, including face, cloth, voice, gait, and subtitles, for character identification.</td><td><a href="http://challenge.ai.iqiyi.com/detail?raceId=5b1129e42a360316a898ff4f">Download</a></td><td>2018</td></tr></tbody></table></div><h4 id="3D-face-recognition"><a href="#3D-face-recognition" class="headerlink" title="3D face recognition"></a>3D face recognition</h4><div class="table-container"><table><thead><tr><th>Datasets</th><th>Description</th><th>Links</th><th>Publish Time</th></tr></thead><tbody><tr><td><strong>Bosphorus</strong>🏅</td><td>105 subjects and 4666 faces 2D &amp; 3D face data</td><td><a href="http://bosphorus.ee.boun.edu.tr/default.aspx">Download</a></td><td>2008</td></tr><tr><td><strong>BD-3DFE</strong></td><td>Analyzing <strong>Facial Expressions</strong> in <strong>3D</strong> Space</td><td><a href="http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html">Download</a></td><td>2006</td></tr><tr><td><strong>ND-2006</strong></td><td>422 subjects and 9443 faces 3D Face Recognition</td><td><a href="https://sites.google.com/a/nd.edu/public-cvrl/data-sets">Download</a></td><td>2006</td></tr><tr><td><strong>FRGC V2.0</strong></td><td>466 subjects and 4007 of 3D Face, Visible Face Images</td><td><a href="https://sites.google.com/a/nd.edu/public-cvrl/data-sets">Download</a></td><td>2005</td></tr><tr><td><strong>B3D(AC)^2</strong></td><td><strong>1000</strong> high quality, dynamic <strong>3D scans</strong> of faces, recorded while pronouncing a set of English sentences.</td><td><a href="http://www.vision.ee.ethz.ch/datasets/b3dac2.en.html">Download</a></td><td>2010</td></tr></tbody></table></div><h4 id="Anti-spoofing"><a href="#Anti-spoofing" class="headerlink" title="Anti-spoofing"></a>Anti-spoofing</h4><div class="table-container"><table><thead><tr><th>Datasets</th><th style="text-align:center"># of subj. / # of sess.</th><th>Links</th><th>Year</th><th>Spoof attacks attacks</th><th>Publish Time</th></tr></thead><tbody><tr><td><strong>NUAA</strong></td><td style="text-align:center">15/3</td><td><a href="http://parnec.nuaa.edu.cn/xtan/data/nuaaimposterdb.html">Download</a></td><td>2010</td><td><strong>Print</strong></td><td>2010</td></tr><tr><td><strong>CASIA-MFSD</strong></td><td style="text-align:center">50/3</td><td>Download(link failed)</td><td>2012</td><td><strong>Print, Replay</strong></td><td>2012</td></tr><tr><td><strong>Replay-Attack</strong></td><td style="text-align:center">50/1</td><td><a href="https://www.idiap.ch/dataset/replayattack">Download</a></td><td>2012</td><td><strong>Print, 2 Replay</strong></td><td>2012</td></tr><tr><td><strong>MSU-MFSD</strong></td><td style="text-align:center">35/1</td><td><a href="https://www.cse.msu.edu/rgroups/biometrics/Publications/Databases/MSUMobileFaceSpoofing/index.htm">Download</a></td><td>2015</td><td><strong>Print, 2 Replay</strong></td><td>2015</td></tr><tr><td><strong>MSU-USSA</strong></td><td style="text-align:center">1140/1</td><td><a href="http://biometrics.cse.msu.edu/Publications/Databases/MSU_USSA/">Download</a></td><td>2016</td><td><strong>2 Print, 6 Replay</strong></td><td>2016</td></tr><tr><td><strong>Oulu-NPU</strong></td><td style="text-align:center">55/3</td><td><a href="https://sites.google.com/site/oulunpudatabase/">Download</a></td><td>2017</td><td><strong>2 Print, 6 Replay</strong></td><td>2017</td></tr><tr><td><strong>Siw</strong></td><td style="text-align:center">165/4</td><td><a href="http://cvlab.cse.msu.edu/spoof-in-the-wild-siw-face-anti-spoofing-database.html">Download</a></td><td>2018</td><td><strong>2 Print, 4 Replay</strong></td><td>2018</td></tr></tbody></table></div><h4 id="cross-age-and-cross-pose"><a href="#cross-age-and-cross-pose" class="headerlink" title="cross age and cross pose"></a>cross age and cross pose</h4><div class="table-container"><table><thead><tr><th>Datasets</th><th style="text-align:left">Description</th><th>Links</th><th>Publish Time</th></tr></thead><tbody><tr><td><strong>CACD2000</strong></td><td style="text-align:left">The dataset contains more than 160,000 images of 2,000 celebrities with <strong>age ranging from 16 to 62</strong>.</td><td><a href="http://bcsiriuschen.github.io/CARC/">Download</a></td><td>2014</td></tr><tr><td><strong>FGNet</strong></td><td style="text-align:left">The dataset contains more than 1002 images of 82 people with <strong>age ranging from 0 to 69</strong>.</td><td><a href="http://www-prima.inrialpes.fr/FGnet/html/benchmarks.html">Download</a></td><td>2000</td></tr><tr><td><strong>MPRPH</strong></td><td style="text-align:left">The MORPH database contains <strong>55,000</strong> images of more than <strong>13,000</strong> people within the age ranges of <strong>16</strong> to <strong>77</strong></td><td><a href="http://www.faceaginggroup.com/morph/">Download</a></td><td>2016</td></tr><tr><td><strong>CPLFW</strong></td><td style="text-align:left">we construct a Cross-Pose LFW (CPLFW) which deliberately searches and selects <strong>3,000 positive face pairs</strong> with <strong>pose difference</strong> to add pose variation to intra-class variance.</td><td><a href="http://www.whdeng.cn/cplfw/index.html">Download</a></td><td>2017</td></tr><tr><td><strong>CALFW</strong></td><td style="text-align:left">Thereby we construct a Cross-Age LFW (CALFW) which deliberately searches and selects <strong>3,000 positive face pairs</strong> with <strong>age gaps</strong> to add aging process intra-class variance.</td><td><a href="http://www.whdeng.cn/calfw/index.html">Download</a></td><td>2017</td></tr></tbody></table></div><h4 id="Face-Detection-1"><a href="#Face-Detection-1" class="headerlink" title="Face Detection"></a>Face Detection</h4><div class="table-container"><table><thead><tr><th>Datasets</th><th>Description</th><th>Links</th><th>Publish Time</th></tr></thead><tbody><tr><td><strong>FDDB</strong>🏅</td><td><strong>5171</strong> faces in a set of <strong>2845</strong> images</td><td><a href="http://vis-www.cs.umass.edu/fddb/index.html">Download</a></td><td>2010</td></tr><tr><td><strong>Wider-face</strong> 🏅</td><td><strong>32,203</strong> images and label <strong>393,703</strong> faces with a high degree of variability in scale, pose and occlusion, organized based on <strong>61</strong> event classes</td><td><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/">Download</a></td><td>2015</td></tr><tr><td><strong>AFW</strong></td><td>AFW dataset is built using Flickr images. It has <strong>205</strong> images with <strong>473</strong> labeled faces. For each face, annotations include a rectangular <strong>bounding box</strong>, <strong>6 landmarks</strong> and the <strong>pose angles</strong>.</td><td><a href="http://www.ics.uci.edu/~xzhu/face/">Download</a></td><td>2013</td></tr><tr><td><strong>MALF</strong></td><td>MALF is the first face detection dataset that supports fine-gained evaluation. MALF consists of <strong>5,250</strong> images and <strong>11,931</strong> faces.</td><td><a href="http://www.cbsr.ia.ac.cn/faceevaluation/">Download</a></td><td>2015</td></tr></tbody></table></div><h4 id="Face-Attributes"><a href="#Face-Attributes" class="headerlink" title="Face Attributes"></a>Face Attributes</h4><div class="table-container"><table><thead><tr><th>Datasets</th><th>Description</th><th>Links</th><th>Key features</th><th>Publish Time</th></tr></thead><tbody><tr><td><strong>CelebA</strong></td><td><strong>10,177</strong> number of <strong>identities</strong>,  <strong>202,599</strong> number of <strong>face images</strong>, and  <strong>5 landmark locations</strong>, <strong>40 binary attributes</strong> annotations per image.</td><td><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">Download</a></td><td><strong>attribute &amp; landmark</strong></td><td>2015</td></tr><tr><td><strong>IMDB-WIKI</strong></td><td>500k+ face images with <strong>age</strong> and <strong>gender</strong> labels</td><td><a href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/">Download</a></td><td><strong>age &amp; gender</strong></td><td>2015</td></tr><tr><td><strong>Adience</strong></td><td>Unfiltered faces for <strong>gender</strong> and <strong>age</strong> classification</td><td><a href="http://www.openu.ac.il/home/hassner/Adience/data.html">Download</a></td><td><strong>age &amp; gender</strong></td><td>2014</td></tr><tr><td><strong>WFLW</strong>🏅</td><td>WFLW contains <strong>10000 faces</strong> (7500 for training and 2500 for testing) with <strong>98 fully manual annotated landmarks</strong>.</td><td><a href="https://wywu.github.io/projects/LAB/WFLW.html">Download</a></td><td><strong>landmarks</strong></td><td>2018</td></tr><tr><td><strong>Caltech10k Web Faces</strong></td><td>The dataset has 10,524 human faces of various resolutions and in <strong>different settings</strong></td><td><a href="http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/#Description">Download</a></td><td><strong>landmarks</strong></td><td>2005</td></tr><tr><td><strong>EmotioNet</strong></td><td>The EmotioNet database includes<strong>950,000 images</strong> with <strong>annotated AUs</strong>.  A <strong>subset</strong> of the images in the EmotioNet database correspond to <strong>basic and compound emotions.</strong></td><td><a href="http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html#overview">Download</a></td><td><strong>AU and Emotion</strong></td><td>2017</td></tr><tr><td><strong>RAF( Real-world Affective Faces)</strong></td><td><strong>29672</strong> number of <strong>real-world images</strong>,  including <strong>7</strong> classes of basic emotions and <strong>12</strong> classes of compound emotions,  <strong>5 accurate landmark locations</strong>,  <strong>37 automatic landmark locations</strong>, <strong>race, age range</strong> and  <strong>gender</strong> <strong>attributes</strong> annotations per image</td><td><a href="http://www.whdeng.cn/RAF/model1.html">Download</a></td><td><strong>Emotions、landmark、race、age and gender</strong></td><td>2017</td></tr></tbody></table></div><h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4><div class="table-container"><table><thead><tr><th>Datasets</th><th>Description</th><th>Links</th><th>Publish Time</th></tr></thead><tbody><tr><td><strong>IJB C/B/A</strong>🏅</td><td>IJB C/B/A is currently running <strong>three challenges</strong> related to  <strong>face detection, verification, identification, and identity clustering.</strong></td><td><a href="https://www.nist.gov/programs-projects/face-challenges">Download</a></td><td>2015</td></tr><tr><td><strong>MOBIO</strong></td><td><strong>bi-modal</strong> (<strong>audio</strong> and <strong>video</strong>) data taken from 152 people.</td><td><a href="https://www.idiap.ch/dataset/mobio">Download</a></td><td>2012</td></tr><tr><td><strong>BANCA</strong></td><td>The BANCA database was captured in four European languages in <strong>two modalities</strong> (<strong>face</strong> and <strong>voice</strong>).</td><td><a href="http://www.ee.surrey.ac.uk/CVSSP/banca/">Download</a></td><td>2014</td></tr><tr><td><strong>3D Mask Attack</strong></td><td><strong>76500</strong> frames of <strong>17</strong> persons using Kinect RGBD with eye positions (Sebastien Marcel).</td><td><a href="https://www.idiap.ch/dataset/3dmad">Download</a></td><td>2013</td></tr><tr><td><strong>WebCaricature</strong></td><td><strong>6042</strong> <strong>caricatures</strong> and <strong>5974 photographs</strong> from <strong>252 persons</strong> collected from the web</td><td><a href="https://cs.nju.edu.cn/rl/WebCaricature.htm">Download</a></td><td>2018</td></tr></tbody></table></div><h2 id="🏠-Research-home-conf-amp-workshop-amp-trans"><a href="#🏠-Research-home-conf-amp-workshop-amp-trans" class="headerlink" title="🏠 Research home(conf &amp; workshop &amp; trans)"></a>🏠 Research home(conf &amp; workshop &amp; trans)</h2><p><img src="/2019/04/18/awesome-face/research_home.png" alt></p><p><strong>ICCV</strong>: <a href="http://iccv2019.thecvf.com">IEEE International Conference on Computer Vision</a></p><p><strong>CVPR</strong>: <a href="http://cvpr2018.thecvf.com/">IEEE Conference on Computer Vision and Pattern Recognition</a></p><p><strong>ECCV</strong>: <a href="https://eccv2018.org">European Conference on Computer Vision</a></p><p><strong>FG</strong>: <a href="http://dblp.uni-trier.de/db/conf/fgr/">IEEE International Conference on Automatic Face and Gesture Recognition</a></p><p><strong>BMVC:</strong> <a href="http://www.bmva.org/bmvc/?id=bmvc">The British Machine Vision Conference</a></p><p><strong>IJCB[ICB+BTAS]</strong>:International Joint Conference on Biometrics</p><ul><li><p><strong>ICB</strong>: <a href="http://icb2018.org">International Conference on Biometrics</a></p></li><li><p><strong>BTAS</strong>: <a href="https://www.isi.edu/events/btas2018/home">IEEE International Conference on Biometrics: Theory, Applications and Systems</a></p></li></ul><p><strong>AMFG</strong>: IEEE workshop on Analysis and Modeling of Faces and Gestures</p><p><strong>CVPR Workshop on Biometrics</strong></p><p><strong>TPAMI:</strong> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE Transactions on Pattern Analysis and Machine Intelligence</a></p><p><strong>IJCV:</strong> <a href="https://link.springer.com/journal/11263">International Journal of Computer Vision</a> </p><p><strong>TIP:</strong> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing</a></p><p><strong>TIFS:</strong> <a href="IEEE Transactions on Information Forensics and Security">IEEE Transactions on Information Forensics and Security</a></p><p><strong>PR:</strong> <a href="https://www.journals.elsevier.com/pattern-recognition/">Pattern Recognition</a></p><h2 id="🏷-References"><a href="#🏷-References" class="headerlink" title="🏷 References:"></a>🏷 References:</h2><p>[1] <a href="https://github.com/RiweiChen/DeepFace/tree/master/FaceDataset">https://github.com/RiweiChen/DeepFace/tree/master/FaceDataset</a></p><p>[2] <a href="https://www.zhihu.com/question/33505655?sort=created">https://www.zhihu.com/question/33505655?sort=created</a></p><p>[3] <a href="https://github.com/betars/Face-Resources">https://github.com/betars/Face-Resources</a></p><p>[4] <a href="https://zhuanlan.zhihu.com/p/33288325">https://zhuanlan.zhihu.com/p/33288325</a></p><p>[5] <a href="https://github.com/L706077/DNN-Face-Recognition-Papers">https://github.com/L706077/DNN-Face-Recognition-Papers</a></p><p>[6] <a href="https://www.zhihu.com/question/67919300">https://www.zhihu.com/question/67919300</a></p><p>[7] <a href="https://jackietseng.github.io/conference_call_for_paper/2018-2019-conferences.html">https://jackietseng.github.io/conference_call_for_paper/2018-2019-conferences.html</a></p><p>[8]<a href="http://history.ccf.org.cn/sites/ccf/biaodan.jsp?contentId=2903940690839">http://history.ccf.org.cn/sites/ccf/biaodan.jsp?contentId=2903940690839</a></p><p>[9]<a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/WiderFace_Results.html">http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/WiderFace_Results.html</a></p>]]></content>
    
    
    <categories>
      
      <category>Face</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>pytorch常见工具箱</title>
    <link href="/2019/04/17/pytorch%E5%B8%B8%E8%A7%81%E5%B7%A5%E5%85%B7%E7%AE%B1/"/>
    <url>/2019/04/17/pytorch%E5%B8%B8%E8%A7%81%E5%B7%A5%E5%85%B7%E7%AE%B1/</url>
    
    <content type="html"><![CDATA[<p>收集整理了一些常见的可以用到的深度学习网址、工具</p><a id="more"></a><h3 id="1-预训练模型"><a href="#1-预训练模型" class="headerlink" title="1. 预训练模型"></a>1. 预训练模型</h3><p><a href="https://github.com/Cadene/pretrained-models.pytorch">https://github.com/Cadene/pretrained-models.pytorch</a></p><p><a href="https://github.com/rwightman/pytorch-image-models">https://github.com/rwightman/pytorch-image-models</a></p><p><a href="https://github.com/welkin-feng/ComputerVision">https://github.com/welkin-feng/ComputerVision</a></p><h3 id="2-数据增强"><a href="#2-数据增强" class="headerlink" title="2. 数据增强"></a>2. 数据增强</h3><p><a href="https://github.com/albumentations-team/albumentations">https://github.com/albumentations-team/albumentations</a></p><h3 id="3-标记工具"><a href="#3-标记工具" class="headerlink" title="3. 标记工具"></a>3. 标记工具</h3><p><a href="https://github.com/wkentaro/labelme"><strong>Labelme:</strong></a> Image Polygonal Annotation with Python</p><p><a href="https://github.com/tzutalin/labelImg"><strong>LabelImg</strong></a>：LabelImg is a graphical image annotation tool and label object bounding boxes in images</p><h3 id="4-数据集查找"><a href="#4-数据集查找" class="headerlink" title="4. 数据集查找"></a>4. 数据集查找</h3><p><strong>! ! !  You can find datasets in Paper Beachmark</strong></p><p><a href="https://www.kaggle.com/"><strong>Kaggle</strong></a></p><p><a href="https://toolbox.google.com/datasetsearch"><strong>Google Datasets Search Engine</strong></a></p><p><a href="https://msropendata.com/"><strong>Microsoft Datasets</strong></a></p><p><a href="https://www.visualdata.io/"><strong>Computer Vision Datasets</strong></a></p><p><a href="https://github.com/awesomedata/awesome-public-datasets"><strong>Github awesomedata</strong></a></p><p><a href="https://archive.ics.uci.edu/ml/datasets.html"><strong>UCI Machine Learning Repository.</strong></a></p><p><a href="https://registry.opendata.aws/"><strong>Amazon Datasets</strong></a></p><p><strong>Government Datasets:</strong> <a href="https://data.europa.eu/euodp/data/dataset"><strong>EU</strong></a> <a href="https://www.data.gov/"><strong>US</strong></a> <a href="https://catalogue.data.govt.nz/dataset"><strong>NZL</strong></a> <a href="https://data.gov.in/"><strong>IND</strong></a></p><h3 id="5-模型分析工具"><a href="#5-模型分析工具" class="headerlink" title="5. 模型分析工具"></a>5. 模型分析工具</h3><h5 id="1-卷积层输出大小计算"><a href="#1-卷积层输出大小计算" class="headerlink" title="(1) 卷积层输出大小计算"></a>(1) 卷积层输出大小计算</h5><h5 id="https-ezyang-github-io-convolution-visualizer-index-html"><a href="#https-ezyang-github-io-convolution-visualizer-index-html" class="headerlink" title="https://ezyang.github.io/convolution-visualizer/index.html"></a><a href="https://ezyang.github.io/convolution-visualizer/index.html">https://ezyang.github.io/convolution-visualizer/index.html</a></h5><h5 id="2-计算模型参数量"><a href="#2-计算模型参数量" class="headerlink" title="(2) 计算模型参数量"></a>(2) 计算模型参数量</h5><p><a href="https://github.com/sksq96/pytorch-summary">https://github.com/sksq96/pytorch-summary</a></p><h5 id="3-模型可视化工具"><a href="#3-模型可视化工具" class="headerlink" title="(3) 模型可视化工具"></a>(3) 模型可视化工具</h5><p><a href="https://github.com/lutzroeder/Netron"><strong>Netron:</strong></a> now supports <strong>ONNX</strong>, <strong>Keras</strong>, <strong>CoreML</strong>, <strong>Caffe2</strong>, <strong>Mxnet</strong>, <strong>Pytorch</strong> and <strong>Tensorflow</strong>.</p><p><a href="https://github.com/szagoruyko/pytorchviz"><strong>Graphviz:</strong></a> <strong>Pytorch</strong></p><h3 id="6-可视化工具"><a href="#6-可视化工具" class="headerlink" title="6. 可视化工具"></a>6. 可视化工具</h3><p><a href="https://github.com/facebookresearch/visdom">visdom</a></p><pre><code class="hljs python"><span class="hljs-comment"># Example using Visdom.</span>vis = visdom.Visdom(env=<span class="hljs-string">&#x27;Learning curve&#x27;</span>, use_incoming_socket=<span class="hljs-literal">False</span>)<span class="hljs-keyword">assert</span> self._visdom.check_connection()self._visdom.close()options = collections.namedtuple(<span class="hljs-string">&#x27;Options&#x27;</span>, [<span class="hljs-string">&#x27;loss&#x27;</span>, <span class="hljs-string">&#x27;acc&#x27;</span>, <span class="hljs-string">&#x27;lr&#x27;</span>])(    loss=&#123;<span class="hljs-string">&#x27;xlabel&#x27;</span>: <span class="hljs-string">&#x27;Epoch&#x27;</span>, <span class="hljs-string">&#x27;ylabel&#x27;</span>: <span class="hljs-string">&#x27;Loss&#x27;</span>, <span class="hljs-string">&#x27;showlegend&#x27;</span>: <span class="hljs-literal">True</span>&#125;,    acc=&#123;<span class="hljs-string">&#x27;xlabel&#x27;</span>: <span class="hljs-string">&#x27;Epoch&#x27;</span>, <span class="hljs-string">&#x27;ylabel&#x27;</span>: <span class="hljs-string">&#x27;Accuracy&#x27;</span>, <span class="hljs-string">&#x27;showlegend&#x27;</span>: <span class="hljs-literal">True</span>&#125;,    lr=&#123;<span class="hljs-string">&#x27;xlabel&#x27;</span>: <span class="hljs-string">&#x27;Epoch&#x27;</span>, <span class="hljs-string">&#x27;ylabel&#x27;</span>: <span class="hljs-string">&#x27;Learning rate&#x27;</span>, <span class="hljs-string">&#x27;showlegend&#x27;</span>: <span class="hljs-literal">True</span>&#125;)<span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> epoch(<span class="hljs-number">80</span>):    tran(...)    val(...)    vis.line(X=torch.Tensor([t + <span class="hljs-number">1</span>]), Y=torch.Tensor([train_loss]),             name=<span class="hljs-string">&#x27;train&#x27;</span>, win=<span class="hljs-string">&#x27;Loss&#x27;</span>, update=<span class="hljs-string">&#x27;append&#x27;</span>, opts=options.loss)    vis.line(X=torch.Tensor([t + <span class="hljs-number">1</span>]), Y=torch.Tensor([val_loss]),             name=<span class="hljs-string">&#x27;val&#x27;</span>, win=<span class="hljs-string">&#x27;Loss&#x27;</span>, update=<span class="hljs-string">&#x27;append&#x27;</span>, opts=options.loss)    vis.line(X=torch.Tensor([t + <span class="hljs-number">1</span>]), Y=torch.Tensor([train_acc]),             name=<span class="hljs-string">&#x27;train&#x27;</span>, win=<span class="hljs-string">&#x27;Accuracy&#x27;</span>, update=<span class="hljs-string">&#x27;append&#x27;</span>, opts=options.acc)    vis.line(X=torch.Tensor([t + <span class="hljs-number">1</span>]), Y=torch.Tensor([val_acc]),             name=<span class="hljs-string">&#x27;val&#x27;</span>, win=<span class="hljs-string">&#x27;Accuracy&#x27;</span>, update=<span class="hljs-string">&#x27;append&#x27;</span>, opts=options.acc)    vis.line(X=torch.Tensor([t + <span class="hljs-number">1</span>]), Y=torch.Tensor([lr]),             win=<span class="hljs-string">&#x27;Learning rate&#x27;</span>, update=<span class="hljs-string">&#x27;append&#x27;</span>, opts=options.lr)</code></pre><p><a href="https://pytorch.org/docs/stable/tensorboard.html">Tensorboard</a></p><ul><li><strong>acc / loss</strong></li></ul><pre><code class="hljs python"><span class="hljs-keyword">from</span> tensorboard <span class="hljs-keyword">import</span> SummaryWriterwriter = SummaryWriter()<span class="hljs-keyword">for</span> n_iter <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>):    dummy_s1 = torch.rand(<span class="hljs-number">1</span>)    writer.add_scalar(<span class="hljs-string">&#x27;data/scalar1&#x27;</span>, dummy_s1[<span class="hljs-number">0</span>], n_iter)writer.close()</code></pre><ul><li><strong>img</strong></li></ul><pre><code class="hljs python"><span class="hljs-keyword">from</span> tensorboard <span class="hljs-keyword">import</span> SummaryWriter<span class="hljs-keyword">import</span> torchvision.utils <span class="hljs-keyword">as</span> vutilswriter = SummaryWriter()<span class="hljs-keyword">if</span> n_iter % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:    x = vutils.make_grid(dummy_img, normalize=<span class="hljs-literal">True</span>, scale_each=<span class="hljs-literal">True</span>)    writer.add_image(<span class="hljs-string">&#x27;Image&#x27;</span>, x, n_iter)writer.close()</code></pre><ul><li>在一张图中加入两条曲线</li></ul><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>):    writer.add_scalars(<span class="hljs-string">&#x27;run_14h&#x27;</span>, &#123;<span class="hljs-string">&#x27;xsinx&#x27;</span>:i*np.sin(i/r),                                    <span class="hljs-string">&#x27;xcosx&#x27;</span>:i*np.cos(i/r),                                    <span class="hljs-string">&#x27;tanx&#x27;</span>: np.tan(i/r)&#125;, i)</code></pre><h3 id="7-Pytorch-加速"><a href="#7-Pytorch-加速" class="headerlink" title="7. Pytorch 加速"></a>7. Pytorch 加速</h3><p><strong>NVIDIA/DLAI:</strong> <a href="https://github.com/NVIDIA/DALI">https://github.com/NVIDIA/DALI</a></p><p><strong>Efficient-Pytorch:</strong> <a href="https://github.com/Lyken17/Efficient-PyTorch">https://github.com/Lyken17/Efficient-PyTorch</a></p><p><strong>NVIDIA/APEX:</strong> <a href="https://github.com/nvidia/apex">https://github.com/nvidia/apex</a></p><h3 id="8-性能分析工具"><a href="#8-性能分析工具" class="headerlink" title="8. 性能分析工具"></a>8. 性能分析工具</h3><ul><li>nvidia-smi</li><li>htop</li><li>iotop</li><li>nvtop</li><li>py-spy</li><li>strace</li></ul><h3 id="9-深度学习绘图"><a href="#9-深度学习绘图" class="headerlink" title="9. 深度学习绘图"></a>9. 深度学习绘图</h3><ul><li><a href="https://github.com/dair-ai/ml-visuals"><strong>ML Visuals</strong></a></li><li><a href="https://github.com/HarisIqbal88/PlotNeuralNet"><strong>PlotNeuralNet</strong></a></li></ul><h3 id="10-其他辅助工具"><a href="#10-其他辅助工具" class="headerlink" title="10. 其他辅助工具"></a>10. 其他辅助工具</h3><ul><li><strong>byobu</strong></li><li><strong>screen</strong></li></ul>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>lightweight-cnn-architecture-design</title>
    <link href="/2019/04/15/lightweight-cnn-architecture-design/"/>
    <url>/2019/04/15/lightweight-cnn-architecture-design/</url>
    
    <content type="html"><![CDATA[<p>常见的移动端模型：mobilenet 系列和 shufflenet 系列</p><a id="more"></a><h4 id="1-Mobilenet-v1"><a href="#1-Mobilenet-v1" class="headerlink" title="1. Mobilenet v1"></a>1. Mobilenet v1</h4><p>MobileNet模型的核心就是<strong>将原本标准的卷积操作因式分解成一个depthwise convolution和一个1*1的卷积（文中叫pointwise convolution）操作。简单讲就是将原来一个卷积层分成两个卷积层，其中前面一个卷积层的每个filter都只跟input的每个channel进行卷积，然后后面一个卷积层则负责combining，即将上一层卷积的结果进行合并。 </strong></p><h5 id="具体实现："><a href="#具体实现：" class="headerlink" title="具体实现："></a>具体实现：</h5><p>(1) 传统的卷积计算：假设M表示input的channel个数，N表示output的channel个数（也是本层的卷积核个数）。因此如果假设卷积核大小是<code>DK*DK*M*N</code>，输出是<code>DF*DF*N</code>，那么标准卷积的计算量是<code>DK*DK*M*N*DF*DF</code>。如下图(a)所示。</p><p><img src="/2019/04/15/lightweight-cnn-architecture-design/1.png" alt="将原本的卷积操作进行因式分解"></p><ul><li><p>首先使用<code>M</code>个<code>Dk*Dk</code>的卷积核对输入(<code>M*DF*DF</code>)进行卷积,这里要注意的是每个filter只跟输入的一个通道进行卷积(也就是说，卷积核的通道数为1)，将不同卷积核的结果进行组合，可以得到大小为：<code>M*DF*DF</code>的输出。其计算量为 <code>M*DK*DK*DF*DF</code>。(这里使用的3*3的卷积核，padding为1)。如上图(b)所示。</p></li><li><p>第二步是对之前的<code>M*DF*DF</code> 的输出进行卷积，卷积核为N个<code>1*1*M</code> 的卷积，这样可以得到大小<code>N*DF*DF</code> 的输出。其计算量为<code>1*1*N*M*DF*DF</code>。如上图(c)所示。</p></li></ul><p>所以其总的计算量相比传统的卷积计算减少了：</p><p><img src="/2019/04/15/lightweight-cnn-architecture-design/2.png" alt></p><p>（2）标准卷积（左边）和因式分解后的卷积（右边）的差别如下所示。注意到卷积操作后都会跟一个Batchnorm和ReLU操作。</p><p><img src="/2019/04/15/lightweight-cnn-architecture-design/3.png" alt></p><h4 id="2-Shufflenet-v1"><a href="#2-Shufflenet-v1" class="headerlink" title="2. Shufflenet v1"></a>2. Shufflenet v1</h4><p>Shufflenet v1 主要采用<strong>channel shuffle、pointwise group convolutions和depthwise convolution来修改原来的ResNet单元</strong>。从而大幅度降低深度网络计算量。</p><ul><li>channel shuffle：不同group的通道进行组合。</li><li>pointwise group convolutions：带有 group 的 1 * 1 卷积。</li><li>depthwise convolutions：group 等于通道数的组卷积。</li></ul><h5 id="具体实现：-1"><a href="#具体实现：-1" class="headerlink" title="具体实现："></a>具体实现：</h5><p>（1）a 图是一个带有 depthwise convolution 的resnet 结果，所谓的 depthwise convolution 可以参见[Mobilenet]</p><p>（2）a 图 —&gt; b 图，用带group的 1 <em> 1卷积(2个)代替原来的1 </em> 1卷积，同时添加一个channel shuffle 操作。</p><p>（3）b 图 —&gt; c 图，添加了一个步长为2的 Average pooling，将Resnet最后的Add操作c改为concat操作，也就是按channel合并，类似googleNet的Inception操作。</p><p><img src="/2019/04/15/lightweight-cnn-architecture-design/5.png" alt></p><h4 id="3-Mobilenet-V2"><a href="#3-Mobilenet-V2" class="headerlink" title="3. Mobilenet V2"></a>3. Mobilenet V2</h4><h5 id="具体实现：-2"><a href="#具体实现：-2" class="headerlink" title="具体实现："></a>具体实现：</h5><p>(1) <strong>添加 residual connection</strong>。</p><p>(2) <strong>通过1x1卷积先提升通道数，再通过depthwise的3x3空间卷积，再用1x1卷积降低维度。作者称之为Inverted residual block，两边窄中间宽，像柳叶，仅用较小的计算量就能得到较好的性能</strong>。</p><p>(3) <strong>使用ReLU6替换传统的ReLU6。最后输出的ReLU6去掉，直接线性输出</strong>。</p><p>如下图是 mobilenet v1与 mobilenet v2 的对比图。两者的区别在于：</p><ul><li>v2在原有的dw之前加了一个pw专门用来升维。这么做是因为不能改变通道数量，先加pw升维后，dw就能在高维提特征了。</li><li>v2把原本dw之后用来降维的pw后的激活函数给去掉了。这么做是因为他认为非线性在高维有益处，但在低维（例如pw降维后的空间）不如线性好。</li></ul><p><img src="/2019/04/15/lightweight-cnn-architecture-design/8.png" alt></p><p>如下图是 Resnet 与 mobilenet v2 的对比图。可以看到两者的结果很相似。不过ResNet是先降维（0.25倍）、提特征、再升维。而v2则是先升维（6倍）、提特征、再降维。另外v2也用DW代替了标准卷积来做特征提取。</p><p>注：示意表达式省略了Shortcut。</p><p><img src="/2019/04/15/lightweight-cnn-architecture-design/7.png" alt></p><h4 id="4-shufflenet-V2"><a href="#4-shufflenet-V2" class="headerlink" title="4. shufflenet V2"></a>4. shufflenet V2</h4><h5 id="具体实现：-3"><a href="#具体实现：-3" class="headerlink" title="具体实现："></a>具体实现：</h5><p><strong>(1) 基本单元</strong></p><p>如下左图所示</p><p>(1) 在每个单元的开始，<code>c</code> 特征通道的输入被分为两支，分别带有 <code>c−c&#39;</code> 和 <code>c&#39;</code>个通道。</p><p>(2)  一个分支仍然保持不变。另一个分支由三个卷积组成，令输入和输出通道相同。与 ShuffleNet V1 不同的是，两个 1×1 卷积不再是组卷积。是因为分割操作已经产生了两个组。</p><p>(3) 卷积之后，把两个分支拼接起来，从而通道数量保持不变。</p><p>(4) 然后进行与 ShuffleNet V1 相同的「Channel Shuﬄe」操作来保证两个分支间能进行信息交流。「Shuffle」之后，下一个单元开始运算。</p><p>注意，! ShuﬄeNet V1 [15] 中的「加法」操作不再存在。! 像 ReLU 和深度卷积这样的操作只存在一个分支中。另外，! 三个连续的操作「拼接」、「Channel Shuﬄe」和「通道分割」合并成一个操作。</p><p><strong>空间下采样单元：</strong></p><p>对于空间下采样，该单元经过稍微修改，详见下图右， 通道分割运算被移除。因此，输出通道数量翻了一倍。</p><p><img src="/2019/04/15/lightweight-cnn-architecture-design/9.png" alt></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>人脸识别损失函数</title>
    <link href="/2019/04/15/Face-recog-loss-fun/"/>
    <url>/2019/04/15/Face-recog-loss-fun/</url>
    
    <content type="html"><![CDATA[<p>常见的损失函数一览:从softmax loss 到 triplet loss 再到各种 softmax 改型。</p><a id="more"></a><h4 id="softmax-loss"><a href="#softmax-loss" class="headerlink" title="softmax loss"></a>softmax loss</h4><p>softmax是最常见的人脸识别函数。softmax 函数将人脸识别问题看做一个经典的分类问题。</p><script type="math/tex; mode=display">L_{softmax} = -\frac{1}{N}\sum_{i=1}^{N} log \frac{e^{W^T_{y_i}x_i+b_{y_i}}}{\sum_{j=1}^{k}e^{W^T_jx_i+b_j}}</script><p>公式中 N 是 batch size 的大小，k是类别数目。</p><h4 id="Triplet-loss"><a href="#Triplet-loss" class="headerlink" title="Triplet loss"></a>Triplet loss</h4><p>​        Triplet loss 采用三元组进行训练，所谓的三元组就是三个样例，如(anchor, pos, neg)，其中，x和p是同一类，x和n是不同类。那么学习的过程就是学到一种表示，对于尽可能多的三元组，使得anchor和pos的距离，小于anchor和neg的距离。</p><p><img src="/2019/04/15/Face-recog-loss-fun/triplet.png" alt></p><script type="math/tex; mode=display">||x_i^a - x_i^p||^2_2 +m < ||x_i^a-x_i^n||_2^2</script><h4 id="center-loss"><a href="#center-loss" class="headerlink" title="center loss"></a>center loss</h4><p>​       center loss的核心是：为每一个类别提供一个类别中心，最小化每个样本与该中心的距离，从而减小类内差距。center loss 由两项构成，第一项是传统的 softmax loss， 第二项是样本到类中心的距离。</p><script type="math/tex; mode=display">L_{center} = -\frac{1}{N}\sum_{i=1}^{N} log \frac{e^{W^T_{y_i}x_i+b_{y_i}}}{\sum_{j=1}^{k}e^{W^T_jx_i+b_j}} + \frac{\lambda}{2}\sum_{i=1}^N||x_i-c_{y_i}||^2_2</script><p><img src="/2019/04/15/Face-recog-loss-fun/center loss.png" alt></p><h4 id="L-softmax"><a href="#L-softmax" class="headerlink" title="L-softmax"></a>L-softmax</h4><p>考虑一个两分类问题。原始的Softmax的目的是使得  $W_1^Tx &gt; W_2^Tx$， 即 $||W_1||||x||cos(\theta_1) &gt; ||W_1||||x||cos(\theta_2)$，在这个基础上，L-softmax(Large-margin softmax) 希望可以通过增加一个正整数变量m，使得产生的决策边界可以更加严格地约束上述不等式，让类内的间距更加的紧凑，类间的间距更加具有区分性。</p><script type="math/tex; mode=display">L_{l-softmax} = -\frac{1}{N}\sum_{i=1}^N log(\frac{e^{||W_{yi}||||x_i||\psi{(\theta_{y_i})}}}{e^{||W_{yi}||||x_i||\psi{(\theta_{y_i})}} + \sum_{j=1, j\neq y_i}^k{e^{||W_j||||x_i||cos(\theta_j)}}})</script><p>其中,</p><script type="math/tex; mode=display">\psi(\theta) = \begin{equation}  \left\{      \begin{array}{**lr**}     cos(m\theta), 0 \leq \theta \leq \frac{\pi}{m}\\     D(\theta) ,  \frac{\pi}{m}\leq \theta \leq \pi \\        \end{array}  \right. \end{equation}</script><p>cos函数在(0,π)内是单调递减的，乘上正整数m后内积会减小，这样可以加大类间的差别。通过控制m的大小，调整类间距离。m越大，类间距离就越大，类内更加紧凑。</p><p><img src="/2019/04/15/Face-recog-loss-fun/lms.png" alt></p><h4 id="sphereface-A-softmax"><a href="#sphereface-A-softmax" class="headerlink" title="sphereface(A-softmax)"></a>sphereface(A-softmax)</h4><p>SphereFace是在 softmax 的基础上将权重归一化，即式 $||W||=1, b=0$  。它与前面提到的L-sofrmax最大的区别在于SphereFace将W权重归一化了。L-Softmax会同时从角度和权重长度上区分不同类别，而SphereFace只从角度上去区分不同类别。</p><script type="math/tex; mode=display">L_{A-softmax} = -\frac{1}{N}\sum_{i=1}^{N}log(\frac{e^{||x_i||\psi(\theta_{y_i})}}{e^{||x_i||\psi(\theta_{y_i})} + \sum_{j=1, j \neq y_i}^k{e^{||x_i||cos\theta_j}}})</script><h4 id="cosface"><a href="#cosface" class="headerlink" title="cosface"></a>cosface</h4><p>cosFace的思想和sphereFace( A-softmax)的思想接近，其中主要做了以下三点的改进：</p><ul><li>loss的形式做了稍微的改变，将超参数m由乘法运算变成了减法运算</li><li>不仅对权重进行了正则化，还对特征进行了正则化</li><li>再乘上一个 s 参数，当超球面过小时，分类映射到超球面上不好分类，这个 s 参数可以扩大超球面体积，帮助分类</li></ul><script type="math/tex; mode=display">L_{cos} = -\frac{1}{N} \sum_{i=1}^{N} log(\frac{e^{s(cos(\theta_{yi})-m)}}{e^{s(cos(\theta_{yi})-m)}+\sum_{j=1, j\neq y_i}^k e^{scos \theta_j}})</script><h4 id="arcface"><a href="#arcface" class="headerlink" title="arcface"></a>arcface</h4><p>arcface的思想和cosface类似，主要区别是将m放入了cos中，角度距离比余弦距离对角度的影响更加直接:</p><script type="math/tex; mode=display">L_{arc} = -\frac{1}{N} \sum_{i=1}^{N} log(\frac{e^{s(cos(\theta_{yi}+m))}}{e^{s(cos(\theta_{yi}+m))}+\sum_{j=1,j\neq y_i}^k e^{scos\theta_j}})</script><p>arcface 人脸识别流程图：</p><p><img src="/2019/04/15/Face-recog-loss-fun/flowchart.png" alt></p><p>不同损失函数的分类边界如下图所示：</p><p><img src="/2019/04/15/Face-recog-loss-fun/cos_loss.png" alt></p>]]></content>
    
    
    <categories>
      
      <category>Face</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Face-Detection-Solution</title>
    <link href="/2019/04/13/Face-Detection-Solution/"/>
    <url>/2019/04/13/Face-Detection-Solution/</url>
    
    <content type="html"><![CDATA[<p>Face Detection 人脸检测设计方案</p><a id="more"></a><h4 id="1-整体网络架构设计"><a href="#1-整体网络架构设计" class="headerlink" title="1. 整体网络架构设计"></a>1. 整体网络架构设计</h4><p><strong>DSFD</strong>：ResNet101, DPN−98, SE-ResNeXt101 32×4d、<strong>ResNet152</strong>、SE-ResNet101（SSD 架构、未进行上下层的融合）</p><p><strong>Faceboxes</strong>：(conv+CReLU+pool)<em>2 + inception </em> 3 + conv * 4（SSD 架构、未进行上下层的融合）</p><p><strong>PymaridBox</strong>：VGG16（LFPN架构）</p><p><strong>VIM-FD</strong> ： densenet121 （SSD+attention：STR+STC）</p><p><strong>S3FD</strong>：VGG16 （SSD + norm）</p><p><strong>SSH</strong>: vgg16</p><p><strong>Scale Face</strong>：Resnet 50</p><p><strong>smallhardface</strong>：VGG16 （one shot、直通式架构：能实现这个性能简直有点不可思议！ 值得研究的架构）</p><p><strong>SRN</strong>：combining DRN with Root-ResNet-18 to have a training speed/accuracy trade-off backbone for SRN</p><p>(比较新的架构、值得研究一下：STR+STC)</p><p><strong>MSFD</strong>：VGG 16（上中下三层融合机制）</p><p><strong>对于主干架构，通常选择 vgg16 或者 resnet50/101, 如果追求轻量级可以使用 mobilenetv2 or shufflenet v2。</strong></p><h4 id="2-如何设计多尺度特征"><a href="#2-如何设计多尺度特征" class="headerlink" title="2. 如何设计多尺度特征"></a>2. 如何设计多尺度特征</h4><p>常见的多尺度的设计方案：</p><p>（a）图像金字塔，即将图像做成不同的scale，然后不同scale的图像生成对应的不同scale的特征。这种方法的缺点在于增加了时间成本。有些算法会在测试时候采用图像金字塔。 </p><p>（b）像SPP net，Fast RCNN，Faster RCNN是采用这种方式，即仅采用网络最后一层的特征。 </p><p>（c）像SSD（Single Shot Detector）采用这种多尺度特征融合的方式，没有上采样过程，即从网络不同层抽取不同尺度的特征做预测，这种方式不会增加额外的计算量。作者认为SSD算法中没有用到足够低层的特征（在SSD中，最低层的特征是VGG网络的conv4_3），而在作者看来足够低层的特征对于检测小物体是很有帮助的。 </p><p>（d）本文作者是采用这种方式，顶层特征通过上采样和低层特征做融合，而且每层都是独立预测的。</p><p><img src="/2019/04/13/Face-Detection-Solution/2.png" alt></p><p><strong>进入 2018年以来，大部分网络均采用 FPN 网络结构的形式，融合多个特征层进行检测。有时候会搭配一个独特设计的网络，比如SRN 的 STR_STC 结构、MSFD 的三层融合机制、DSFD 的 Two-shot 结构。都进行了不同程度的创新。</strong></p><h4 id="3-Loss-设计"><a href="#3-Loss-设计" class="headerlink" title="3. Loss 设计"></a>3. Loss 设计</h4><h5 id="（1）smooth-l1-loss"><a href="#（1）smooth-l1-loss" class="headerlink" title="（1）smooth l1  loss"></a>（1）smooth l1  loss</h5><p>Smooth l1 loss 的定义如下所示：</p><script type="math/tex; mode=display">smooth_{L_1}(x) = \begin{equation}  \left\{      \begin{array}{**lr**}      0.5 x^2,  if |x| < 1  \\      |x|-0.5,  otherwise \\        \end{array}  \right.  \end{equation}</script><h5 id="（2）focal-loss"><a href="#（2）focal-loss" class="headerlink" title="（2）focal loss"></a>（2）focal loss</h5><script type="math/tex; mode=display">L_{focol\ loss} = \begin{equation}  \left\{      \begin{array}{**lr**}      -(1-y')^\gamma logy',  y=1 \\      -y'^\gamma log(1-y'),  y=0 \\        \end{array}  \right.  \end{equation}</script><h4 id="4-anchor-设计"><a href="#4-anchor-设计" class="headerlink" title="4. anchor 设计"></a>4. anchor 设计</h4><p><strong>anchor 设计的三个原则：</strong><br><strong>(1) anchor 的尺寸、长宽比、位置都应该 match 源数据中的bbox。一种方法是针对特定数据集设计anchor，如YOLOv2中的聚类，和近期有论文CNN训练anchor的设置，这些方法或许更适合某一数据集，但也可能影响模型的泛化能力，换一个库是否依然够用。</strong><br><strong>(2) anchor的size 必须小于感受野</strong><br><strong>(3) 不同size的anchor应当具有相同的空间密度分布。密度一致的话，要求 anchor/stride 为一个定值。</strong></p><h4 id="5-数据增强策略"><a href="#5-数据增强策略" class="headerlink" title="5. 数据增强策略:"></a>5. 数据增强策略:</h4><p><strong>S3FD</strong>：Color distort、Random crop、Horizontal flip</p><p><strong>Faceboxes</strong>：Color distort、Random crop、Horizontal flip、Scale transformation、Face-box filter</p><p><strong>SRN</strong>：photometric distortions<strong>, </strong>randomly expanding by zero-padding operation<strong>, </strong>randomly cropping patches<strong>、 </strong>data-anchor-sampling in PyramidBox</p><p><strong>Pyramid box</strong>： color distort, random crop and horizontal flip.data-anchor-sampling</p><p><strong>VIMFD</strong>: data-anchor-sampling method in PyramidBox</p><p><strong>Small hard face</strong>: random cropping,photometric distortion</p><p><strong>Tiny face</strong>: resize</p><p><strong>最常见的几种方案是：Color distort、Random crop(resize)、Horizontal flip、data-anchor-sampling method in PyramidBox </strong></p><p>其他值得借鉴的数据增强策略:</p><p>[1] SSD &amp; YOLO v3 相关数据增强</p><p>[2] <a href="https://github.com/maozezhong/CV_ToolBox/blob/master/DataAugForObjectDetection/DataAugmentForObejctDetection.py">https://github.com/maozezhong/CV_ToolBox/blob/master/DataAugForObjectDetection/DataAugmentForObejctDetection.py</a></p><p>[3] torchvision</p><p>[4] <a href="https://github.com/dmlc/gluon-cv?files=1">mxnet：Bag of Freebies for Training Object Detection Neural Networks</a></p><h4 id="6-深度学习人脸检测方案的发展"><a href="#6-深度学习人脸检测方案的发展" class="headerlink" title="6. 深度学习人脸检测方案的发展"></a>6. 深度学习人脸检测方案的发展</h4><p><img src="/2019/04/13/Face-Detection-Solution/face_detection.png" alt="Face Detection"></p><h4 id="7-常见数据集"><a href="#7-常见数据集" class="headerlink" title="7. 常见数据集"></a>7. 常见数据集</h4><ul><li><p>FDDB</p></li><li><p>Wider-Face</p></li><li><p>PASCAL</p></li><li><p>AFW</p></li></ul><p><strong>相比而言：WIDER-FACE 更加权威、FDDB 次之，PASCAL和AFW 都是比较小的数据集，基本可以忽略。</strong></p>]]></content>
    
    
    <categories>
      
      <category>Face</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>FaceBoxes</title>
    <link href="/2019/04/13/FaceBoxes/"/>
    <url>/2019/04/13/FaceBoxes/</url>
    
    <content type="html"><![CDATA[<p>FaceBoxes: A CPU Real-time Face Detector with High Accuracy</p><a id="more"></a><h3 id="FaceBoxes-A-CPU-Real-time-Face-Detector-with-High-Accuracy"><a href="#FaceBoxes-A-CPU-Real-time-Face-Detector-with-High-Accuracy" class="headerlink" title="FaceBoxes: A CPU Real-time Face Detector with High Accuracy"></a>FaceBoxes: A CPU Real-time Face Detector with High Accuracy</h3><p>论文阅读：FaceBoxes: A CPU Real-time Face Detector with High Accuracy</p><p>文章： <a href="http://cn.arxiv.org/abs/1708.05234">http://cn.arxiv.org/abs/1708.05234</a></p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>2个挑战：</p><p>1) 在杂乱背景下人脸视角大的变化需要人脸检测器精准的解决复杂人脸和非人脸的分类问题。</p><p>2) 较大的搜索空间和人脸尺寸进一步增加了时间效率的需要。</p><p>​        传统方法效率高但在人脸大的视角变化下精度不够，基于CNN的方法精度高但速度很慢。受到Faster R-CNN的RPN以及SSD中多尺度机制的启发，便有了这篇可以在CPU上实时跑的FaceBoxes。</p><h4 id="FaceBoxes"><a href="#FaceBoxes" class="headerlink" title="FaceBoxes"></a>FaceBoxes</h4><p><strong>（1）RDCL：Rapidly Digested Convolutional Layers, 加速计算</strong></p><p>​    缩小输入的空间大小：为了快速减小输入的空间尺度大小，在卷积核池化上使用了一系列的大的stride,在Conv1,Pool1,Conv2,Pool2上stride分别是4,2,2,2,RDCL的stride一共是32，意味着输入的尺度大小被快速减小了32倍。这里是区别于YOLO v3 的关键， YOLO v3 整体下降了32倍，这里仅仅在 RDCL 部分就下降了32倍，参数量是少了很多，可以加快速度，这个点还是值得借鉴的</p><ul><li><p>选择合适的kernel size：一个网络开始的一些层的kernel size应该比较小以用来加速，同时也应该足够大用以减轻空间大小减小带来的信息损失。Conv1,Conv2和所有的Pool分别选取7x7, 5x5, 3x3的kernel size。</p></li><li><p>减少输出通道数：使用C.ReLU来增加输出通道数，比使用卷积增加通道数需要的参数量更少。</p></li></ul><p><strong>（2）MSCL：Multiple Scale Convolutional Layers,丰富感受野，使不同层的anchor离散化以处理多尺度人脸</strong></p><p>　　将RPN作为一个人脸检测器，不能获取很好的性能有以下两个原因：(1) RPN中的anchor只和最后一个卷积层相关，其中的特征和分辨率在处理人脸变化上太弱。(2) anchor相应的层使用一系列不同的尺度来检测人脸，但只有单一的感受野，不能匹配不同尺度的人脸。</p><p>　　为解决这个问题，对MSCL从以下两个角度去设计：</p><p>　　Multi-scale design along the dimension of network depth.如下图，anchor在多尺度的feature map上面取，类似SSD。</p><p><img src="/2019/04/13/FaceBoxes/1.png" alt="img"></p><p>Multi-scale design along the dimension of network width.使用inception模块，内部使用不同大小的卷积核，可以捕获到更多的尺度信息。</p><p><strong>（3）Anchor densification strategy：</strong></p><p> Inception的anchor尺度为32<em>32,64</em>64,128<em>128,Conv3_2、Conv4_2的尺度分别为256</em>256和512<em>512。比如Conv3_2的stride是64、anchor大小为256</em>256，表示对应输入图片每64像素大小有一个256*256的anchor。</p><p><img src="/2019/04/13/FaceBoxes/2.png" alt="img"></p><p>​    我们定义 anchor密度为：Adensity = Ascale/Ainterval。Ascale表示anchor的尺度，Ainterval表示anchor间隔。</p><p>​    显然在不同尺度上anchor的密度不均衡。相比大的anchor（128-256-512），小的anchor（32和64）过于稀疏，将会导致在小脸检测中低的召回率。</p><p>为解决不均衡问题，此处提出新的anchor策略。为了加大一种anchor的密度，在一个感受野的中心均匀的堆叠n^2 个anchor（本来是1个）用来预测。文章里对32<em>32的anchor做了4倍，对64</em>64的anchor做了2倍，这样就可以保证不同尺度的anchor有相同的密度。</p><p><img src="/2019/04/13/FaceBoxes/3.png" alt="img"></p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><h5 id="1-Training-dataset"><a href="#1-Training-dataset" class="headerlink" title="1. Training dataset:"></a>1. Training dataset:</h5><p>​    WIDER FACE的子集，12880个图片。</p><p><strong>（1）Data augmentation:</strong></p><ul><li><p>Color distorition:根据<a href="https://arxiv.org/ftp/arxiv/papers/1312/1312.5402.pdf">《Some Improvements on Deep Convolutional Neural Network Based Image Classification》</a></p></li><li><p>Random cropping: 从原图中随机裁剪5个方块patch:一个最大方块，其他的分别在范围[0.3, 1]之于原图尺寸。</p></li><li><p>Scale transformation:将随机裁剪后的方块patch给resize到1024*1024.</p></li><li><p>Horizontal flipping: 0.5的概率翻转。</p></li><li><p>Face-box filter: 如果face box的中心在处理后的图片上，则保持其重叠，然后将高或宽小于20像素的face box过滤出来。</p></li></ul><p><strong>（2）Matching strategy:</strong></p><p>​    在训练时需要判断哪个anchor是和哪个face bounding box相关的。首先使用jaccard overlap将每个脸和anchor对应起来，然后对anchor和任意脸jaccard overlap高于阈值（0.35）的匹配起来。</p><p><strong>（3）Loss function:</strong></p><p>​    和Faster R-CNN中的RPN用同样的loss,一个2分类的softmax loss用来做分类，smooth L1用来做回归。</p><p><strong>（4）Hard negative mining:</strong></p><p>在anchor匹配后，大多数anchor都是负样本，导致正样本和负样本严重不均衡。为了更快更稳定的训练，将他们按照loss值排序并选取最高的几个，保证正样本和负样本的比例最高不超过3:1.</p><p><strong>（5）Other implementation details:</strong></p><p>​    Xavier随机初始化、优化器SGD、momentum:0.9、weight decay:5e-4，batch size:32，迭代最大次数:120k，初始80k迭代learning rate:1e-3，80-100k迭代用1e-4，,100-120k迭代用1e-5，使用caffe实现。</p><h4 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h4><p><img src="/2019/04/13/FaceBoxes/4.png" alt="img"></p><p><strong>(2) Model analysis</strong></p><p>​    FDDB相比 AFW 和 PASCAL face 较为困难，因此这里在FDDB上作分析。</p><p><strong>(2) Ablative Setting:</strong></p><p>1) 去掉 anchor densification strategy. =&gt; Anchor densification strategy is crucial.  这种 anchor 策略也提升了大约1个百分点</p><p>2)把 MSCL 替换为三层卷积，其大小都为3*3，输出数都和MSCL中前三个Inception的保持一致. 同时，把anchor只和最后一层卷积关联。 =&gt; MSCL is better.  稍微慢了 1点，但是精度提高了一个点（！FPN 已经成为现有网络都借鉴的一点）</p><p>3)把RDCL中的C.ReLU替换为ReLU。=&gt; RDCL is efficient and accuracy-preserving. 不改变精确度的情况下， 提升了大约 1/3 的速度</p><p><img src="/2019/04/13/FaceBoxes/5.png" alt="img"></p><p><strong>(3) 实验结果:</strong></p><p>AFW:</p><p><img src="/2019/04/13/FaceBoxes/6.png" alt="img"></p><p>PASCAL face:</p><p><img src="/2019/04/13/FaceBoxes/7.png" alt="img"></p><p>FDDB:</p><p><img src="/2019/04/13/FaceBoxes/8.png" alt="img"></p><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><ul><li>大卷积快速降低运算量 + CReLU</li><li>anchor 采样机制</li><li>多尺度特征融合</li></ul>]]></content>
    
    
    <categories>
      
      <category>Face</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>MTCNN</title>
    <link href="/2019/04/13/MTCNN/"/>
    <url>/2019/04/13/MTCNN/</url>
    
    <content type="html"><![CDATA[<p>论文阅读: MTCNN  Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</p><a id="more"></a><h4 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h4><p>​    相比于R-CNN系列通用检测方法，本文更加针对人脸检测这一专门的任务，速度和精度都有足够的提升。R-CNN，Fast R-CNN，FasterR-CNN这一系列的方法不是一篇博客能讲清楚的，有兴趣可以找相关论文阅读。类似于TCDCN，<strong>本文提出了一种 Multi-task的人脸检测框架，将人脸检测和人脸特征点检测同时进行。论文使用3个CNN级联的方式，和Viola-Jones类似，实现了coarse-to-fine的算法结构</strong>。</p><h4 id="2-框架"><a href="#2-框架" class="headerlink" title="2. 框架"></a>2. 框架</h4><p><strong>（1）.算法流程</strong></p><p><img src="/2019/04/13/MTCNN/1.jpg" alt="algorithm flow"></p><p><strong>当给定一张照片的时候，将其缩放到不同尺度形成图像金字塔，以达到尺度不变。</strong></p><p><strong>Stage 1：使用P-Net是一个全卷积网络，用来生成候选窗和边框回归向量(bounding box regression vectors)。使用Bounding box regression的方法来校正这些候选窗，使用非极大值抑制（NMS）合并重叠的候选框。全卷积网络和Faster R-CNN中的RPN一脉相承。</strong></p><p><strong>Stage 2：使用N-Net改善候选窗。将通过P-Net的候选窗输入R-Net中，拒绝掉大部分false的窗口，继续使用Bounding box regression和NMS合并。</strong></p><p><strong>Stage 3：最后使用O-Net输出最终的人脸框和特征点位置。和第二步类似，但是不同的是生成5个特征点位置。</strong></p><p><strong>（2）CNN结构</strong></p><p>本文使用三个CNN，结构如图：</p><p><img src="/2019/04/13/MTCNN/2.png" alt="MTCNN"></p><p><strong>（3）.训练</strong></p><p>这个算法需要实现三个任务的学习：人脸非人脸的分类，bounding box regression和人脸特征点定位。</p><p>(1)人脸检测</p><p>这就是一个分类任务，使用交叉熵损失函数即可：</p><script type="math/tex; mode=display">L_i^{det} = -(y_i^{det}log(p_i)+(1-y_i^{det})(1-log(p_i)))</script><p>(2)Bounding box regression</p><p>这是一个回归问题，使用平方和损失函数：</p><script type="math/tex; mode=display">L_i^{box} = || y_i^{box} - y_i^{box} ||_2^2</script><p>(3)人脸特征点定位</p><p>这也是一个回归问题，目标是5个特征点与标定好的数据的平方和损失：</p><script type="math/tex; mode=display">L_i^{Landmark} = ||y_i^{landmark} - y_i^{landmark}||</script><p><strong>(4)多任务训练</strong></p><p>不是每个sample都要使用这三种损失函数的，比如对于背景只需要计算$L^{det}_i$，不需要计算别的损失，这样就需要引入一个指示值指示样本是否需要计算某一项损失。最终的训练目标函数是：</p><script type="math/tex; mode=display">min\sum_{i=1}^{N} \sum_{j \in \{det,box,landmark\}} \alpha_j\beta_i^jL_i^j</script><p>N是训练样本的数量，$\alpha<em>j$表示任务的重要性。在 P-Net 和 R-Net 中，$\alpha</em>{det}=1, \alpha<em>{box}=0.5, \alpha</em>{landmark}=0.5$，在O-Net中，$\alpha<em>{det}=1, \alpha</em>{box}=0.5, \alpha_{landmark}=1$</p><p><strong>(5)online hard sample mining</strong></p><p>传统的难例处理方法是检测过一次以后，手动检测哪些困难的样本无法被分类，本文采用online hard sample mining的方法。具体就是在每个mini-batch中，取loss最大的70%进行反向传播，忽略那些简单的样本。</p><h4 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h4><p>本文主要使用三个数据集进行训练：FDDB，Wider Face，AFLW。</p><p>A、训练数据</p><p>本文将数据分成4种：</p><p>Negative：非人脸 </p><p>Positive：人脸 </p><p>Part faces：部分人脸 </p><p>Landmark face：标记好特征点的人脸</p><p>分别用于训练三种不同的任务。Negative和Positive用于人脸分类，positive和part faces用于bounding box regression，landmark face用于特征点定位。</p><p>B、效果</p><p>本文的人脸检测和人脸特征点定位的效果都非常好。关键是这个算法速度很快，在2.6GHZ的CPU上达到16fps，在Nvidia Titan达到99fps。</p><h4 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h4><p>本文使用一种级联的结构进行人脸检测和特征点检测，该方法速度快效果好，可以考虑在移动设备上使用。这种方法也是一种由粗到细的方法，和Viola-Jones的级联AdaBoost思路相似。</p><p>类似于Viola-Jones：1、如何选择待检测区域：图像金字塔+P-Net；2、如何提取目标特征：CNN；3、如何判断是不是指定目标：级联判断。</p><h4 id="5-训练流程"><a href="#5-训练流程" class="headerlink" title="5. 训练流程"></a>5. 训练流程</h4><p>第一阶段：首先对原图片构建一个金字塔，对不同尺寸的图片调整到12x12输入到PNet(Propossal)中，PNet会返回诸多的Bbox，利用nms选取合适的Bbox。</p><p>第二阶段，由PNet得出的候选框输送给RNet(Refine)， RNet会对相应的Bbox进行精细回归，并返回置信度。利用nms对精细回归后的Bbox进行nms选取， 并舍弃相应的置信度较低的人脸。</p><p>第三阶段和第二阶段相似，只不过最后会返回相应的Landmark坐标。</p><p>具体的训练过程如下所示：</p><p><img src="/2019/04/13/MTCNN/train_step.png" alt></p><p><strong>具体的技术细节：</strong></p><p><strong>(1) 如何构造金字塔：</strong></p><p>如果输入图像为100×120, 假设输入图像中存在一个人脸，则其中人脸最小为20×20，最大为100×100——对应图像较短边长, 为了将人脸放缩到12×12，同时保证相邻层间缩放比率factor=0.709，则金子塔中图像尺寸依次为 60×72、52×61、36×43、26×31、18×22、13×16，其中60×72对应把20×20的人脸缩放到12×12，13×16对应把100×100的人脸缩放到12×12</p><pre><code class="hljs python"><span class="hljs-comment"># scales for scaling the image</span>scales = [] m = min_detection_size/min_face_sizemin_length *= m factor_count = <span class="hljs-number">0</span><span class="hljs-keyword">while</span> min_length &gt; min_detection_size:      scales.append(m*factor**factor_count)    min_length *= factor      factor_count += <span class="hljs-number">1</span></code></pre><p><strong>(2) 如何生成训练数据?</strong></p><p>PNet 网络：pos、part、neg 是随机裁剪得到的图像，landmark截取的是带有关键点的图像。将这些图像都resize成12x12 作为PNet的输入。</p><p>RNet网络：图片(金字塔之后)经过PNet 产生产生的大量人脸框，然后将其resize为 24x24大小作为Rnet 输入。</p><p>ONet网络： 图片(金字塔之后)经过PNet 和 RNet 产生的过滤后的人脸框，然后将其 resize 为 48x48大小作为ONet 的输入。</p><p>注意：</p><ul><li>pos[IoU&gt;0.65]、part[0.4&lt;=IoU&lt;0.65]、neg[IoU&lt;0.3] </li><li>BBox 和 Landmark 都是需要进行归一化的</li></ul><p><strong>(2)NMS的基本原理：</strong> </p><p>非极大值抑制（NMS）顾名思义就是抑制不是极大值的元素，搜索局部的极大值。</p><ul><li>将所有框的得分降序排列，选中最高分及其对应的框。</li><li>遍历其余的框，如果和当前最高分框的重叠面积(IOU)大于一定阈值，我们就将框删除。</li><li>从未处理的框中继续选一个得分最高的，重复上述过程。</li></ul><pre><code class="hljs python"><span class="hljs-comment"># nms python cpu 实现</span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">py_cpu_nms</span>(<span class="hljs-params">dets, thresh</span>):</span>    <span class="hljs-string">&quot;&quot;&quot;Pure Python NMS baseline.&quot;&quot;&quot;</span>    x1 = dets[:, <span class="hljs-number">0</span>]    y1 = dets[:, <span class="hljs-number">1</span>]    x2 = dets[:, <span class="hljs-number">2</span>]    y2 = dets[:, <span class="hljs-number">3</span>]    scores = dets[:, <span class="hljs-number">4</span>]    areas = (x2 - x1 + <span class="hljs-number">1</span>) * (y2 - y1 + <span class="hljs-number">1</span>)    order = scores.argsort()[::<span class="hljs-number">-1</span>]    keep = []    <span class="hljs-keyword">while</span> order.size &gt; <span class="hljs-number">0</span>:        i = order[<span class="hljs-number">0</span>]        keep.append(i)        xx1 = np.maximum(x1[i], x1[order[<span class="hljs-number">1</span>:]])        yy1 = np.maximum(y1[i], y1[order[<span class="hljs-number">1</span>:]])        xx2 = np.minimum(x2[i], x2[order[<span class="hljs-number">1</span>:]])        yy2 = np.minimum(y2[i], y2[order[<span class="hljs-number">1</span>:]])        w = np.maximum(<span class="hljs-number">0.0</span>, xx2 - xx1 + <span class="hljs-number">1</span>)        h = np.maximum(<span class="hljs-number">0.0</span>, yy2 - yy1 + <span class="hljs-number">1</span>)        inter = w * h        ovr = inter / (areas[i] + areas[order[<span class="hljs-number">1</span>:]] - inter)        inds = np.where(ovr &lt;= thresh)[<span class="hljs-number">0</span>]        order = order[inds + <span class="hljs-number">1</span>]    <span class="hljs-keyword">return</span> keep</code></pre><p><strong>（3）三个任务的损失是什么？函数如何平衡？</strong></p><p>第一个任务是分类任务：其损失函数为交叉熵损失函数。第二个任务和第三个任务均是回归任务，其损失是平方均差损失函数。通过加权 $\alpha$ 来平衡三个任务的权重：在 P-Net 和 R-Net 中，$\alpha<em>{det}=1$, $\alpha</em>{box}=0.5$, $\alpha<em>{landmark}=0.5$，在O-Net中，$\alpha</em>{det}=1$, $\alpha<em>{box}=0.5$, $\alpha</em>{landmark}=1$</p><p><strong>（4）论文的主要创新点？ 尽量简单概括</strong></p><p>​    通过级联三个卷积神经网络来实现人脸的检测和Landmark定位。另外本文也提出了一种online hard sample mining方法，具体就是在每个mini-batch中，取loss最大的70%进行反向传播，忽略那些简单的样本。</p><p><strong>（5）有什么改进措施？</strong></p><ul><li>融入 anchor机制， 同时可以修改损失函数为 focal loss</li><li>加大数据增强方法</li><li>bn层、leakey relu/prelu</li><li>模仿shufflenet、mobilenet进行改进提高速度</li></ul>]]></content>
    
    
    <categories>
      
      <category>Face</category>
      
    </categories>
    
    
    <tags>
      
      <tag>face、MTCNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Classification_neural_network</title>
    <link href="/2019/04/13/Classification-neural-network/"/>
    <url>/2019/04/13/Classification-neural-network/</url>
    
    <content type="html"><![CDATA[<p>本文主要梳理了四种主要常见的分类网络 alexnet、vgg、inception、resnet。</p><a id="more"></a><h3 id="1-Alexnet"><a href="#1-Alexnet" class="headerlink" title="1. Alexnet"></a>1. Alexnet</h3><p>Alexnet 将 LeNet 的思想发扬光大，把CNN 的基本原理应用到了很深很宽的网络中。</p><p>AlexNet 主要用到的新技术点如下：</p><p>（1） 成功<strong>使用ReLU作为CNN 的激活函数</strong>，并验证其效果在较深的网络超过 Sigmoid， 成功解决了Sigmoid在网络较深时的梯度弥散问题。</p><p>（2） 训练时<strong>使用Dropout随机忽略一部分神经元，以避免过拟合</strong>。</p><p>（3）在CNN中<strong>使用重叠的最大池化</strong>。此前CNN 中普遍采用平均池化,AlexNet 全部使用最大池化，避免平均池化的模糊化效果。并且 AlexNet 中提出让步长比池化核的尺寸小，这样池化层的输出之间有重叠和覆盖，提高了特征的丰富性。</p><p>（4）提出<strong>LRN层，对局部神经元的活动创建竞争机制</strong>，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强模型的泛化能力。</p><p>（5） <strong>使用CUDA加速深度卷积网络的训练</strong>，利用GPU强大的并行能力，处理神经网络训练时大量的矩阵运算。</p><p>（6）<strong>数据增强</strong>，随机地从 256 x 256 的原始图像中截取 224 x 224 大小的区域（以及水平翻转的镜像）相当于增加了(256-224)^2 * 2 = 2048 倍的数据量。大大减轻了模型过拟合，提升泛化能力。同时 AlexNet 论文中提到了会对图像的RGB 数据进行PCA 处理，并对主成分做一个标准差为0.1的高斯扰动， 增加一些噪声。</p><p><img src="/2019/04/13/Classification-neural-network/Alexnet.png" alt></p><h3 id="2-VGGNet"><a href="#2-VGGNet" class="headerlink" title="2. VGGNet"></a>2. VGGNet</h3><p><strong>VGGNet探索了卷积神经网络的深度与其性能之间的关系，通过反复堆叠3x3的小型卷积核和2x2 的最大池化层， VGGnet成功构筑了16~19层深的卷积神经网络</strong>。</p><p>（1）<strong>通过将多个卷积层堆叠在一起，可以减少参数数目的同时增加卷积层的非线性变换，使得CNN 对特征的学习能力更强</strong>。</p><p>（2）VGGNet 在训练时有个小技巧，先训练级别A 的简单网络，再复用A网络的权重来初始化后面几个复杂模型，这样训练收敛的速度更快。</p><p>（3）在测试，VGG 采用了 Multi-Scale 的方法，将 图像scale到一个尺寸Q， 并将图片输入卷积网络运算。再将不同尺寸Q的结果平均得到最后结果，这样提高图片数据的利用率并提升准确率。同时在训练中还是用了Multi-Scale 的方法做数据增强。</p><p><img src="/2019/04/13/Classification-neural-network/vgg.jpg" alt></p><h3 id="3-Google-Inception-Net"><a href="#3-Google-Inception-Net" class="headerlink" title="3. Google Inception Net"></a>3. Google Inception Net</h3><p>（1）<strong>精心设计了 Inception Module提高参数的利用效率，其结构如下所示，Inception Module中包含3种不同尺寸的卷积核1个最大池化，增加了网络对不同尺度的适应性</strong>。</p><p>第一个分支对输入进行 1x1卷积，<strong>1x1卷积可以跨通道组织信息，提高网络的表达能力，同时可以对输出通道升维和降维</strong>。</p><p>第二个分支先使用了 1x1 卷积，然后连接 3x3 卷积，相当于进行两次特征变换。</p><p>第三个分支和第二个分支类似，先是使用了1x1 的卷积，然后连接 5x5 的卷积。</p><p>最后一个分支则是3x3 最大池化后直接使用1x1卷积。</p><p>Inception Module 的4个分支在最后通过一个聚合操作合并（再输出通道这个维度上聚合）</p><p><img src="/2019/04/13/Classification-neural-network/inception.png" alt="img"></p><p>（2） <strong>去除了最后的全连接层，用全局平均池化层来取代它</strong>。</p><h3 id="4-ResNet"><a href="#4-ResNet" class="headerlink" title="4. ResNet"></a>4. ResNet</h3><p>​    <strong>ResNet 通过调整网络结构来解决梯度消失问题</strong>（反向传播时，梯度将涉及多层参数的交叉相乘，可能会在离输入近的网络层中产生梯度消失的现象）。首先考虑两层神经网络的简单叠加，这时  x 经过两个网络层的变换得到H(x),  激活函数采用 ReLU， 如下图所示。既然离输入近的神经网络层较难训练，那么我们可以将它短接到更靠近输出的层，如下图所示。输入  经过两个神经网络变换得到F(x) , 同时也短接到两层之后，最后这个包含两层的神经网络模块输出H(x) = F(x) + x 。这样一来，F(x) 被设计为只需要拟合输入x与目标输入H(x)的残差H(x)-x ， 残差网络的名称也因此而来。如果某一层的输出已经较好的拟合了期望结果，那么多加入一层也不会使得模型变得更差，因为该层的输出将直接短接到两层之后，相当于直接学习了一个恒等映射，而跳过的两层只需要拟合上层输出和目标之间的残差即可。</p><p><img src="/2019/04/13/Classification-neural-network/resnet.png" alt="img"></p>]]></content>
    
    
    
    <tags>
      
      <tag>classification</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>efficient-conv</title>
    <link href="/2019/04/13/efficient-conv/"/>
    <url>/2019/04/13/efficient-conv/</url>
    
    <content type="html"><![CDATA[<p><img src="/2019/04/13/efficient-conv/1.png" alt></p><p>！！！ 先挖个坑吧，等有空再填，现在是没有时间了🤣</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>mobile-neural-network-engine-design</title>
    <link href="/2019/04/13/mobile-neural-network-engine-design/"/>
    <url>/2019/04/13/mobile-neural-network-engine-design/</url>
    
    <content type="html"><![CDATA[<h4 id="1-基于C-C-的基本优化"><a href="#1-基于C-C-的基本优化" class="headerlink" title="1. 基于C/C++的基本优化"></a>1. 基于C/C++的基本优化</h4><ol><li><p>编译器很牛逼，GCC/CLANG 都有运行速度的优化选项，打开这些选项能帮助程序显著提升速度，虽然这还远远不够，但聊胜于无吧。</p><p>下面是 ncnn 示例项目中的一段代码:</p><pre><code class="hljs cmake"><span class="hljs-comment"># ncnn/examples/squeezencnn/jni/Android.mk</span>LOCAL_CFLAGS := -O2 -fvisibility=hidden -fomit-frame-pointer -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-<span class="hljs-keyword">math</span>LOCAL_CPPFLAGS := -O2 -fvisibility=hidden -fvisibility-inlines-hidden -fomit-frame-pointer -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-<span class="hljs-keyword">math</span>LOCAL_LDFLAGS += -Wl,--gc-sections</code></pre><p>以前工作时候写的CMakeLists 中的代码：</p><pre><code class="hljs cmake"><span class="hljs-comment"># set(CMAKE_BUILD_TYPE Release)</span><span class="hljs-keyword">if</span> (CMAKE_BUILD_TYPE <span class="hljs-keyword">STREQUAL</span> <span class="hljs-string">&quot;Debug&quot;</span>)    <span class="hljs-keyword">set</span>(CMAKE_CXX_FLAGS_DEBUG <span class="hljs-string">&quot;$ENV&#123;CXXFLAGS&#125; -O0 -Wall -g2 -ggdb&quot;</span>)    <span class="hljs-keyword">message</span>(STATUS <span class="hljs-string">&quot;CMAKE_BUILD_TYPE = Debug&quot;</span>)<span class="hljs-keyword">else</span>()    <span class="hljs-keyword">set</span>(CMAKE_CXX_FLAGS_RELEASE <span class="hljs-string">&quot;$ENV&#123;CXXFLAGS&#125; -O3 -Wall&quot;</span>)    <span class="hljs-keyword">message</span>(STATUS <span class="hljs-string">&quot;CMAKE_BUILD_TYPE = Release&quot;</span>)<span class="hljs-keyword">endif</span>()</code></pre></li><li><p>书写高效的C代码。循环展开、内联、分支优化，避免除法，查表等优化小技巧要滚瓜烂熟，信手拈来。</p></li><li><p>必须看得懂汇编，即使你不写，也要知道编译器编译出来的汇编代码效率如何。这样你可以通过调整C/C++代码，让编译器生成你需要的代码。</p></li></ol><h4 id="2-缓存友好"><a href="#2-缓存友好" class="headerlink" title="2. 缓存友好"></a>2. 缓存友好</h4><h5 id="（1）少用内存"><a href="#（1）少用内存" class="headerlink" title="（1）少用内存"></a>（1）少用内存</h5><h5 id="（2）连续访问、对齐访问、合并访问、显示对齐数据加载、缓存预取"><a href="#（2）连续访问、对齐访问、合并访问、显示对齐数据加载、缓存预取" class="headerlink" title="（2）连续访问、对齐访问、合并访问、显示对齐数据加载、缓存预取"></a>（2）连续访问、对齐访问、合并访问、显示对齐数据加载、缓存预取</h5><h4 id="3-多线程"><a href="#3-多线程" class="headerlink" title="3. 多线程"></a>3. 多线程</h4><h5 id="1-OpenMP"><a href="#1-OpenMP" class="headerlink" title="1. OpenMP"></a>1. OpenMP</h5><p>OpenMP会自动为循环分配线程，使用OpenMP加速只需要在串行代码中添加编译指令以及少量API即可。理想情况下，加速比大约能达到0.75*cores。</p><pre><code class="hljs cpp"><span class="hljs-comment">// ncnn/src/layer/relu.cpp</span><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">ReLU::forward_inplace</span><span class="hljs-params">(Mat&amp; bottom_top_blob, <span class="hljs-keyword">const</span> Option&amp; opt)</span> <span class="hljs-keyword">const</span></span><span class="hljs-function"></span>&#123;    <span class="hljs-keyword">if</span> (bottom_top_blob.elemsize == <span class="hljs-number">1u</span>)        <span class="hljs-keyword">return</span> ReLU::forward_inplace_int8(bottom_top_blob, opt);    <span class="hljs-keyword">int</span> w = bottom_top_blob.w;    <span class="hljs-keyword">int</span> h = bottom_top_blob.h;    <span class="hljs-keyword">int</span> channels = bottom_top_blob.c;    <span class="hljs-keyword">int</span> size = w * h;    <span class="hljs-keyword">if</span> (slope == <span class="hljs-number">0.f</span>)    &#123;        <span class="hljs-meta">#<span class="hljs-meta-keyword">pragma</span> omp parallel for num_threads(opt.num_threads)</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)        &#123;            <span class="hljs-keyword">float</span>* ptr = bottom_top_blob.channel(q);            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)            &#123;                <span class="hljs-keyword">if</span> (ptr[i] &lt; <span class="hljs-number">0</span>)                    ptr[i] = <span class="hljs-number">0</span>;            &#125;        &#125;    &#125;    <span class="hljs-keyword">else</span>    &#123;        #pragma omp parallel <span class="hljs-keyword">for</span> num_threads(opt.num_threads)        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> q=<span class="hljs-number">0</span>; q&lt;channels; q++)        &#123;            <span class="hljs-keyword">float</span>* ptr = bottom_top_blob.channel(q);            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;size; i++)            &#123;                <span class="hljs-keyword">if</span> (ptr[i] &lt; <span class="hljs-number">0</span>)                    ptr[i] *= slope;            &#125;        &#125;    &#125;    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;</code></pre><p>​    但并非所有循环都适合做多线程优化，如果每次循环只做了非常少的事情，那么使用多线程会得不偿失。实际运用中，可以通过 <code>#pragma omp parallel for if (cond)</code> 语句来判断runtime过程中是否要启用多线程。</p><h5 id="动态调度"><a href="#动态调度" class="headerlink" title="动态调度"></a>动态调度</h5><h5 id="稀疏化"><a href="#稀疏化" class="headerlink" title="稀疏化"></a>稀疏化</h5><h5 id="定点化"><a href="#定点化" class="headerlink" title="定点化"></a>定点化</h5><h5 id="NEON-汇编"><a href="#NEON-汇编" class="headerlink" title="NEON 汇编"></a>NEON 汇编</h5><h4 id="4-内存精简"><a href="#4-内存精简" class="headerlink" title="4. 内存精简"></a>4. 内存精简</h4><p>每个layer都会产生blob，除了最后的结果和多分支中间结果，大部分blob都可以不保留，开启ncnn的轻模式可以在运算后自动回收，省下内存。  如下图所示：</p><p><img src="/2019/04/13/mobile-neural-network-engine-design/1.png" alt>某网络结构为 <code>Conv</code> -&gt; <code>Bias</code> -&gt; <code>ReLU</code> -&gt; <code>Concat</code>，在轻模式下，向ncnn索要Concat结果时，Conv结果会在运算Bias时自动回收，而Bais结果会在运算ReLU时自动回收，而ReLU结果会在运算Concat时自动回收, 最后只保留Concat结果，后面再需要C结果会直接获得，满足绝大部分深度网络的使用方式。ncnn 开启轻模式仅需要一行代码:</p><pre><code class="hljs cpp">set_light_mode(<span class="hljs-literal">true</span>)</code></pre><p>相关参考资料：</p><p><a href="https://m.facebook.com/notes/facebook-engineering/three-optimization-tips-for-c/10151361643253920">Three-optimization-tips-for-c</a></p><p><a href="https://www.ibm.com/developerworks/cn/aix/library/au-aix-openmp-framework/index.html">通过GCC学习OpenMP框架</a></p><p><a href="https://www.openmp.org/">OPEN MP官网</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>create user and setup security</title>
    <link href="/2019/04/12/create-user-and-setup-security/"/>
    <url>/2019/04/12/create-user-and-setup-security/</url>
    
    <content type="html"><![CDATA[<p>Let’s create a new user and then setup some security.</p><h3 id="1-New-User"><a href="#1-New-User" class="headerlink" title="1. New User"></a>1. New User</h3><p>login first</p><pre><code class="hljs shell">mkdfideloper<span class="hljs-meta">#</span><span class="bash"> Create password, skip extra field and Set Y to save the new user</span></code></pre><p>Become new user fideloper</p><pre><code class="hljs ebnf"><span class="hljs-attribute">sudo su fideloper</span></code></pre><p>Head to home directory</p><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~/</code></pre><p>See the file path</p><pre><code class="hljs bash"><span class="hljs-built_in">pwd</span> <span class="hljs-comment"># /home/ubuntu</span></code></pre><h3 id="2-Setup-SSH-Key-Authentication"><a href="#2-Setup-SSH-Key-Authentication" class="headerlink" title="2 . Setup SSH Key Authentication"></a>2 . Setup SSH Key Authentication</h3><p>We can re-use the SSH key we created to allow us to log in as user root. On our Mac, we can get the public key into our clipboard again:</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> On our host (Macintosh):</span>cat ~/.ssh/id_sfh_start.pub | pbcopy</code></pre><p>Then over in the server, add that public key to user fideloper’s authorized_keys file:</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> Logged <span class="hljs-keyword">in</span> as user fideloper</span>cd ~mkdir .sshvim .ssh/authorized_keys <span class="hljs-meta">#</span><span class="bash"> Paste <span class="hljs-keyword">in</span> the public key </span></code></pre><h3 id="3-Disallow-Root-Login"><a href="#3-Disallow-Root-Login" class="headerlink" title="3. Disallow Root Login"></a>3. Disallow Root Login</h3><p>First, we want user fideloper to be able to use sudo commands, so we don’t need the root user to perform administrative tasks.</p><pre><code class="hljs dockerfile">sudo <span class="hljs-keyword">user</span></code></pre><p>We can do this easily in Ubuntu by adding the user fideloper to the group sudo or admin (More explanation on that within the video).</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> Append (-a) secondary group (-G) <span class="hljs-string">&quot;admin&quot;</span> to user <span class="hljs-string">&quot;fideloper&quot;</span></span>usermod -aG admin fideloper</code></pre><p>Then log out, and log back in as user fideloper and you’ll be able to use sudocommands. Next, let’s secure our server further and disallow root. </p><h4 id="Configure-SSH"><a href="#Configure-SSH" class="headerlink" title="Configure SSH"></a>Configure SSH</h4><p>Now that user fideloper can do administrative tasks (thingsrequiring super user access), let’s edit the SSH daemonconfiguration to change this.</p><p>We’ll do two things:</p><ul><li><p>Disallow password based authentication</p></li><li><p>Disallow root user login</p></li></ul><p>Do to that, we update the file <code>/etc/ssh/sshd_config</code> and change the following: </p><pre><code class="hljs yaml"><span class="hljs-comment"># Disallow root login over ssh</span><span class="hljs-string">PermitRootLogin</span> <span class="hljs-literal">no</span> <span class="hljs-comment"># Disallow password authentication</span><span class="hljs-string">PasswordAuthentication</span> <span class="hljs-literal">no</span></code></pre><p> Then restart the SSH daemon: </p><pre><code class="hljs routeros">sudo<span class="hljs-built_in"> service </span>ssh restart</code></pre><p>And you’re all set!</p><p><strong>reference</strong>:<a href="https://serversforhackers.com/c/creating-users-">https://serversforhackers.com/c/creating-users-</a> and-ssh-security</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>img2col</title>
    <link href="/2019/04/12/img2col/"/>
    <url>/2019/04/12/img2col/</url>
    
    <content type="html"><![CDATA[<h4 id="1-im2col-基本原理"><a href="#1-im2col-基本原理" class="headerlink" title="1. im2col 基本原理"></a>1. im2col 基本原理</h4><p><strong>(1) 对于图像的变换：</strong></p><p>首先将每个卷积所卷积的区域作为一行，所有的卷积区域纵向排列，作为右乘矩阵。需要注意的是多个通道应该堆叠在下面。如下图所示：</p><p>单通道：</p><p><img src="/2019/04/12/img2col/1.jpeg" alt="img"></p><p>多通道：</p><p><img src="/2019/04/12/img2col/2.jpeg" alt="img"></p><p><strong>输入特征图转化得到的矩阵尺度 = (卷积组输入通道数*卷积核高*卷积核宽) * (卷积层输出单通道特征图高 * 卷积层输出单通道特征图宽)</strong></p><p><strong>(2) 对于卷积核的变换</strong></p><p>将一个卷积核拉伸为一个横行，作为左乘矩阵：(为什么要拉伸为横行，在于对应的卷积区域拉伸为竖行，这样才能与之相对应，进行矩阵乘法)</p><p><img src="/2019/04/12/img2col/3.jpeg" alt="img"></p><p><strong>权值矩阵尺度 = (输出层通道数) * (卷积输入通道数*卷积核高*卷积核宽)</strong></p><p><strong>(3) 将卷积核矩阵核图像矩阵相乘即可</strong></p><p><img src="/2019/04/12/img2col/4.jpeg" alt="img"></p><p><strong>卷积层输出尺度 = (卷积层输出通道数) * (卷积层输出单通道特征图高 * 卷积层输出单通道特征图宽)</strong></p><p>最后需要将结果调整成为需要的大小： </p><p><img src="/2019/04/12/img2col/5.jpeg" alt="img"></p><p><strong>(4) 对于多个输出通道图像的卷积</strong></p><p>应该纵向堆叠每个卷积层</p><p><img src="/2019/04/12/img2col/6.jpeg" alt="img"></p><p>最后再调整输出矩阵：</p><p><img src="/2019/04/12/img2col/7.jpeg" alt="img"></p><h4 id="2-总体简图："><a href="#2-总体简图：" class="headerlink" title="2. 总体简图："></a>2. 总体简图：</h4><p>下面是一个整体矩阵乘法的简图：</p><p><img src="/2019/04/12/img2col/8.jpeg" alt="img"></p><h4 id="3-im2col-cpu-源码剖析"><a href="#3-im2col-cpu-源码剖析" class="headerlink" title="3. im2col_cpu 源码剖析"></a>3. im2col_cpu 源码剖析</h4><p>im2col_cpu函数将卷积层输入转化为矩阵相乘的右元，核心是5个for循环，首先第一个for循环表示按照输入的通道数逐个处理卷积层输入的特征图，下面笔者将用图示表示剩余的四个for循环操作，向读者朋友们展示卷积层输入的单通道特征图是通过怎样的方式转化为一个矩阵。在这里我们假设，卷积层输入单通道特征图原大小为5*5，高和宽方向的pad为1，高和宽方向步长为2，卷积核不进行扩展。</p><p>  我们先计算一下，卷积层输入单通道特征图转化得到的矩阵的尺度，矩阵的行数应该为卷积核高<em>卷积核宽，即为9，列数应该为卷积层输出特征图高(output_h)</em>卷积层输出特征图宽(output_w)，也为9，那么，im2col算法起始由下图开始：</p><p><img src="/2019/04/12/img2col/11.png" alt></p><p>首先kernel_row为0，kernel_col也为0。按照input_row = -pad_h + kernel_row * dilation_h计算input_row的值，在这里，pad_h为1，kernel_row为0，dilation_h为1，计算出input_row为-1，此时output_row为3，满足函数中的第一个if条件，那么在输出图像上先置output_w个零，因为output_w为3，因此得到下图：</p><p><img src="/2019/04/12/img2col/12.png" alt></p><p> 然后input_row加上步长2，由-1变成1，此时output_rows为2，计算input_col等于-1，此时执行input_col定义下面的for循环，得到3个值：依次往目标矩阵中填入0，data_im[1*5+1]和data_im[1*5+3]，即填入0,7和9。得到下图：</p><p><img src="/2019/04/12/img2col/13.png" alt></p><p>再接着执行，此时input_row再加上2变为3，此时output_rows变为1，计算input_col等于-1，执行input_col定义下面的for循环，得到3个值，分别为0，data_im[3*5+1]和data_im[3*5+3]，即填入0,17和19。得到下图：(以上操作是将第一个通道的卷积对应第一个对应位置进放好)</p><p><img src="/2019/04/12/img2col/14.png" alt></p><p> 接着，kernel_col变成1，此时kernel_row为0，kernel_col为1。计算input_row又变成-1，第一个if条件成立，那么，再在输出矩阵上输出3个0。然后，input_row变成1，input_col分别为0(-1+1)，2(-1+1+2)和4(-1+1+2+2)时，输出矩阵上分别输出data_im[1*5+0]，data[1*5+2]，data[1*5+4]，即分别填入6,8,10。然后，input_row变成3，input_col分别为0，2，4时，输出矩阵上分别输出data_im[3*5+0]，data[3*5+2]，data[3*5+4]，即分别输出16,18,20。（将第一个通道的卷积对应第二个对应位置进放好）</p><p><img src="/2019/04/12/img2col/15.png" alt></p><p>然后，kernel_col变成2，此时kernel_row为0，kernel_col为2。计算input_row又变成-1，第一个if条件成立，那么，再在输出矩阵上输出3个0。然后，input_row变成1，input_col分别为1(-1+2)，3(-1+2+2)和5(-1+2+2+2)时，输出矩阵上分别输出data_im[1*5+1]，data[1*5+3]，0，即分别填入7,9,0。然后，input_row变成3，input_col分别为1，3，5时，输出矩阵上分别输出data_im[3*5+0]，data[3*5+2]，0，即分别输出17,19,0。见下图：（将第一个通道的卷积对应第三个对应位置进放好）</p><p><img src="/2019/04/12/img2col/16.png" alt></p><p>接着，kernel_row变成1，kernel_col变成0。计算input_row又变成0，input_col分别为-1(-1+0)，1(-1+0+2)和3(-1+0+2+2)，输出矩阵上分别输出0，data[0*5+1]，data[0*5+3]，即分别填入0,2,4。然后，input_row变成2，input_col分别为-1，1和3时，输出矩阵上分别输出0，data[2*5+1]，data[2*5+3]，即分别填入0,12,14。然后，input_row变成4，input_col分别为-1，1，3时，输出矩阵上分别输出0，data[4*5+1]，data[4*5+3]，即分别输出0,22,24。见下图：（将第一个通道的卷积对应第四个对应位置进放好）</p><p><img src="/2019/04/12/img2col/17.png" alt></p><p>然后，kernel_row为1，kernel_col变成1。计算input_row为0，input_col分别为0(-1+1)，2(-1+1+2)和4(-1+1+2+2)，输出矩阵上分别输出data[0*5+0]，data[0*5+2]，data[0*5+4]，即分别填入1,3,5。然后，input_row变成2，input_col分别为0，2和4时，输出矩阵上分别输出data[2*5+0]，data[2*5+2]，data[2*5+4]，即分别填入11,13,15。然后，input_row变成4，input_col分别为0，2，4时，输出矩阵上分别输出data[4*5+0]，data[4*5+2]，data[4*5+4]，即分别输出21,23,25。见下图：（将第一个通道的卷积对应第五个对应位置进放好）</p><p><img src="/2019/04/12/img2col/18.png" alt></p><p>然后，kernel_row为1，kernel_col变成2。计算input_row为0，input_col分别为1(-1+2)，3(-1+2+2)和5(-1+2+2+2)，输出矩阵上分别输出data[0*5+1]，data[0*5+3]，0，即分别填入2,4,0。然后，input_row变成2，input_col分别为1，3和5时，输出矩阵上分别输出data[2*5+1]，data[2*5+3]，0，即分别填入12,14,0。然后，input_row变成4，input_col分别为1，3，5时，输出矩阵上分别输出data[4*5+1]，data[4*5+3]，0，即分别输出22,24,0。见下图：（将第一个通道的卷积对应第六个对应位置进放好）</p><p><img src="/2019/04/12/img2col/19.png" alt></p><p>   接着，kernel_row变成2，kernel_col变成0。计算input_row为1，input_col分别为-1(-1+0)，1(-1+0+2)和3(-1+0+2+2)，输出矩阵上分别输出0，data[1*5+1]，data[1*5+3]，即分别填入0,7,9。然后，input_row变成3，input_col分别为-1，1和3时，输出矩阵上分别输出0，data[3*5+1]，data[3*5+3]，即分别填入0,17,19。然后，input_row变成5，满足第一个if条件，直接输出三个0。见下图：（**将第一个通道的卷积对应第七个对应位置进放好）</p><p><img src="/2019/04/12/img2col/20.png" alt="Center"></p><p>   然后，kernel_row为2，kernel_col变成1。计算input_row为1，input_col分别为0(-1+1)，2(-1+1+2)和4(-1+1+2+2)，输出矩阵上分别输出data[1*5+0]，data[1*5+2]，data[1*5+4]，即分别填入6,8,10。然后，input_row变成3，input_col分别为0，2和4时，输出矩阵上分别输出data[3*5+0]，data[3*5+2]，data[3*5+4]，即分别填入16,18,20。然后，input_row变成5，满足第一个if条件，直接输出三个0。见下图：（将第一个通道的卷积对应第八个对应位置进放好）</p><p><img src="/2019/04/12/img2col/21.png" alt></p><p>   最后，kernel_row为2，kernel_col变成2。计算input_row为1，input_col分别为1(-1+2)，3(-1+2+2)和5(-1+2+2+2)，输出矩阵上分别输出data[1*5+1]，data[1*5+3]，0，即分别填入7,9,0。然后，input_row变成3，input_col分别为1，3和5时，输出矩阵上分别输出data[3*5+1]，data[3*5+3]，0，即分别填入17,19,0。然后，input_row变成5，满足第一个if条件，直接输出三个0。见下图：（将第一个通道的卷积对应第五个对应位置进放好）</p><p><img src="/2019/04/12/img2col/22.png" alt></p><p>   到此卷积层单通道输入特征图就转化成了一个矩阵，请读者朋友们仔细看看，矩阵的各列就是卷积核操作的各小窗口。</p><p><img src="/2019/04/12/img2col/23.png" alt></p><p><strong>!!! 注意点：</strong></p><p><strong>（1）如果是多个通道的话，就要将其他通道放置在这个通道的下面，最终的结果是生成矩阵的每一列对应一个卷积核(多个通道)。</strong></p><p><strong>（2）卷积中的zero-padding操作的实现，并不是真正在原始输入特征图周围添加0，而是在特征图转化得到的矩阵上的对应位置添加0。</strong></p><p><strong>（3）这种算法的核心点在于，先去找卷积核(单通道)对应所有卷积结果对应的某个位置的像素值，将其放置在一行，从而完整单通道的复制。当然也可以找到某个卷积核(单通道)对应的位置，将其拉伸为列。</strong></p><p><strong>而im2col_cpu函数功能的相反方向的实现则有由col2im_cpu函数完成，笔者依旧把该函数的代码注释放在下面：</strong></p><pre><code class="hljs cpp"><span class="hljs-comment">/*col2im_cpu为im2col_cpu的逆操作接收13个参数，分别为输入矩阵数据指针(data_col)，卷积操作处理的一个卷积组的通道 数(channels)，输入图像的高(height)与宽(width)，原始卷积核的高(kernel_h)与宽(kernel_w)， 输入图像高(pad_h)与宽(pad_w)方向的pad，卷积操作高(stride_h)与宽(stride_w)方向的步长， 卷积核高(stride_h)与宽(stride_h)方向的扩展，输出图像数据指针(data_im)*/</span>  <span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> Dtype&gt;  <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">col2im_cpu</span><span class="hljs-params">(<span class="hljs-keyword">const</span> Dtype* data_col, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> channels,  </span></span><span class="hljs-function"><span class="hljs-params">    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> height, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> width, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> kernel_h, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> kernel_w,  </span></span><span class="hljs-function"><span class="hljs-params">    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> pad_h, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> pad_w,  </span></span><span class="hljs-function"><span class="hljs-params">    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> stride_h, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> stride_w,  </span></span><span class="hljs-function"><span class="hljs-params">    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> dilation_h, <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> dilation_w,  </span></span><span class="hljs-function"><span class="hljs-params">    Dtype* data_im)</span> </span>&#123;      caffe_set(height * width * channels, Dtype(<span class="hljs-number">0</span>), data_im);   <span class="hljs-comment">//首先对输出的区域进行初始化，全部填充0  </span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> output_h = (height + <span class="hljs-number">2</span> * pad_h - (dilation_h * (kernel_h - <span class="hljs-number">1</span>) + <span class="hljs-number">1</span>)) / stride_h + <span class="hljs-number">1</span>;  <span class="hljs-comment">//计算卷积层输出图像的宽  </span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> output_w = (width + <span class="hljs-number">2</span> * pad_w - (dilation_w * (kernel_w - <span class="hljs-number">1</span>) + <span class="hljs-number">1</span>)) / stride_w + <span class="hljs-number">1</span>;  <span class="hljs-comment">//计算卷积层输出图像的高  </span>    <span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> channel_size = height * width;   <span class="hljs-comment">//col2im输出的单通道图像容量</span>      <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> channel = channels; channel--; data_im += channel_size) &#123;<span class="hljs-comment">//按照输出通道数一个一个处理  </span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> kernel_row = <span class="hljs-number">0</span>; kernel_row &lt; kernel_h; kernel_row++) &#123;              <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> kernel_col = <span class="hljs-number">0</span>; kernel_col &lt; kernel_w; kernel_col++) &#123;              <span class="hljs-keyword">int</span> input_row = -pad_h + kernel_row * dilation_h;<span class="hljs-comment">//在这里找到卷积核中的某一行在输入图像中的第一个操作区域的行索引  </span>         <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> output_rows = output_h; output_rows; output_rows--) &#123;               <span class="hljs-keyword">if</span> (!is_a_ge_zero_and_a_lt_b(input_row, height)) &#123;<span class="hljs-comment">//如果计算得到的输入图像的行值索引小于零或者大于输入图像的高(该行为pad)  </span>                data_col += output_w;  <span class="hljs-comment">//那么，直接跳过这output_w个数，这些数是输入图像第一行上面或者最后一行下面pad的0  </span>             &#125; <span class="hljs-keyword">else</span> &#123;                   <span class="hljs-keyword">int</span> input_col = -pad_w + kernel_col * dilation_w;<span class="hljs-comment">//在这里找到卷积核中的某一列在输入图像中的第一个操作区域的列索引  </span>                 <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> output_col = output_w; output_col; output_col--) &#123;                        <span class="hljs-keyword">if</span> (is_a_ge_zero_and_a_lt_b(input_col, width)) &#123;<span class="hljs-comment">//如果计算得到的输入图像的列值索引大于等于于零或者小于输入图像的宽(该列不是pad)  </span>                      data_im[input_row * width + input_col] += *data_col;<span class="hljs-comment">//将矩阵上对应的元放到将要输出的图像上  </span>                 &#125; <span class="hljs-comment">//这里没有else，因为如果紧挨的if条件不成立的话，input_row*width + input_col这个下标在data_im中不存在，同时遍历到data_col的对应元为0  </span>                 data_col++;<span class="hljs-comment">//遍历下一个data_col中的数  </span>                 input_col += stride_w;<span class="hljs-comment">//按照宽方向步长遍历卷积核上固定列在输入图像上滑动操作的区域  </span>             &#125;            &#125;            input_row += stride_h;<span class="hljs-comment">//按照高方向步长遍历卷积核上固定行在输入图像上滑动操作的区域  </span>             &#125;              &#125;          &#125;      &#125;  &#125;</code></pre>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>常用开发工具</title>
    <link href="/2019/04/10/Dev-tools/"/>
    <url>/2019/04/10/Dev-tools/</url>
    
    <content type="html"><![CDATA[<h4 id="1-Mac-必备软件"><a href="#1-Mac-必备软件" class="headerlink" title="1. Mac 必备软件"></a>1. Mac 必备软件</h4><h5 id="快捷键查询：cheat-sheet"><a href="#快捷键查询：cheat-sheet" class="headerlink" title="快捷键查询：cheat sheet"></a>快捷键查询：cheat sheet</h5><h5 id="软件安装与下载：brew"><a href="#软件安装与下载：brew" class="headerlink" title="软件安装与下载：brew"></a>软件安装与下载：brew</h5><h5 id="Github：Github-Desktop"><a href="#Github：Github-Desktop" class="headerlink" title="Github：Github Desktop"></a>Github：Github Desktop</h5><h5 id="图片标记：-圈点"><a href="#图片标记：-圈点" class="headerlink" title="图片标记： 圈点"></a>图片标记： 圈点</h5><h5 id="远程会议：-TeamViewer"><a href="#远程会议：-TeamViewer" class="headerlink" title="远程会议： TeamViewer"></a>远程会议： TeamViewer</h5><h5 id="待办事项：Things3"><a href="#待办事项：Things3" class="headerlink" title="待办事项：Things3"></a>待办事项：Things3</h5><h5 id="防止待机软件：-caffeine"><a href="#防止待机软件：-caffeine" class="headerlink" title="防止待机软件： caffeine"></a>防止待机软件： caffeine</h5><h5 id="文档-API查询-Dash"><a href="#文档-API查询-Dash" class="headerlink" title="文档/API查询: Dash"></a>文档/API查询: Dash</h5><h5 id="护眼：Flux"><a href="#护眼：Flux" class="headerlink" title="护眼：Flux"></a>护眼：Flux</h5><h5 id="影音播放：IINA"><a href="#影音播放：IINA" class="headerlink" title="影音播放：IINA"></a>影音播放：IINA</h5><h5 id="下载-Motrix"><a href="#下载-Motrix" class="headerlink" title="下载 Motrix"></a>下载 Motrix</h5><h5 id="代码比对：-Beyond-compare"><a href="#代码比对：-Beyond-compare" class="headerlink" title="代码比对： Beyond compare"></a>代码比对： Beyond compare</h5><h4 id="2-Python"><a href="#2-Python" class="headerlink" title="2. Python"></a>2. Python</h4><h5 id="2-3版本管理-anaconda"><a href="#2-3版本管理-anaconda" class="headerlink" title="2/3版本管理: anaconda"></a>2/3版本管理: anaconda</h5><h5 id="Python-代码可视化工具-yapf"><a href="#Python-代码可视化工具-yapf" class="headerlink" title="Python 代码可视化工具 yapf"></a>Python 代码可视化工具 yapf</h5><h5 id="3-笔记类软件"><a href="#3-笔记类软件" class="headerlink" title="3. 笔记类软件"></a>3. 笔记类软件</h5><h5 id="1-Markdown：Typora"><a href="#1-Markdown：Typora" class="headerlink" title="1. Markdown：Typora"></a>1. Markdown：Typora</h5><h5 id="2-笔记软件：evernote、有道云笔记"><a href="#2-笔记软件：evernote、有道云笔记" class="headerlink" title="2. 笔记软件：evernote、有道云笔记"></a>2. 笔记软件：evernote、有道云笔记</h5><h5 id="3-pdf-阅读：PDF-Expert、margin-note3、adobe-reader、预览"><a href="#3-pdf-阅读：PDF-Expert、margin-note3、adobe-reader、预览" class="headerlink" title="3. pdf 阅读：PDF Expert、margin note3、adobe reader、预览"></a>3. pdf 阅读：PDF Expert、margin note3、adobe reader、预览</h5><h5 id="4-编辑器：-Sublime-Text3、Visual-Studio-Code"><a href="#4-编辑器：-Sublime-Text3、Visual-Studio-Code" class="headerlink" title="4. 编辑器： Sublime Text3、Visual Studio Code"></a>4. 编辑器： Sublime Text3、Visual Studio Code</h5><h5 id="5-论文编辑：Latex"><a href="#5-论文编辑：Latex" class="headerlink" title="5.论文编辑：Latex"></a>5.论文编辑：Latex</h5><h5 id="4-思维导图："><a href="#4-思维导图：" class="headerlink" title="4. 思维导图："></a>4. 思维导图：</h5><h5 id="Xmind-ZEN"><a href="#Xmind-ZEN" class="headerlink" title="Xmind ZEN"></a>Xmind ZEN</h5><h5 id="MindNode"><a href="#MindNode" class="headerlink" title="MindNode"></a>MindNode</h5><h4 id="chrome-插件"><a href="#chrome-插件" class="headerlink" title="chrome 插件"></a>chrome 插件</h4><h5 id="1-clip-to-evernote"><a href="#1-clip-to-evernote" class="headerlink" title="1. clip to evernote"></a>1. clip to evernote</h5><h5 id="2-Fireshot"><a href="#2-Fireshot" class="headerlink" title="2. Fireshot"></a>2. Fireshot</h5><h5 id="3-One-Tap"><a href="#3-One-Tap" class="headerlink" title="3. One Tap"></a>3. One Tap</h5><h5 id="4-Pocket"><a href="#4-Pocket" class="headerlink" title="4. Pocket"></a>4. Pocket</h5><h5 id="5-Adblock"><a href="#5-Adblock" class="headerlink" title="5. Adblock"></a>5. Adblock</h5>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>FDDB测评</title>
    <link href="/2019/04/06/FDDB-benchmark/"/>
    <url>/2019/04/06/FDDB-benchmark/</url>
    
    <content type="html"><![CDATA[<h4 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1. 准备工作"></a>1. 准备工作</h4><h5 id="（1）下载-FDDB-相关文件"><a href="#（1）下载-FDDB-相关文件" class="headerlink" title="（1）下载 FDDB 相关文件"></a>（1）下载 FDDB 相关文件</h5><ul><li>从 <a href="http://vis-www.cs.umass.edu/fddb/index.html">官网</a>下载 FDDB 数据集，解压得到 <strong>originalPics</strong> 文件夹，<strong>FDDB-folds</strong> 文件夹和 <strong>README.txt</strong> </li><li>到 <a href="http://vis-www.cs.umass.edu/fddb/results.html">results 页面</a> 下载评估程序，解压得到 evaluation 文件夹。</li></ul><h5 id="（2）准备-txt-文件"><a href="#（2）准备-txt-文件" class="headerlink" title="（2）准备 .txt 文件"></a>（2）准备 .txt 文件</h5><p>用你的模型按照 FDDB-folds/FDDB-fold-i.txt 的顺序检测图片，生成与之对应的 10 个 fold-i-out.txt，存放在 out-folds 文件夹中，并合并或复制粘贴成 results.txt。结果文件的格式需要为 </p><pre><code class="hljs tex">... image name i number of faces in this image =im face i1 face i2 ... face im ...</code></pre><h5 id="（3）安装-OPENCV（v3-2）"><a href="#（3）安装-OPENCV（v3-2）" class="headerlink" title="（3）安装 OPENCV（v3.2）"></a>（3）安装 OPENCV（v3.2）</h5><h5 id="（4）安装-gnuplot"><a href="#（4）安装-gnuplot" class="headerlink" title="（4）安装 gnuplot"></a>（4）安装 gnuplot</h5><p>Ubuntu 上执行如下命令：</p><pre><code class="hljs shell">sudo apt-get install gnuplot</code></pre><h4 id="2-编译-evalution"><a href="#2-编译-evalution" class="headerlink" title="2. 编译 evalution"></a>2. 编译 evalution</h4><h5 id="（1）修改Makefile-文件："><a href="#（1）修改Makefile-文件：" class="headerlink" title="（1）修改Makefile 文件："></a>（1）修改Makefile 文件：</h5><p>打开evalution 文件夹下的MakeFile， 添加如下相应的行</p><pre><code class="hljs makefile">INCS = -I/usr/local/<span class="hljs-keyword">include</span>/opencvLIBS = -L/usr/local/lib -lopencv_core -lopencv_imgproc -lopencv_highgui       -lopencv_ml -lopencv_video -lopencv_features2d -lopencv_calib3d       -lopencv_objdetect -lopencv_contrib -lopencv_legacy<span class="hljs-comment"># 对于 opencv 3.2 去掉 -lopencv_contrib and -lopencv_legacy  加上 -lopencv_imgcodecs</span></code></pre><p>修改如下行：</p><pre><code class="hljs gams"># evaluate: <span class="hljs-symbol">$</span>(OBJS)#        <span class="hljs-symbol">$</span>(CC) <span class="hljs-symbol">$</span>(LIBS) <span class="hljs-symbol">$</span>(OBJS) -o <span class="hljs-symbol">$</span>@evaluate: <span class="hljs-symbol">$</span>(OBJS)    <span class="hljs-symbol">$</span>(CC) <span class="hljs-symbol">$</span>(OBJS) -o <span class="hljs-symbol">$</span>@ <span class="hljs-symbol">$</span>(LIBS)</code></pre><h5 id="（2）修改-common-hpp"><a href="#（2）修改-common-hpp" class="headerlink" title="（2）修改 common.hpp"></a>（2）修改 common.hpp</h5><p>打开 evaluation 文件夹下的 common.hpp，将</p><pre><code class="hljs cpp"><span class="hljs-comment">// #define __IMAGE_FORMAT__ &quot;.jpg&quot;</span><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> __IMAGE_FORMAT__ <span class="hljs-meta-string">&quot;.ppm&quot;</span></span><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> __CVLOADIMAGE_WORKING__</span></code></pre><p>修改为：</p><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> __IMAGE_FORMAT__ <span class="hljs-meta-string">&quot;.jpg&quot;</span></span><span class="hljs-comment">// #define __IMAGE_FORMAT__ &quot;.ppm&quot;</span><span class="hljs-meta">#<span class="hljs-meta-keyword">define</span> __CVLOADIMAGE_WORKING__</span></code></pre><h5 id="（3）编译"><a href="#（3）编译" class="headerlink" title="（3）编译"></a>（3）编译</h5><pre><code class="hljs ebnf"><span class="hljs-attribute">make</span></code></pre><p>如果出现错误： <strong>fatal error</strong>: ‘cv.h’ file not found [Mac platform]</p><p><strong>solution</strong>: 找到对应文件：将其修改为</p><pre><code class="hljs cpp"><span class="hljs-comment">// #include &quot;opencv/cv.h&quot;</span><span class="hljs-comment">// #include &quot;opencv/highgui.h&quot;</span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;opencv/cv.h&quot;</span></span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&quot;opencv/highgui.h&quot;</span></span></code></pre><h4 id="3-输出文件并绘图"><a href="#3-输出文件并绘图" class="headerlink" title="3. 输出文件并绘图"></a>3. 输出文件并绘图</h4><h5 id="（1）修改-runEvaluate-pl"><a href="#（1）修改-runEvaluate-pl" class="headerlink" title="（1）修改 runEvaluate.pl"></a>（1）修改 runEvaluate.pl</h5><p>打开 evaluation 文件夹下的 runEvaluate.pl，填写路径如下：</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">where</span> gnuplot is</span>my $GNUPLOT = &quot;/usr/local/bin/gnuplot&quot;;<span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">where</span> the binary is</span>my $evaluateBin = &quot;./evaluate&quot;;<span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">where</span> the images are</span>my $imDir = &quot;../originalPics/&quot;;<span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">where</span> the folds are</span>my $fddbDir = &quot;../FDDB-folds&quot;;<span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">where</span> the detections are</span>my $detDir = &quot;../results&quot;;</code></pre><p>注意若文件名称和路径与笔者 (见第 2 步中的截图) 不同，需要相应修改。</p><h5 id="（2）执行-runEvaluate-pl"><a href="#（2）执行-runEvaluate-pl" class="headerlink" title="（2）执行 runEvaluate.pl"></a>（2）执行 runEvaluate.pl</h5><pre><code class="hljs reasonml">perl runEvaluate.pl 完成后在 results 文件夹下生成了 <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">ContROC</span>.</span></span>txt, <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">DiscROC</span>.</span></span>txt, <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">ContROC</span>.</span></span>png 和 <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">DiscROC</span>.</span></span>png 四个文件。</code></pre>]]></content>
    
    
    <categories>
      
      <category>Face</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>pytorch Cookbook</title>
    <link href="/2019/03/19/Pytorch-Cookbook/"/>
    <url>/2019/03/19/Pytorch-Cookbook/</url>
    
    <content type="html"><![CDATA[<p>🔥 一些 pytorch 编程的小技巧、trick 和 示例代码</p><a id="more"></a><p>本文代码基于PyTorch 1.0版本，需要用到以下包</p><pre><code class="hljs elm"><span class="hljs-keyword">import</span> collections<span class="hljs-keyword">import</span> os<span class="hljs-keyword">import</span> shutil<span class="hljs-keyword">import</span> tqdm<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> PIL.Image<span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torchvision</code></pre><h3 id="1-基础配置"><a href="#1-基础配置" class="headerlink" title="1. 基础配置"></a>1. 基础配置</h3><h5 id="1-check-pytorch-version"><a href="#1-check-pytorch-version" class="headerlink" title="(1) check pytorch version"></a>(1) check pytorch version</h5><pre><code class="hljs python">torch.__version__               <span class="hljs-comment"># PyTorch version</span>torch.version.cuda              <span class="hljs-comment"># Corresponding CUDA version</span>torch.backends.cudnn.version()  <span class="hljs-comment"># Corresponding cuDNN version</span>torch.cuda.get_device_name(<span class="hljs-number">0</span>)   <span class="hljs-comment"># GPU type</span></code></pre><h5 id="2-update-pytorch"><a href="#2-update-pytorch" class="headerlink" title="(2) update pytorch"></a>(2) update pytorch</h5><pre><code class="hljs ebnf"><span class="hljs-attribute">conda update pytorch torchvision -c pytorch</span></code></pre><h5 id="3-random-seed-setting"><a href="#3-random-seed-setting" class="headerlink" title="(3) random seed setting"></a>(3) random seed setting</h5><pre><code class="hljs css"><span class="hljs-selector-tag">torch</span><span class="hljs-selector-class">.manual_seed</span>(0) # <span class="hljs-selector-tag">CPU</span><span class="hljs-selector-tag">torch</span><span class="hljs-selector-class">.cuda</span><span class="hljs-selector-class">.manual_seed_all</span>(0) # <span class="hljs-selector-tag">GPU</span></code></pre><h5 id="4-指定程序运行在特定显卡上："><a href="#4-指定程序运行在特定显卡上：" class="headerlink" title="(4) 指定程序运行在特定显卡上："></a>(4) 指定程序运行在特定显卡上：</h5><p>在命令行指定环境变量</p><pre><code class="hljs angelscript">CUDA_VISIBLE_DEVICES=<span class="hljs-number">0</span>,<span class="hljs-number">1</span> python train.py</code></pre><p>在代码中指定</p><pre><code class="hljs lua"><span class="hljs-built_in">os</span>.environ[<span class="hljs-string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="hljs-string">&#x27;0,1&#x27;</span></code></pre><h5 id="5-判断是否有CUDA支持"><a href="#5-判断是否有CUDA支持" class="headerlink" title="(5) 判断是否有CUDA支持"></a>(5) 判断是否有CUDA支持</h5><pre><code class="hljs ceylon">torch.cuda.<span class="hljs-keyword">is</span><span class="hljs-number">_</span>available()torch.set<span class="hljs-number">_</span><span class="hljs-keyword">default</span><span class="hljs-number">_</span>tensor<span class="hljs-number">_</span>type(<span class="hljs-string">&#x27;torch.cuda.FloatTensor&#x27;</span>)   os.environ[<span class="hljs-string">&#x27;CUDA_LAUNCH_BLOCKING&#x27;</span>] = <span class="hljs-string">&#x27;1&#x27;</span></code></pre><h5 id="6-设置为cuDNN-benchmark模式"><a href="#6-设置为cuDNN-benchmark模式" class="headerlink" title="(6) 设置为cuDNN benchmark模式"></a>(6) 设置为cuDNN benchmark模式</h5><p>Benchmark模式会提升计算速度，但是由于计算中有随机性，每次网络前馈结果略有差异。</p><pre><code class="hljs ini"><span class="hljs-attr">toch.backends.cudnn.benchmark</span> = <span class="hljs-literal">True</span></code></pre><p>如果想要避免这种结果波动，设置</p><pre><code class="hljs ini"><span class="hljs-attr">torch.backends.cudnn.deterministic</span> = <span class="hljs-literal">True</span></code></pre><h5 id="7-手动清除GPU存储"><a href="#7-手动清除GPU存储" class="headerlink" title="(7) 手动清除GPU存储"></a>(7) 手动清除GPU存储</h5><p>有时Control-C中止运行后GPU存储没有及时释放，需要手动清空。在PyTorch内部可以</p><pre><code class="hljs css"><span class="hljs-selector-tag">torch</span><span class="hljs-selector-class">.cuda</span><span class="hljs-selector-class">.empty_cache</span>()</code></pre><p>或在命令行可以先使用ps找到程序的PID，再使用kill结束该进程</p><pre><code class="hljs vim"><span class="hljs-keyword">ps</span> aux | <span class="hljs-keyword">grep</span> <span class="hljs-keyword">python</span>    kill -<span class="hljs-number">9</span> [pid]</code></pre><p>或者直接重置没有被清空的GPU</p><pre><code class="hljs ada">nvidia-smi <span class="hljs-comment">--gpu-reset -i [gpu_id]</span></code></pre><h3 id="2-模型"><a href="#2-模型" class="headerlink" title="2. 模型"></a>2. 模型</h3><h5 id="1-提取ImageNet预训练模型某层的卷积特征"><a href="#1-提取ImageNet预训练模型某层的卷积特征" class="headerlink" title="(1) 提取ImageNet预训练模型某层的卷积特征"></a>(1) 提取ImageNet预训练模型某层的卷积特征</h5><pre><code class="hljs gams"># VGG<span class="hljs-number">-16</span> relu5<span class="hljs-number">-3</span> feature.<span class="hljs-keyword">model</span> = torchvision.<span class="hljs-keyword">models</span>.vgg16(pretrained=True).features# VGG<span class="hljs-number">-16</span> pool5 feature.<span class="hljs-keyword">model</span> = torchvision.<span class="hljs-keyword">models</span>.vgg16(pretrained=True)<span class="hljs-keyword">model</span> = torch.nn.Sequential(<span class="hljs-keyword">model</span>.features, <span class="hljs-keyword">model</span>.avgpool)# VGG<span class="hljs-number">-16</span> fc7 feature.<span class="hljs-keyword">model</span> = torchvision.<span class="hljs-keyword">models</span>.vgg16(pretrained=True)<span class="hljs-keyword">model</span>.classifier = torch.nn.Sequential(*list(<span class="hljs-keyword">model</span>.classifier.children())[:<span class="hljs-number">-3</span>])# ResNet GAP feature.<span class="hljs-keyword">model</span> = torchvision.<span class="hljs-keyword">models</span>.resnet18(pretrained=True)<span class="hljs-keyword">model</span> = torch.nn.Sequential(collections.OrderedDict(    list(<span class="hljs-keyword">model</span>.named_children())[:<span class="hljs-number">-1</span>]))with torch.no_grad():    <span class="hljs-keyword">model</span>.eval()    conv_representation = <span class="hljs-keyword">model</span>(image)</code></pre><h5 id="2-提取ImageNet预训练模型多层的卷积特征"><a href="#2-提取ImageNet预训练模型多层的卷积特征" class="headerlink" title="(2) 提取ImageNet预训练模型多层的卷积特征"></a>(2) 提取ImageNet预训练模型多层的卷积特征</h5><pre><code class="hljs vim">class FeatureExtractor(torch.<span class="hljs-keyword">nn</span>.Module):    <span class="hljs-string">&quot;&quot;</span><span class="hljs-comment">&quot;Helper class to extract several convolution features from the given</span>    <span class="hljs-keyword">pre</span>-trained model.    Attribute<span class="hljs-variable">s:</span>        _model, torch.<span class="hljs-keyword">nn</span>.Module.        _layers_to_extract, <span class="hljs-keyword">list</span><span class="hljs-symbol">&lt;str&gt;</span> <span class="hljs-built_in">or</span> <span class="hljs-keyword">set</span><span class="hljs-symbol">&lt;str&gt;</span>    Example:        &gt;&gt;&gt; model = torchvision.models.resnet152(pretrained=True)        &gt;&gt;&gt; model = torch.<span class="hljs-keyword">nn</span>.Sequential(collections.OrderedDict(                <span class="hljs-keyword">list</span>(model.named_children())[:-<span class="hljs-number">1</span>]))        &gt;&gt;&gt; conv_representation = FeatureExtractor(                pretrained_model=model,                layers_to_extract=&#123;<span class="hljs-string">&#x27;layer1&#x27;</span>, <span class="hljs-string">&#x27;layer2&#x27;</span>, <span class="hljs-string">&#x27;layer3&#x27;</span>, <span class="hljs-string">&#x27;layer4&#x27;</span>&#125;)(image)    <span class="hljs-string">&quot;&quot;</span><span class="hljs-comment">&quot;</span>    def __init__(self, pretrained_model, layers_to_extract):        torch.<span class="hljs-keyword">nn</span>.Module.__init__(self)        self._model = pretrained_model        self._model.<span class="hljs-built_in">eval</span>()        self._layers_to_extract = <span class="hljs-keyword">set</span>(layers_to_extract)        def forward(self, <span class="hljs-keyword">x</span>):        with torch.no_grad():            conv_representation = []            <span class="hljs-keyword">for</span> name, layer in self._model.named_children():                <span class="hljs-keyword">x</span> = layer(<span class="hljs-keyword">x</span>)                <span class="hljs-keyword">if</span> name in self._layers_to_extrac<span class="hljs-variable">t:</span>                    conv_representation.<span class="hljs-keyword">append</span>(<span class="hljs-keyword">x</span>)            <span class="hljs-keyword">return</span> conv_representation</code></pre><h5 id="３-部分层使用预训练模型"><a href="#３-部分层使用预训练模型" class="headerlink" title="(３)  部分层使用预训练模型"></a>(３)  部分层使用预训练模型</h5><p>注意如果保存的模型是<code>torch.nn.DataParallel</code>，则当前的模型也需要是<code>torch.nn.DataParallel</code>。<code>torch.nn.DataParallel(model).module == model</code>。</p><pre><code class="hljs reasonml">model.load<span class="hljs-constructor">_state_dict(<span class="hljs-params">torch</span>.<span class="hljs-params">load</span>(&#x27;<span class="hljs-params">model</span>,<span class="hljs-params">pth</span>&#x27;)</span>, strict=False)</code></pre><p>将在GPU保存的模型加载到CPU:</p><pre><code class="hljs reasonml">model.load<span class="hljs-constructor">_state_dict(<span class="hljs-params">torch</span>.<span class="hljs-params">load</span>(&#x27;<span class="hljs-params">model</span>,<span class="hljs-params">pth</span>&#x27;, <span class="hljs-params">map_location</span>=&#x27;<span class="hljs-params">cpu</span>&#x27;)</span>)</code></pre><h5 id="（４）fine-tune-微调全连接层"><a href="#（４）fine-tune-微调全连接层" class="headerlink" title="（４）fine-tune 微调全连接层"></a>（４）fine-tune 微调全连接层</h5><h5 id="4-微调全连接层"><a href="#4-微调全连接层" class="headerlink" title="(4) 微调全连接层"></a>(4) 微调全连接层</h5><pre><code class="hljs nix"><span class="hljs-attr">model</span> = torchvision.models.resnet18(<span class="hljs-attr">pretrained=True)</span>for param <span class="hljs-keyword">in</span> model.parameters():    param.<span class="hljs-attr">requires_grad</span> = Falsemodel.<span class="hljs-attr">fc</span> = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">100</span>)  <span class="hljs-comment"># Replace the last fc layer</span><span class="hljs-attr">optimizer</span> = torch.optim.SGD(model.fc.parameters(), <span class="hljs-attr">lr=1e-2,</span> <span class="hljs-attr">momentum=0.9,</span> <span class="hljs-attr">weight_decay=1e-4)</span></code></pre><p>以较大学习率微调全连接层，较小学习率微调卷积层</p><pre><code class="hljs ini"><span class="hljs-attr">model</span> = torchvision.models.resnet18(pretrained=<span class="hljs-literal">True</span>)<span class="hljs-attr">finetuned_parameters</span> = list(map(id, model.fc.parameters()))<span class="hljs-attr">conv_parameters</span> = (p for p in model.parameters() if id(p) not in finetuned_parameters)<span class="hljs-attr">parameters</span> = [&#123;<span class="hljs-string">&#x27;parameters&#x27;</span>: conv_parameters, <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">1</span>e-<span class="hljs-number">3</span>&#125;,               &#123;<span class="hljs-string">&#x27;parameters&#x27;</span>: model.fc.parameters()&#125;]<span class="hljs-attr">optimizer</span> = torch.optim.SGD(parameters, lr=<span class="hljs-number">1</span>e-<span class="hljs-number">2</span>, momentum=<span class="hljs-number">0.9</span>, weight_decay=<span class="hljs-number">1</span>e-<span class="hljs-number">4</span>)</code></pre><h5 id="（５）保存与加载断点"><a href="#（５）保存与加载断点" class="headerlink" title="（５）保存与加载断点"></a>（５）保存与加载断点</h5><p>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。</p><pre><code class="hljs lua"># Save checkpoint.is_best = current_acc &gt; best_accbest_acc = <span class="hljs-built_in">max</span>(best_acc, current_acc)checkpoint = &#123;    <span class="hljs-string">&#x27;best_acc&#x27;</span>: best_acc,        <span class="hljs-string">&#x27;epoch&#x27;</span>: t + <span class="hljs-number">1</span>,    <span class="hljs-string">&#x27;model&#x27;</span>: model.state_dict(),    <span class="hljs-string">&#x27;optimizer&#x27;</span>: optimizer.state_dict(),&#125;model_path = <span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-string">&#x27;model&#x27;</span>, <span class="hljs-string">&#x27;checkpoint.pth.tar&#x27;</span>)torch.save(checkpoint, model_path)<span class="hljs-keyword">if</span> is_best:    shutil.copy(<span class="hljs-string">&#x27;checkpoint.pth.tar&#x27;</span>, model_path)# Load checkpoint.<span class="hljs-keyword">if</span> <span class="hljs-built_in">resume</span>:    model_path = <span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-string">&#x27;model&#x27;</span>, <span class="hljs-string">&#x27;checkpoint.pth.tar&#x27;</span>)    <span class="hljs-built_in">assert</span> <span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.isfile(model_path)    checkpoint = torch.<span class="hljs-built_in">load</span>(model_path)    best_acc = checkpoint[<span class="hljs-string">&#x27;best_acc&#x27;</span>]    start_epoch = checkpoint[<span class="hljs-string">&#x27;epoch&#x27;</span>]    model.load_state_dict(checkpoint[<span class="hljs-string">&#x27;model&#x27;</span>])    optimizer.load_state_dict(checkpoint[<span class="hljs-string">&#x27;optimizer&#x27;</span>])    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Load checkpoint at epoch %d.&#x27;</span> % start_epoch)</code></pre><h5 id="６-计算模型参数量-D"><a href="#６-计算模型参数量-D" class="headerlink" title="(６) 计算模型参数量[D]"></a>(６) 计算模型参数量[D]</h5><pre><code class="hljs lisp"># Total parameters                    num_params = sum(<span class="hljs-name">p</span>.numel() for p in model.parameters()) # Trainable parametersnum_trainable_params = sum(<span class="hljs-name">p</span>.numel() for p in model.parameters() if p.requires_grad)</code></pre><h5 id="７-模型权值初始化-D"><a href="#７-模型权值初始化-D" class="headerlink" title="(７) 模型权值初始化[D]"></a>(７) 模型权值初始化[D]</h5><p>注意<code>model.modules()</code>和<code>model.children()</code>的区别：<code>model.modules()</code>会迭代地遍历模型的所有子层，而<code>model.children()</code>只会遍历模型下的一层。</p><pre><code class="hljs python"><span class="hljs-comment"># Common practise for initialization.</span><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> model.modules():    <span class="hljs-keyword">if</span> isinstance(m, torch.nn.Conv2d):        torch.nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>,                                      nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)        <span class="hljs-keyword">if</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:            torch.nn.init.constant_(m.bias, val=<span class="hljs-number">0.0</span>)        <span class="hljs-keyword">elif</span> isinstance(m, torch.nn.BatchNorm2d):        torch.nn.init.constant_(m.weight, <span class="hljs-number">1.0</span>)        torch.nn.init.constant_(m.bias, <span class="hljs-number">0.0</span>)      <span class="hljs-keyword">elif</span> isinstance(m, torch.nn.Linear):        torch.nn.init.xavier_normal_(m.weight)        <span class="hljs-keyword">if</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:            torch.nn.init.constant_(m.bias, <span class="hljs-number">0.0</span>)<span class="hljs-comment"># Initialization with given tensor.</span>m.weight = torch.nn.Parameter(tensor)</code></pre><h5 id="8-冻结参数"><a href="#8-冻结参数" class="headerlink" title="(8) 冻结参数"></a>(8) 冻结参数</h5><pre><code class="hljs sqf"><span class="hljs-keyword">if</span> <span class="hljs-built_in">not</span> requires_grad:    <span class="hljs-keyword">for</span> <span class="hljs-built_in">param</span> <span class="hljs-built_in">in</span> self.parameters():        <span class="hljs-built_in">param</span>.requires_grad = <span class="hljs-literal">False</span></code></pre><h3 id="3-数据"><a href="#3-数据" class="headerlink" title="3. 数据"></a>3. 数据</h3><h5 id="1-常见训练和验证数据预处理"><a href="#1-常见训练和验证数据预处理" class="headerlink" title="(1) 常见训练和验证数据预处理"></a>(1) 常见训练和验证数据预处理</h5><p>ToTensor操作会将PIL.Image或形状为H×W×D，数值范围为[0, 255]的np.ndarray转换为形状为D×H×W，数值范围为[0.0, 1.0]的torch.Tensor。</p><pre><code class="hljs angelscript">train_transform = torchvision.transforms.Compose([    torchvision.transforms.RandomResizedCrop(size=<span class="hljs-number">224</span>,                                             scale=(<span class="hljs-number">0.08</span>, <span class="hljs-number">1.0</span>)),    torchvision.transforms.RandomHorizontalFlip(),    torchvision.transforms.ToTensor(),    torchvision.transforms.Normalize(mean=(<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>),                                     std=(<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>)), ]) val_transform = torchvision.transforms.Compose([    torchvision.transforms.Resize(<span class="hljs-number">224</span>),    torchvision.transforms.CenterCrop(<span class="hljs-number">224</span>),    torchvision.transforms.ToTensor(),    torchvision.transforms.Normalize(mean=(<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>),                                     std=(<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>)),])</code></pre><h3 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h3><h5 id="1-将整数标记转换成独热（one-hot）编码"><a href="#1-将整数标记转换成独热（one-hot）编码" class="headerlink" title="(1) 将整数标记转换成独热（one-hot）编码"></a>(1) 将整数标记转换成独热（one-hot）编码</h5><p> (PyTorch中的标记默认从0开始)</p><pre><code class="hljs routeros">N = tensor.size(0)one_hot = torch.zeros(N, num_classes).long()one_hot.scatter_(<span class="hljs-attribute">dim</span>=1, <span class="hljs-attribute">index</span>=torch.unsqueeze(tensor, <span class="hljs-attribute">dim</span>=1), <span class="hljs-attribute">src</span>=torch.ones(N, num_classes).long())</code></pre><h5 id="2-计算两组数据之间的两两欧式距离"><a href="#2-计算两组数据之间的两两欧式距离" class="headerlink" title="(2) 计算两组数据之间的两两欧式距离"></a>(2) 计算两组数据之间的两两欧式距离</h5><pre><code class="hljs markdown"><span class="hljs-section"># X1 is of shape m<span class="hljs-emphasis">*d.</span></span><span class="hljs-section"><span class="hljs-emphasis">X1 = torch.unsqueeze(X1, dim=1).expand(m, n, d)</span></span><span class="hljs-section"><span class="hljs-emphasis"># X2 is of shape n*</span>d.</span>X2 = torch.unsqueeze(X2, dim=0).expand(m, n, d)<span class="hljs-section"># dist is of shape m<span class="hljs-emphasis">*n, where dist[<span class="hljs-string">i</span>][<span class="hljs-symbol">j</span>] = sqrt(|X1[i, :] - X[j, :]|^2)</span></span><span class="hljs-section"><span class="hljs-emphasis">dist = torch.sqrt(torch.sum((X1 - X2) <span class="hljs-strong">** 2, dim=2))</span></span></span></code></pre><h5 id="3-双线性汇合（bilinear-pooling）"><a href="#3-双线性汇合（bilinear-pooling）" class="headerlink" title="(3) 双线性汇合（bilinear pooling）"></a>(3) 双线性汇合（bilinear pooling）</h5><pre><code class="hljs tp"><span class="hljs-keyword">X</span> = torch.reshape(N, D, H * <span class="hljs-keyword">W</span>)                        # Assume <span class="hljs-keyword">X</span> has shape N*D*H*<span class="hljs-keyword">W</span><span class="hljs-keyword">X</span> = torch.bmm(<span class="hljs-keyword">X</span>, torch.transpose(<span class="hljs-keyword">X</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)) / (H * <span class="hljs-keyword">W</span>)  # Bilinear poolingassert <span class="hljs-keyword">X</span>.size() == (N, D, D)<span class="hljs-keyword">X</span> = torch.reshape(<span class="hljs-keyword">X</span>, (N, D * D))<span class="hljs-keyword">X</span> = torch.sign(<span class="hljs-keyword">X</span>) * torch.sqrt(torch.abs(<span class="hljs-keyword">X</span>) + <span class="hljs-number">1e-5</span>)   # Signed-sqrt normalization<span class="hljs-keyword">X</span> = torch.nn.functional.normalize(<span class="hljs-keyword">X</span>)                  # L<span class="hljs-number">2</span> normalization</code></pre><h5 id="4-L1-正则化"><a href="#4-L1-正则化" class="headerlink" title="(4) L1 正则化"></a>(4) L1 正则化</h5><pre><code class="hljs gams">l1_regularization = torch.nn.L1Loss(reduction=<span class="hljs-string">&#x27;sum&#x27;</span>)loss = ...  # Standard cross-<span class="hljs-built_in">entropy</span> loss<span class="hljs-keyword">for</span> param in <span class="hljs-keyword">model</span>.parameters():    loss += torch.<span class="hljs-keyword">sum</span>(torch.<span class="hljs-built_in">abs</span>(param))loss.backward()reg = <span class="hljs-number">1e-6</span>l2_loss = <span class="hljs-keyword">Variable</span>(torch.FloatTensor(1), requires_grad=True)for <span class="hljs-comment">name, param in model.named_parameters():</span>    if <span class="hljs-comment">&#x27;bias&#x27;</span><span class="hljs-comment"> not in name:</span>        l2_loss <span class="hljs-comment">= l2_loss + (0.5 * reg * torch.sum(torch.pow(W, 2)))</span></code></pre><h5 id="5-不对偏置项进行L2正则化-权值衰减（weight-decay）"><a href="#5-不对偏置项进行L2正则化-权值衰减（weight-decay）" class="headerlink" title="(5) 不对偏置项进行L2正则化/权值衰减（weight decay）"></a>(5) 不对偏置项进行L2正则化/权值衰减（weight decay）</h5><pre><code class="hljs ini"><span class="hljs-attr">bias_list</span> = (param for name, param in model.named_parameters() if name[-<span class="hljs-number">4</span>:] == <span class="hljs-string">&#x27;bias&#x27;</span>)<span class="hljs-attr">others_list</span> = (param for name, param in model.named_parameters() if name[-<span class="hljs-number">4</span>:] != <span class="hljs-string">&#x27;bias&#x27;</span>)<span class="hljs-attr">parameters</span> = [&#123;<span class="hljs-string">&#x27;parameters&#x27;</span>: bias_list, <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0</span>&#125;,                              &#123;<span class="hljs-string">&#x27;parameters&#x27;</span>: others_list&#125;]<span class="hljs-attr">optimizer</span> = torch.optim.SGD(parameters, lr=<span class="hljs-number">1</span>e-<span class="hljs-number">2</span>, momentum=<span class="hljs-number">0.9</span>, weight_decay=<span class="hljs-number">1</span>e-<span class="hljs-number">4</span>)</code></pre><h5 id="6-梯度裁剪（gradient-clipping）"><a href="#6-梯度裁剪（gradient-clipping）" class="headerlink" title="(6) 梯度裁剪（gradient clipping）"></a>(6) 梯度裁剪（gradient clipping）</h5> <pre><code class="hljs reasonml">torch.nn.utils.clip<span class="hljs-constructor">_grad_norm_(<span class="hljs-params">model</span>.<span class="hljs-params">parameters</span>()</span>, max_norm=<span class="hljs-number">20</span>)</code></pre><h5 id="7-计算Softmax-输出的正确率"><a href="#7-计算Softmax-输出的正确率" class="headerlink" title="(7) 计算Softmax 输出的正确率"></a>(7) 计算Softmax 输出的正确率</h5><pre><code class="hljs ini"><span class="hljs-attr">score</span> = model(images)<span class="hljs-attr">prediction</span> = torch.argmax(score, dim=<span class="hljs-number">1</span>)<span class="hljs-attr">num_correct</span> = torch.sum(prediction == labels).item()<span class="hljs-attr">accuruacy</span> = num_correct / labels.size(<span class="hljs-number">0</span>)</code></pre><h5 id="8-获取当前学习率"><a href="#8-获取当前学习率" class="headerlink" title="(8) 获取当前学习率"></a>(8) 获取当前学习率</h5><pre><code class="hljs vim"># If there <span class="hljs-keyword">is</span> one <span class="hljs-keyword">global</span> learning rate (which <span class="hljs-keyword">is</span> the common case).<span class="hljs-keyword">lr</span> = <span class="hljs-keyword">next</span>(iter(optimizer.param_groups))[<span class="hljs-string">&#x27;lr&#x27;</span>]# If there are multiple learning rates <span class="hljs-keyword">for</span> different layers.all_lr = []<span class="hljs-keyword">for</span> param_group in optimizer.param_group<span class="hljs-variable">s:</span>    all_lr.<span class="hljs-keyword">append</span>(param_group[<span class="hljs-string">&#x27;lr&#x27;</span>])</code></pre><h3 id="5-Trick"><a href="#5-Trick" class="headerlink" title="5. Trick"></a>5. Trick</h3><h5 id="1-label-smothing"><a href="#1-label-smothing" class="headerlink" title="(1)  label smothing"></a>(1)  label smothing</h5><pre><code class="hljs nix">for images, labels <span class="hljs-keyword">in</span> train_loader:    images, <span class="hljs-attr">labels</span> = images.cuda(), labels.cuda()    <span class="hljs-attr">N</span> = labels.size(<span class="hljs-number">0</span>)    <span class="hljs-comment"># C is the number of classes.</span>    <span class="hljs-attr">smoothed_labels</span> = torch.full(<span class="hljs-attr">size=(N,</span> C), <span class="hljs-attr">fill_value=0.1</span> / (C - <span class="hljs-number">1</span>)).cuda()    smoothed_labels.scatter_(<span class="hljs-attr">dim=1,</span> <span class="hljs-attr">index=torch.unsqueeze(labels,</span> <span class="hljs-attr">dim=1),</span> <span class="hljs-attr">value=0.9)</span>    <span class="hljs-attr">score</span> = model(images)    <span class="hljs-attr">log_prob</span> = torch.nn.functional.log_softmax(score, <span class="hljs-attr">dim=1)</span>    <span class="hljs-attr">loss</span> = -torch.sum(log_prob * smoothed_labels) / N    optimizer.zero_grad()    loss.backward()    optimizer.step()</code></pre><h5 id="2-Mixup"><a href="#2-Mixup" class="headerlink" title="(2) Mixup"></a>(2) Mixup</h5><pre><code class="hljs reasonml">beta_distribution = torch.distributions.beta.<span class="hljs-constructor">Beta(<span class="hljs-params">alpha</span>, <span class="hljs-params">alpha</span>)</span><span class="hljs-keyword">for</span> images, labels <span class="hljs-keyword">in</span> train_loader:    images, labels = images.cuda<span class="hljs-literal">()</span>, labels.cuda<span class="hljs-literal">()</span>    # Mixup images.    lambda_ = beta_distribution.sample(<span class="hljs-literal">[]</span>).item<span class="hljs-literal">()</span>    index = torch.randperm(images.size(<span class="hljs-number">0</span>)).cuda<span class="hljs-literal">()</span>    mixed_images = lambda_<span class="hljs-operator"> * </span>images + (<span class="hljs-number">1</span> - lambda_)<span class="hljs-operator"> * </span>images<span class="hljs-literal">[<span class="hljs-identifier">index</span>, :]</span>    # Mixup loss.        scores = model(mixed_images)    loss = (lambda_<span class="hljs-operator"> * </span>loss<span class="hljs-constructor">_function(<span class="hljs-params">scores</span>, <span class="hljs-params">labels</span>)</span>             + (<span class="hljs-number">1</span> - lambda_)<span class="hljs-operator"> * </span>loss<span class="hljs-constructor">_function(<span class="hljs-params">scores</span>, <span class="hljs-params">labels</span>[<span class="hljs-params">index</span>])</span>)    optimizer.zero<span class="hljs-constructor">_grad()</span>    loss.backward<span class="hljs-literal">()</span>    optimizer.step<span class="hljs-literal">()</span></code></pre><h5 id="3-多卡同步BN（Batch-normalization）"><a href="#3-多卡同步BN（Batch-normalization）" class="headerlink" title="(3) 多卡同步BN（Batch normalization）"></a>(3) 多卡同步BN（Batch normalization）</h5><p>当使用torch.nn.DataParallel将代码运行在多张GPU卡上时，PyTorch的BN层默认操作是各卡上数据独立地计算均值和标准差，同步BN使用所有卡上的数据一起计算BN层的均值和标准差，缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。</p><p>参见： <a href="vacancy/Synchronized-BatchNorm-PyTorchgithub.com">Synchronized-BatchNorm-PyTorchgithub</a></p><p>Reference:</p><ol><li><a href>Tensorflow cookbook</a></li><li><a href>Pytorch cookbook</a></li><li><a href="https://github.com/kevinzakka/pytorch-goodies">Pytorch-goodies</a></li><li><a href="https://github.com/chenyuntc/pytorch-book">Pytorch book</a></li><li>Pytorch 官方文档 和 Tutorial</li></ol>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch基本概念</title>
    <link href="/2019/03/17/pytorch%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <url>/2019/03/17/pytorch%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
    
    <content type="html"><![CDATA[<p>Pytorch几个基本概念的区分</p><a id="more"></a><h3 id="1-numpy-array-和-Tensor-CPU-amp-GPU"><a href="#1-numpy-array-和-Tensor-CPU-amp-GPU" class="headerlink" title="1. numpy array 和 Tensor(CPU &amp; GPU)"></a>1. numpy array 和 Tensor(CPU &amp; GPU)</h3><pre><code class="hljs shell"><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; import torch</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; import numpy as np</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; a = np.ones(5)</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; a</span>array([1., 1., 1., 1., 1.])<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; b = torch.from_numpy(a)     <span class="hljs-comment"># numpy array-&gt; CPU Tensor</span></span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; b </span>tensor([1., 1., 1., 1., 1.], dtype=torch.float64)<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y = y.cuda()     <span class="hljs-comment"># CPU Tensor -&gt; GPU Tensor</span></span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y</span>tensor([1., 1., 1., 1., 1.], device=&#x27;cuda:0&#x27;, dtype=torch.float64)<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y = y.cpu()  <span class="hljs-comment"># GPU Tensor-&gt; CPU Tensor</span></span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y</span>tensor([1., 1., 1., 1., 1.], dtype=torch.float64)<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y = y.numpy()  <span class="hljs-comment"># CPU Tensor -&gt; numpy array</span></span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y</span>array([1., 1., 1., 1., 1.])device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y = torch.from_numpy(y)</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; y.to(device) <span class="hljs-comment"># 这里 x.to(device) 等价于 x.cuda()</span></span>tensor([1., 1., 1., 1., 1.], device=&#x27;cuda:0&#x27;, dtype=torch.float64)</code></pre><p>索引、 view 是不会开辟新内存的，而像 y = x + y 这样的运算是会新开内存的，然后将 y 指向新内存。</p><h3 id="2-Variable-和-Tensor-require-grad-True"><a href="#2-Variable-和-Tensor-require-grad-True" class="headerlink" title="2. Variable 和　Tensor (require_grad=True)"></a>2. Variable 和　Tensor (require_grad=True)</h3><p>​    Pytorch 0.4 之前的模式为:　<strong>Tensor 没有梯度计算，加上梯度更新等操作后可以变为Variable</strong>.  Pytorch0.4 将 Tensor 和Variable 合并。默认 Tensor 的 require_grad 为 false，可以通过修改 requires_grad 来为其添加梯度更新操作。</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>ytensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>], dtype=torch.float64)  <span class="hljs-meta">&gt;&gt;&gt; </span>y.requires_grad<span class="hljs-literal">False</span><span class="hljs-meta">&gt;&gt;&gt; </span>y.requires_grad = <span class="hljs-literal">True</span><span class="hljs-meta">&gt;&gt;&gt; </span>ytensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>], dtype=torch.float64, requires_grad=<span class="hljs-literal">True</span>)</code></pre><h3 id="3-detach-和-with-torch-no-grad"><a href="#3-detach-和-with-torch-no-grad" class="headerlink" title="3. detach 和　with torch.no_grad()"></a>3. detach 和　with torch.no_grad()</h3><p>一个比较好的 detach和 torch.no_grad区别的解释:</p><blockquote><p><strong><code>detach()</code> detaches the output from the computationnal graph. So no gradient will be backproped along this variable.</strong></p><p><strong><code>torch.no_grad</code> says that no operation should build the graph.</strong></p><p><strong>The difference is that one refers to only a given variable on which it’s called. The other affects all operations taking place within the with statement.</strong></p></blockquote><p><code>detach()</code> 将一个变量从计算图中分离出来，也没有了相关的梯度更新。<code>torch.no_grad()</code>只是说明该操作没必要建图。不同之处在于，前者只能指定一个给定的变量，后者 则会影响影响在 with 语句中发生的所有操作。</p><h3 id="4-model-eval-和-torch-no-grad"><a href="#4-model-eval-和-torch-no-grad" class="headerlink" title="4. model.eval()　和 torch.no_grad()"></a>4. model.eval()　和 torch.no_grad()</h3><blockquote><p><strong>These two have different goals:</strong></p><ul><li><p><code>model.eval()</code> will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.</p></li><li><p><code>torch.no_grad()</code> impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script).</p></li></ul></blockquote><p><code>model.eval()</code>和<code>torch.no_grad()</code>的区别在于，<code>model.eval()</code>是将网络切换为测试状态，例如BN和随机失活（dropout）在训练和测试阶段使用不同的计算方法。<code>torch.no_grad()</code>是关闭PyTorch张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行<code>loss.backward()</code></p><h3 id="5-xx-data-和-xx-detach"><a href="#5-xx-data-和-xx-detach" class="headerlink" title="5. xx.data 和 xx.detach()"></a>5. xx.data 和 xx.detach()</h3><p>​      在 0.4.0 版本之前,  .data 的语义是 获取 Variable 的 内部 Tensor, 在 0.4.0 版本将 Variable 和 Tensor merge 之后,  <code>.data</code> 和之前有类似的语义， 也是内部的 Tensor 的概念。<code>x.data</code> 与 <code>x.detach()</code> 返回的 tensor 有相同的地方, 也有不同的地方:</p><p><strong>相同:</strong></p><ul><li>都和 x 共享同一块数据</li><li>都和 x 的 计算历史无关</li><li>requires_grad = False</li></ul><p><strong>不同:</strong></p><ul><li>y= x.data 在某些情况下不安全, 某些情况, 指的就是上述 inplace operation 的第二种情况, 所以, release note 中指出, 如果想要 detach 的效果的话, 还是 detach() 安全一些.</li></ul><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch<span class="hljs-meta">&gt;&gt;&gt; </span>x = torch.FloatTensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>]])<span class="hljs-meta">&gt;&gt;&gt; </span>w1 = torch.FloatTensor([[<span class="hljs-number">2.</span>], [<span class="hljs-number">1.</span>]])<span class="hljs-meta">&gt;&gt;&gt; </span>w2 = torch.FloatTensor([<span class="hljs-number">3.</span>])<span class="hljs-meta">&gt;&gt;&gt; </span>w1.requires_grad = <span class="hljs-literal">True</span><span class="hljs-meta">&gt;&gt;&gt; </span>w2.requires_grad = <span class="hljs-literal">True</span><span class="hljs-meta">&gt;&gt;&gt; </span>d = torch.matmul(x, w1)<span class="hljs-meta">&gt;&gt;&gt; </span>d_ = d.data<span class="hljs-meta">&gt;&gt;&gt; </span>f = torch.matmul(d, w2)<span class="hljs-meta">&gt;&gt;&gt; </span>d_[:] = <span class="hljs-number">1</span><span class="hljs-meta">&gt;&gt;&gt; </span>f.backward()</code></pre><p><strong>如果需要获取其值，可以使用  xx.cpu().numpy() 或者 xx.cpu().detach().numpy() 然后进行操作，不建议再使用 volatile和  xx.data操作。</strong></p><h3 id="6-ToTensor-amp-ToPILImage-各自都做了什么"><a href="#6-ToTensor-amp-ToPILImage-各自都做了什么" class="headerlink" title="6. ToTensor &amp; ToPILImage 各自都做了什么?"></a>6. ToTensor &amp; ToPILImage 各自都做了什么?</h3><p><strong>ToTensor:</strong></p><ul><li>取值范围：   [0, 255]  —&gt;  [0, 1.0]</li><li>NHWC  —&gt; NCHW</li><li>PILImage  —&gt; FloatTensor</li></ul><pre><code class="hljs python"><span class="hljs-comment"># PIL.Image -&gt; torch.Tensor.</span>tensor = torch.from_numpy(np.asarray(PIL.Image.open(path))    ).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>).float() / <span class="hljs-number">255</span><span class="hljs-comment">#　等价于</span>tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path))</code></pre><p><strong>ToPILImage:</strong></p><ul><li>取值范围:  [0, 1.0] —&gt;  [0, 255]</li><li>NCHW —&gt; NHWC</li><li>类型: FloatTensor -&gt; numpy Uint8 -&gt; PILImage</li></ul><pre><code class="hljs python"><span class="hljs-comment"># torch.Tensor -&gt; PIL.Image.</span>image = PIL.Image.fromarray(torch.clamp(tensor * <span class="hljs-number">255</span>, min=<span class="hljs-number">0</span>, max=<span class="hljs-number">255</span>    ).byte().permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>).cpu().numpy())<span class="hljs-comment">#　等价于</span>image = torchvision.transforms.functional.to_pil_image(tensor)</code></pre><h3 id="7-torch-nn-xxx-与-torch-nn-functional-xxx"><a href="#7-torch-nn-xxx-与-torch-nn-functional-xxx" class="headerlink" title="7. torch.nn.xxx 与 torch.nn.functional.xxx"></a>7. torch.nn.xxx 与 torch.nn.functional.xxx</h3><p>建议统一使用　<code>torch.nn.xxx</code>　模块，<code>torch.functional.xxx</code> 可能会在下一个版本中去掉。</p><p><code>torch.nn</code>　模块和　<code>torch.nn.functional</code>　的区别在于，<code>torch.nn</code>　模块在计算时底层调用了<code>torch.nn.functional</code>，但　<code>torch.nn</code>　模块包括该层参数，还可以应对训练和测试两种网络状态。使用　<code>torch.nn.functional</code>　时要注意网络状态，如:</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>    ...    x = torch.nn.functional.dropout(x, p=<span class="hljs-number">0.5</span>, training=self.training)</code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>neural-network-part1</title>
    <link href="/2019/03/11/neural-network-part1/"/>
    <url>/2019/03/11/neural-network-part1/</url>
    
    <content type="html"><![CDATA[<h3 id="一-数据预处理"><a href="#一-数据预处理" class="headerlink" title="一. 数据预处理"></a>一. 数据预处理</h3><h4 id="1-中心化"><a href="#1-中心化" class="headerlink" title="1. 中心化"></a>1. 中心化</h4><p>中心化是预处理最常用的形式。它对数据中每个独立<em>特征</em>减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。在numpy中，该操作可以通过代码<code>X -= np.mean(X, axis=0)</code>实现。而对于图像，更常用的是对所有像素都减去一个值，可以用<code>X -= np.mean(X)</code>实现，也可以在3个颜色通道上分别操作。</p><h4 id="2-归一化"><a href="#2-归一化" class="headerlink" title="2. 归一化"></a>2. 归一化</h4><p>归一化是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。</p><p>第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为<code>X /= np.std(X, axis=0)</code>。</p><p>第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。</p><p>这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。</p><h3 id="二-激活函数"><a href="#二-激活函数" class="headerlink" title="二. 激活函数"></a>二. 激活函数</h3><h4 id="1-常见的激活函数及其导数"><a href="#1-常见的激活函数及其导数" class="headerlink" title="1. 常见的激活函数及其导数"></a>1. 常见的激活函数及其导数</h4><p>Sigmoid 激活函数的形式为   <script type="math/tex">f(z) = \frac{1}{1+e^{-z}}</script>,  对应的导函数为    $f’(z) = f(z)(1-f(z))$</p><p>Tanh激活函数的形式为   $f(z) = \frac{e^{z} - e^{-z}}{e^{z}+e^{-z}}$,    对应的导函数为$f’(z) = 1 - (f(z))^2$ </p><p>ReLU 激活函数的形式为  $f(z) = max(0, z)$,    对应的导函数为   <script type="math/tex">f'(z)=\left\{\begin{aligned}1, z > 0 \\0, z \leq 0 \\\end{aligned}\right.</script></p><h4 id="2-为什么-sigmoid-和-tanh-激活函数会导致梯度消失的现象？"><a href="#2-为什么-sigmoid-和-tanh-激活函数会导致梯度消失的现象？" class="headerlink" title="2. 为什么 sigmoid 和 tanh 激活函数会导致梯度消失的现象？"></a>2. 为什么 sigmoid 和 tanh 激活函数会导致梯度消失的现象？</h4><p>sigmoid 激活函数的曲线如下左图所示。它将输入z 映射到（0, 1），当z很大时， f(z) 趋近于1；当z很小时， f(z) 趋近于0， 而其导函数 $f’(z) = f(z)(1-f(z)) $ 在 z 很大或者很小时都会趋近于0，造成梯度消失的现象。</p><p>tanh 激活函数的图像如下右图所示。当z很大时，f(z) 趋近于1， 当z 很小时， z 趋近于-1。 其导数 $f’(z) = 1 - (f(z))^2​$  在 z 很大或很小时都会趋近于0，同样会出现梯度消失。</p><p>实际上，Tanh 相当于 Sigmoid的平移：  <script type="math/tex">tanh(z) = 2sigmoid(2x)-1</script>。</p><p><img src="/2019/03/11/neural-network-part1/sigmoid_and_tanh.png" alt="sigmoid and tanh"></p><h4 id="3-ReLU-系列的激活函数相对于-sigmoid-和-tanh-的优点是什么？它们有什么局限性以及如何改进？"><a href="#3-ReLU-系列的激活函数相对于-sigmoid-和-tanh-的优点是什么？它们有什么局限性以及如何改进？" class="headerlink" title="3. ReLU 系列的激活函数相对于 sigmoid 和 tanh 的优点是什么？它们有什么局限性以及如何改进？"></a>3. ReLU 系列的激活函数相对于 sigmoid 和 tanh 的优点是什么？它们有什么局限性以及如何改进？</h4><p>优点：</p><p>（1）从计算角度上， Sigmoid 和 tanh 激活函数均需要计算指数，复杂度高，而ReLU只需要一个阈值即可得到激活值。</p><p>（2）ReLU 的非线性包含线性可以有效地解决梯度消失问题，提供相对较宽的激活边界。</p><p>（3）ReLU的单侧抑制提供了网络的稀疏表达能力。  </p><p>局限性：在训练过程中会会导致神经元死亡的问题。这是由于 $f(z) = max(0, z)$  导致负梯度在经过该ReLU 单元时被置为0， 且在之后也不被任何数据激活，即流经该神经元的梯度永远为0， 不对任何数据产生响应。在实际训练中，如果学习率设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败。</p><p>一些ReLU 的改进措施：</p><p>（1）Leaky ReLU（LReLU）</p><p>Leaky ReLU 的表示形式为：<script type="math/tex">f(z)=\left\{\begin{aligned}z, z > 0 \\\alpha z, z \leq 0 \\\end{aligned}\right.</script> ， Leaky ReLU 与 ReLU 的区别在于当 $z\le 0$ 时， 其值不为零，一般来说a 为一个很小的常数(0.01或者0.001数量级的较小整数)，这样既实现了单侧抑制，又保留了部分负梯度信息以致不完全丢失。但另一方面，$ \alpha$  为超参数，较难设置为合适的值，且较为敏感，因此Leaky ReLU 函数在实际使用中的性能部分十分稳定。</p><p>（2）参数化 ReLU（Parametric  ReLU，PReLU）：PReLU将负轴部分斜率$ \alpha$ 作为网络中的一个可学习的参数融入模型的整体训练过程。有几点有趣的现象需要注意：</p><ul><li><p>自由度较大的各通道独享参数的参数化ReLU性能相比较各通道共享参数更优。</p></li><li><p>在独享参数设定下学到的$ \alpha$取值呈现出由浅层到深层依次递减的趋势，这说明实际上网络所需要的非线性随着网络层数的增加而递减。 </p><p>在分类精度上， 使用PReLU 作为激活函数的网络要优于原始ReLU的网络。但是PReLU在带来更大自由度的同时，也增加了网络模型过拟合的奉献，在实际使用中需要格外注意。</p></li></ul><p>（3）随机化ReLU(Random ReLU, RReLU）：增加了“随机化机制， 其取值在训练阶段服从均匀分布，在测试阶段则将其指定为该均匀分布对应的数学期望 $ \frac{l+u}{2}\ $。</p><p><img src="/2019/03/11/neural-network-part1/relu_adv.png" alt="sigmoid and tanh"></p><p>（4）指数化线性单元（Exponential Linear Unit， ELU）：2016年 Clevert 等人提出了指数化线性单元 ELU，其公式为： <script type="math/tex">ELU(x)=\left\{\begin{aligned}x, x \ge 0 \\\lambda \cdot (e^x - 1), z \lt 0 \\\end{aligned}\right.</script>。ReLU 具备了 ReLU 函数的优点，同时也解决了ReLU 函数自身的“死区”问题。不过ELU 函数中的指数操作稍稍增大了计算量。在实际应用中，ELU 中的超参数 $\lambda$ 一般被设置为1。</p><h3 id="三-网络参数初始化"><a href="#三-网络参数初始化" class="headerlink" title="三. 网络参数初始化"></a>三. 网络参数初始化</h3><p>网络参数初始化方式主要分为四种：</p><p>（1）全零初始化：全零初始化会导致网络不通神经元的输出相同，相同的输出导致梯度更新完全一样，这样便会令更新后的参数仍然保持一样的状态，从而无法对模型进行训练。</p><p>（2）随机初始化：将参数值随机设定为接近0的一个很小的随机数（有正有负）。</p><p>高斯分布：</p><pre><code class="hljs python"><span class="hljs-comment"># origin method</span>w = <span class="hljs-number">0.001</span> * randn(n_in, n_out)<span class="hljs-comment"># Xavier method </span>w = (<span class="hljs-number">0.001</span> * randn(n_in, n_out)) / sqrt(n)<span class="hljs-comment"># He method</span>w = (<span class="hljs-number">0.001</span> * randn(n_in, n_out)) / sqrt(n/<span class="hljs-number">2</span>)<span class="hljs-comment"># 其中n=n_in或n=(n_in+n_out)/2</span></code></pre><p>均匀分布：</p><pre><code class="hljs python"><span class="hljs-comment">#  Xavier method</span>low = -sqrt(<span class="hljs-number">3</span>/n)high = sqrt(<span class="hljs-number">3</span>/n)rand_param = low + (high - low) * rand(n_in, n_out)w = <span class="hljs-number">0.001</span> * rand_param <span class="hljs-comment"># He mothod</span>low = -sqrt(<span class="hljs-number">6</span>/n)high = sqrt(<span class="hljs-number">6</span>/n)rand_param = low + (high - low) * rand(n_in, n_out)w = <span class="hljs-number">0.001</span> * rand_param</code></pre><p>（3）预训练模型初始化</p><p>（4） 数据敏感的参数初始化方式   <a href="https://github.com/philkr/magic_init">github address</a></p><p>总结： </p><ul><li>网络参数初始化的优劣在极大程度上决定了网络的最终性能。</li><li>比较推荐的网络初始化方式为He方式，将参数初始化为服从高斯部分或者均匀分布的较小随机数，同时对参数方差需要加以规范化。</li><li>另外借助预训练模型中的参数作为新任务的参数初始化方式一种简便异性且十分有效的模型参数初始化方法。</li></ul><p>xavier 参数初始化方式的由来：</p><p>假设s为未经非线性变化的该层网络输出结果，w为该层参数，x为该层的输入数据，则：</p><script type="math/tex; mode=display">Var(s) = Var(\sum_i^nw_ix_i)   \\       = \sum_i^nVar(w_ix_i)   \\       = \sum_i^n[E(w_i)]^2Var(x_i) +[E(x_i)]^2Var(w_i) + Var(x_i)Var(x_i)     \\       = \sum_i^nVar(x_i)Var(w_i) \\       = (nVar(w))Var(x) \\</script><p>为了保证输出数据 Var(s) 和输入数据Var(x)的方差一致，需令 nVar（w） = 1，即 $n \cdot Var(a\omega)=n\cdot a^2 \cdot Var(\omega’) = 1$， 则 $a=\sqrt{(1/n)}$， 其中$\omega’$为方差规范化后的参数。</p><h3 id="四-目标函数"><a href="#四-目标函数" class="headerlink" title="四. 目标函数"></a>四. 目标函数</h3><h4 id="1-分类任务的目标函数"><a href="#1-分类任务的目标函数" class="headerlink" title="1. 分类任务的目标函数"></a>1. 分类任务的目标函数</h4><h5 id="1-交叉熵损失函数-Softmax-损失函数"><a href="#1-交叉熵损失函数-Softmax-损失函数" class="headerlink" title="(1) 交叉熵损失函数/Softmax 损失函数"></a>(1) 交叉熵损失函数/Softmax 损失函数</h5><p>交叉熵(cross entropy)损失函数又称Softmax 损失函数，是目前卷积神经网络最常用的分类目标函数。其形式为：</p><p>$L<em>{cross\ entropy\ loss} = L</em>{softmax\ loss} = -\frac{1}{N}\sum<em>{i=1}^{N}log(\frac{e^{h</em>{y<em>i}}}{\sum</em>{j=1}^{C}e^{h_j}})$</p><p>即通过指数化变换使网络输出 $h$ 转换为概率形式。</p><h5 id="2-合页损失函数"><a href="#2-合页损失函数" class="headerlink" title="(2) 合页损失函数"></a>(2) 合页损失函数</h5><p>在支持向量机中被广泛使用的合页损失函数（hinge loss）有时也作为目标函数在神经网络模型中使用：</p><p>$L<em>{hinge\ loss} = \frac{1}{N}\sum</em>{i=1}^{N}max{(0, 1-h_{yi})}$</p><p>合页损失函数的设计理念是：对错误越大的样本施加越严重的惩罚。可是这一损失函数对噪声的抵抗能力较差。另外，一般的分类任务中的交叉熵损失函数的分类效果略优于合页损失函数的分类效果。</p><h5 id="3-坡道损失函数"><a href="#3-坡道损失函数" class="headerlink" title="(3) 坡道损失函数"></a>(3) 坡道损失函数</h5><p>坡道损失函数的定义为：</p><script type="math/tex; mode=display">L_{ramp\ loss} = L_{hinge\ loss} - \frac{1}{N}\sum_{i=1}^{M}max{(0, s-h{yi})}  \\= \frac{1}{N}\sum_{i=1}^{M}(max{(0, 1-h_{yi})} - max{(0, s-h_{yi})})  \\</script><p>其中 , $s$ 制定了“截断点”的文职。由于坡道损失函数实际在 $s$ 处“截断” 合页损失函数，因此坡道损失函数也被称为“截断合页损失函数”(truncated hinge loss function)</p><h5 id="4-大间隔交叉熵损失函数"><a href="#4-大间隔交叉熵损失函数" class="headerlink" title="(4) 大间隔交叉熵损失函数"></a>(4) 大间隔交叉熵损失函数</h5><p>上面提到的网络输出结果 $h$ 实际上市全连接层参数 $W$ 与该层特征向量 $x_i$的内积， 即 $h=W^Tx_i$。因此传统的交叉熵损失还可以表示为：</p><script type="math/tex; mode=display">L_{softmax\ loss} = -\frac{1}{N}\sum_{i=1}^{N}log(\frac{e^{h_{y_i}}}{\sum_{j=1}^{C}e^{h_j}}) \\= -\frac{1}{N}\sum_{i=1}^{N}log(\frac{e^{W^T_{yi}x_i}}{\sum_{j=1}^{C}e^{W^T_jx_i}}) \\</script><p>其中， $W_i^T$ 为$W$ 第i列参数值。根据内积的定义，上式可以变换为</p><script type="math/tex; mode=display">L_{softmax\ loss} = -\frac{1}{N}\sum_{i=1}^{N}log(\frac{e^{||W_{yi}||||x_i||cos(\theta_{yi})}}{\sum_{j=1}^{C}e^{||W_{j}||||x_i||cos(\theta_j)}})</script><p>式中的 $\theta_j(0\leq\theta_j \leq \pi)$ 为向量 $W_i^T$ 和 $x_i$的夹角。以二分类为例，对隶属于第一个类别的某样本 $x_i$而言， 为分类正确，传统的交叉熵损失函数需迫使学到的参数满足 $W_1^Tx_i &gt; W_2^Tx_i$, 亦即$||W_1||||x_i|| cos(\theta) &gt; ||W2||||x_i||cos(\theta)$。大间隔交叉熵损失函数为了使特征更加具有分辨能力，则再次基础上要求二者差异更大，即引入 $m$ “拉大”两者差距，这便是“大间隔”名称的由来。$||W_1||||x_i||cos(m\theta_1) \geq ||W2||||x_i||cos(\theta_2) , (0\leq\theta_1\leq\frac{\pi}{m})$。式中 $m$ 为正整数，起到控制间隔大小的作用，$m$ 越大。类间间隔越大，反之亦然。特别地， 当 $m=1$ 时， 大间隔交叉熵损失函数即退化为传统交叉熵损失函数。</p><p>综上可得：</p><script type="math/tex; mode=display">||W_1||||x_i||cos(\theta_1) \geq ||W_1||||x_i||cos(m\theta_1) \geq ||W2||||x_i||cos(\theta_2) , (0\leq\theta_1\leq\frac{\pi}{m})</script><p>可以发现，上式不仅满足传统交叉熵损失函数的约束，在确保分类正确的同时增大了不同类别间分类的置信度，这有助于进一步提升特征分辨能力。</p><p>大间隔交叉熵损失函数的定义为：</p><script type="math/tex; mode=display">L_{large-margin\ softmax\ loss} = -\frac{1}{N}\sum^{1}_{i=1}log(\frac{e^{||W_i||||x_i||\phi(\theta_{y_i})}}{e^{||W_i||||x_i||\phi(\theta_{yi})}+\sum_{j\neq y_i}e^{||W_j||||x_i||\phi(\theta_{j})}})</script><p>可以发现，上式与$Softmax$ 损失函数的区别仅仅在于第 $i$ 类分类间隔 “拉大”了：由  $cos(\theta<em>{y_i})$ 变为 $\phi(\theta</em>{y_i})$, 其中：</p><script type="math/tex; mode=display">\phi(\theta) = \begin{equation}  \left\{               \begin{array}{**lr**}               cos(m\theta), 0 \leq \theta \leq \frac{\pi}{m}&  \\               \mathcal{D}(\theta) , \frac{\pi}{m} < \theta \leq \pi &\\                  \end{array}  \right.  \end{equation}</script><p>式中，$\mathcal{D}(\theta)$ 只需要满足“单调递减”条件， 且 $D(\frac{\pi}{m})=cos\frac{\pi}{m})$。</p><h5 id="5-中心损失函数"><a href="#5-中心损失函数" class="headerlink" title="(5) 中心损失函数"></a>(5) 中心损失函数</h5><p>大交叉熵损失函数主要考虑增大类间距离，而中心损失函数则在考虑类间距离的同时还将一些注意力放在减小类间差异上。中心损失函数的定义为：</p><script type="math/tex; mode=display">L_{center\ loss} = \frac{1}{2}\sum_{i=1}^{N}||x_i - c_{y_i}||_2^2</script><p>其中， $c_{y_i}$ 为 第 $y_i$ 类所有深度特征均值的中心，故名“中心损失函数”。</p><p>在实际使用中，由于中心损失函数本身考虑类内差异，因此应该讲中心损失函数与其他主要考虑类间距离的损失函数配合使用，如交叉熵损失函数，这样网络最终目标形式可表示为：</p><script type="math/tex; mode=display">L_{final} = L_{cross\ entropy\ loss} + \lambda L_{center\ loss}(h, y_i)  \\= -\frac{1}{N}\sum_{i=1}^{N}log(\frac{e_{h_{y_i}}}{\sum_{j=1}^{C}e^{h_j}})+\frac{\lambda}{2}\sum_{i=1}^{N}||x_i - c_{y_i}||_2^2</script><h4 id="2-回归任务的目标函数"><a href="#2-回归任务的目标函数" class="headerlink" title="2. 回归任务的目标函数"></a>2. 回归任务的目标函数</h4><h5 id="1-mathcal-L1-损失函数"><a href="#1-mathcal-L1-损失函数" class="headerlink" title="(1) $\mathcal{L1}$ 损失函数"></a>(1) $\mathcal{L1}$ 损失函数</h5><script type="math/tex; mode=display">L_{\mathcal{l1}} = \frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{M}||y_t^i-\hat{y}t^i||</script><h5 id="2-mathcal-L2-损失函数"><a href="#2-mathcal-L2-损失函数" class="headerlink" title="(2) $\mathcal{L2}$ 损失函数"></a>(2) $\mathcal{L2}$ 损失函数</h5><script type="math/tex; mode=display">L_{\mathcal{l2}} = \frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{M}(||y_t^i-\hat{y}t^i||)^2</script><p>在实际使用中， $\mathcal{l_1}$ 与 $\mathcal{l_2}$ 损失函数在回归精度上几乎相差无几。不过在一些情况先 $\mathcal{l_2}$ 损失函数会略优于 $\mathcal{l_1}$ ，同时收敛速度方面 $l_2$ 损失函数也略快于 $\mathcal{l_1}$ 损失函数。</p><h5 id="3-Tukey’s-biweight-损失函数"><a href="#3-Tukey’s-biweight-损失函数" class="headerlink" title="(3) Tukey’s biweight 损失函数"></a>(3) Tukey’s biweight 损失函数</h5><p>Tukey’s biweight 损失函数是一类非凸函数，其可克服在回归任务中的利群店或者样本噪声对整体回归模型的干扰和影响，是回归任务中一种健壮的损失函数，其定义如下：</p><script type="math/tex; mode=display">\mathcal{L_{Tukey's\ biweight\ loss}} =\begin{equation}  \left\{               \begin{array}{**lr**}               \frac{c^2}{6N}\sum_{i=1}^{N}\sum_{t=1}^{M}[1-(1-(\frac{l_t^i}{c})^2)], |l_t^i| \leq c  \\               \frac{c^2{M}}{6}, 其他 &\\                  \end{array}  \right.  \end{equation}</script><p>式中， 常数  $c$ 指定了函数拐点的位置。需要说明的是，该超参数并不需要认为指定。一般情况下， 当 $c=4.6851$ 时， $Tukey’s\ biweight $损失函数可取得与 $\mathcal{l_2}$ 损失函数在最小化符合标准正态分布时的残差类似的(95%渐进)回归效果。</p><h4 id="3-其他任务的损失函数"><a href="#3-其他任务的损失函数" class="headerlink" title="3. 其他任务的损失函数"></a>3. 其他任务的损失函数</h4><p>在一些如人脸年龄估计、头部角度识别等任务样本标记具有不确定性的特殊场景下，基于标记分布(label distribution)的损失函数不失为一种优质的选择。具体而言，假设 $h=(h_1, h_2, …, h_C)^T$ 为模型对于输入样本 $x_i$ 的最终输出结果，那么在利用标记分布技术解决问题之前，首先需要将 $h$ 转化为一个合法分布。以softmax 为例可以将 $h$ 转化为:</p><script type="math/tex; mode=display">\hat{y}_k = \frac{e^{h_k}}{\sum_{j=1}^{C}{e^{h_j}}}</script><p>其中，$k \in {1,2,3,…,C}$ 代表标记向量的第 $k$ 维。</p><p>可以用Kullback-Leibler散度(KL divergence)来度量 $\hat{y}$与真实标记 $y$ 之间的误差：$\mathcal{L}_{KL\ loss}=\sum_k{y_klog\frac{y_k}{\hat{y}_k}}$。</p><p>由于 $y<em>k$ 为常量，上式等价于：$\mathcal{L}</em>{KL\ loss} = -\sum_{k}{y_k}{log{\hat{y}_k}}$。通过该式可以衡量样本标记分布与真实部分之间的差异，并利用该差异指导模型训练。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>model_evaluation</title>
    <link href="/2019/03/10/model-evaluation/"/>
    <url>/2019/03/10/model-evaluation/</url>
    
    <content type="html"><![CDATA[<h3 id="Chapter-2-模型评估"><a href="#Chapter-2-模型评估" class="headerlink" title="Chapter 2 模型评估"></a>Chapter 2 模型评估</h3><h4 id="1-准确率和平方根误差的缺陷"><a href="#1-准确率和平方根误差的缺陷" class="headerlink" title="1. 准确率和平方根误差的缺陷"></a>1. 准确率和平方根误差的缺陷</h4><h5 id="准确率"><a href="#准确率" class="headerlink" title="准确率"></a>准确率</h5><p>​     <strong>准确率是指分类正确的样本占样本个数的比例</strong>，即 <strong>$Accuracy  = \frac{n<em>{correct}}{n</em>{total}}$</strong> ,其中 $n<em>{correct}$ 为被正确分类的样本个数， $n</em>{total}$ 为总样本的个数。</p><p>​     准确率是分类问题中最简单也是最直观的评价指标， 但存在明显缺陷。当<strong>不同类别的样本比例非常不均衡</strong>时，占比较大的类别往往会成为影响准确率最主要的因素。比如，当负样本占99%时，分类器把所有样本预测为负样本也可以获得99% 的准确率。 </p><p>​      为了解决这个问题，可以使用更为有效的<strong>平均准确率</strong>(每个类别下的样本准确率的算术平均)作为评估的指标。</p><h5 id="平方根误差-RMSE"><a href="#平方根误差-RMSE" class="headerlink" title="平方根误差 RMSE"></a>平方根误差 RMSE</h5><p>​      RMSE 平方根误差经常被用来衡量回归模型的好坏，其计算公式为 $RMSE = \sqrt{\frac{1}{n}\sum^{n}<em>{i-1}{(y</em>{i}-\hat{y_{i}})^2}}$, 其中 $y_i$  是第i个样本点的真实值吗，$\hat{y_i}$  是 第i个样本的预测值，n 是样本点的个数。</p><p>​     一般情况下，RMSE 能够更好的反映回归模型预测值与真实值的偏离程度。但在实际问题中，如果存在个别偏离程度非常大的<strong>离群点</strong>(Outlier),   即使离群点数量非常少，也会让RSME 指标变得非常差。</p><p>​     针对这种情况从三个角度进行解决。（1） 如何认定这些离群点是“噪声点”的话，就需要在数据预处理的阶段把这些噪声<strong>过滤</strong>掉。（2）如果不认为这些离群点是“噪声点”的话，需要进一步<strong>提高模型的预测能力</strong>，将离群点产生的机制建模进去。（3）找一个更合适的指标来评估模型。比如平均绝对百分比误差<strong>MAPE</strong>， 它定义为  $MAPE  = \sum_{i=1}^{n}\lvert \frac{y_i - \hat{y_i}}{y_i}\rvert \times\frac{100}{n}$,  相比于 RSME， MAPE 相当于把这个点的误差进行了归一化，降低了个别离群点带来的误差影响。</p><h4 id="2-混淆矩阵、P-R曲线、ROC-曲线"><a href="#2-混淆矩阵、P-R曲线、ROC-曲线" class="headerlink" title="2. 混淆矩阵、P-R曲线、ROC 曲线"></a>2. 混淆矩阵、P-R曲线、ROC 曲线</h4><p><img src="/2019/03/10/model-evaluation/matrix.png" alt="混淆矩阵"></p><p>​        <strong>精确率：</strong> 分类正确的正样本个数占分类器判定为正样本的样本个数的比例。即: $Precision = \frac{TP}{TP+FP}$。</p><p>​        <strong>召回率：</strong> 分类正确的正样本个数占真正正样本个数的比例。即：$Recall = \frac{TP}{TP+FN}$。</p><p>​        Precision 值和 Recall 值是既矛盾又统一的两个指标，为了提高Precision值，分类器需要尽量在”更有把握“时才把样本预测为正样本，但此时由于过于保守而漏掉很多”没有把握“的正样本，导致Recall值降低。</p><p>​        P-R 曲线能够综合评估一个排序模型的好坏。P-R 曲线的横轴是召回率，纵轴是精确率。对于一个排序模型来说， 某P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回的结果对应的召回率和精确率。整条P-R曲线是通过将阈值从高到底移动而生成的。</p><p>​        <strong>假阳性率</strong>：分类错误的正样本(预测为正样本，实际是正样本)占真正负样本的比例。即 $FPR = \frac{FP}{FP+TN}$。</p><p>​        <strong>真阳性率</strong>：分类正确的正样本占真正正样本个数的比例。$TPR = \frac{TP}{TP+FN}$。</p><p>​        ROC 曲线是通过不断移动分类器区分正负预测结果的阈值来生成曲线上的一组关键点的。从最高的得分开始（实际上是从正无穷开始，对应ROC 曲线的零点）， 逐渐调整到最低得分，每一个阈值都会对应一个FPR和TPR，在ROC图上绘制出每个阈值对应的位置，再连接所有点酒得到最终的ROC 曲线。另外所谓的AUC 是指ROC 曲线下的面积大小，该值能够量化地反应基于ROC 曲线衡量出的模型性能。计算AUC 的值只需要沿着ROC横轴积分即可。由于ROC 曲线一般都处于y=x 这条直线的上方，所以AUC 的取值一般在0.5-1 之间。AUC 越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。</p><p>​    相比于 P-R 曲线，ROC 曲线有一个特点，当政府样本的分布发生变化时， ROC 曲线的形状能够基本保持不变，而P-R 曲线的形状一般会发生比较剧烈的变化。所以 ROC 曲线的使用场景更多，被广泛用于排序、推荐、广告等领域。</p><p>​        <strong>F1-Score</strong> 也能综合反映一个排序模型的性能。它是精准率和召回率的调和平均值。它的定义为：</p><script type="math/tex; mode=display">F1 = \frac{2*precision*recall}{precision+recall}</script><h4 id="3-余弦距离的应用"><a href="#3-余弦距离的应用" class="headerlink" title="3. 余弦距离的应用"></a>3. 余弦距离的应用</h4><p>​        对于两个向量 A和 B ，其余弦相似度的定义为 $cos(A,B) = \frac{A\cdot B}{\Vert A \Vert_2 \Vert B \Vert_2}$。 即两个向量夹角的余弦，关注的是向量之间的角度关系，并不关心它们的绝对大小，其取值范围是[-1, 1]。 相比较而言，欧式距离的数值受维度的影响，范围不固定，并且含义也比较模糊。总体来说，欧式距离体现在数值上的绝对差异，而余弦距离则体现方向上的相对差异。比如统计两部剧的用户观看行为，用户A 的观看向量是（0，1）， 而用户B 的观看向量为（1，0）；此时两者的余弦距离很大，而欧式距离很小；我们分析两个用户不同视频的偏好，更关注相对差异，显然应当使用余弦距离。而当我们分析用户活跃度时，以登录次数和平均观看市场作为调整，余弦距离会认为（1，10）（10，100） 两个用户距离很近；但是显然两个用户活跃度有着极大差异的，此时我们要关注数值的绝对差异，应当使用欧式距离。</p><p>​        考查一个距离是否是严格定义的距离。要从距离的定义出发：在一个集合中，如果每一对元素均可唯一决定一个实数，使得三条距离公理（正定性、对称性、三角不等式）成立，则该实数可称为这对元素之间的距离。以余弦距离为例：</p><ul><li><p>正定型：$dist(A, B) = 1-  cos\theta \ge 0 $ 恒成立， 特别地，有  $dist(A, B) = 0 \Leftrightarrow \Vert A \Vert_2 \Vert B \Vert_2 = AB \Leftrightarrow A=B$</p><p>因此余弦距离满足正定性。</p></li><li><p>对称性： </p></li><li><script type="math/tex; mode=display">dist(A, B) = 1- cos(A,B) = \frac{\Vert A \Vert_2 \Vert B \Vert_2 - A\cdot B}{\Vert A \Vert_2 \Vert B \Vert_2} = \frac{\Vert B \Vert_2 \Vert A \Vert_2 - A\cdot B}{\Vert A \Vert_2 \Vert B \Vert_2} = dist(B, A)</script><p>因此，余弦距离满足对称性。</p></li><li><p>三角不等式：该性质并不成立，下面给出一个反例。给定 A = (1, 0)， B = (1, 1)，C=（0，1），则有</p><script type="math/tex; mode=display">dist(A, B) = 1 - \frac{\sqrt{2}}{2}</script><script type="math/tex; mode=display">dist(B, C) = 1 - \frac{\sqrt{2}}{2}</script><script type="math/tex; mode=display">dist(A,C) = 1</script><p>因此有 $dist(A, B)+dist(B,C) = 2-\sqrt{2} &lt; 1 = dist(A, C)$</p></li></ul><p>在机器学习领域，被俗称为距离，却不满足三条距离公理的不仅仅有余弦距离，还有KL距离，也叫相对熵， 它常用于计算两个分布之间的差异，但不满足对称性和三角不等式。</p><h4 id="4-为什么对模型进行过充分的离线评估之后，还要进行在线A-B-测试？如何进行A-B-测试？"><a href="#4-为什么对模型进行过充分的离线评估之后，还要进行在线A-B-测试？如何进行A-B-测试？" class="headerlink" title="4. 为什么对模型进行过充分的离线评估之后，还要进行在线A/B 测试？如何进行A/B 测试？"></a>4. 为什么对模型进行过充分的离线评估之后，还要进行在线A/B 测试？如何进行A/B 测试？</h4><p>需要进行在线 A/B 测试的<strong>原因</strong>如下：</p><p>（1）离线评估<strong>无法完全消除模型过拟合的影响</strong>，因此，得出的离线评估结果无法完全替代线上评估结果。</p><p>（2）离线评估<strong>无法完全还原线上工程环境</strong>。一般来讲，离线评估往往不会考虑线上环境的延迟、数据丢失、标签数据缺失等情况。因此，离线评估的结果是理想工程环境下的结果。</p><p>（3）线上系统的<strong>某些商业指标</strong>在离线评估中无法计算。离线评估一般是针对模型本身进行评估，而与模型相关的其他指标，特别是商业指标，往往无法直接获得。</p><p>进行A/B 测试的主要手段是将用户划分为实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以就模型。在划分用户的过程过程中，要注意样本的独立性（同一用户每次只能划分到一个分组中）和采样方式的无偏性（划分过程中选取的usr_id 是一个随机数）。    </p><h4 id="5-模型评估中的验证方法"><a href="#5-模型评估中的验证方法" class="headerlink" title="　5. 模型评估中的验证方法"></a>　5. 模型评估中的验证方法</h4><p><strong>Holdout检验：</strong>　将原始的样本集合随机划分为训练集和验证集两部分。训练集用于模型训练，　验证集用于模型验证。　<strong>缺点</strong>：在验证集上计算出来的最后评估指标与原始分组有很大关系。</p><p><strong>交叉验证：</strong>　最常见的是ｋ-fold　交叉验证。首先将全部样本划分成ｋ个大小相等的样本子集；依次遍历这ｋ个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后把ｋ次评估指标的平均值作为最终的评估指标。在实际实验中，ｋ经常取10。</p><p><strong>自助法(Bootstrap) : </strong> 对于总数为ｎ　的样本集合，进行ｎ次有放回的随机采样，得到大小为ｎ的训练集合。ｎ次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证。</p><p>　　在自助采样过程中。对于样本进行ｎ次自助抽样，当ｎ趋于无穷大时，最终有多少数据从未被选择过？</p><p>一个样本在一次抽样过程中未被抽中的概率为　$1 - \frac{1}{n} $，　ｎ次抽样均未被抽中的概率为 $（１-\frac{1}{n})^n$。 当ｎ趋于无穷大时，概率为　</p><script type="math/tex; mode=display">{\lim_{n\to+\infty}}(1-\frac{1}{n})^n 　 = \lim_{n\to\infty}\frac{1}{(1+\frac{1}{n-1})^n}　 = \frac{1}{\lim_{n\to\infty}(1+\frac{1}{n-1})^{n-1}} ×\frac{1}{\lim_{n\to\infty}(1+\frac{1}{n-1})}   = \frac{1}{e}    \approx 0.368</script><h4 id="6-超参数调优"><a href="#6-超参数调优" class="headerlink" title="　6. 超参数调优"></a>　6. 超参数调优</h4><p><strong>网格搜索：</strong>　网格搜索可能是最简单，应用最广泛的超参数搜索算法，它通过查找搜索范围内的所有的点来确定最优值。在实际应用中，我们一般会先使用较广的搜索范围和较大的步长，来寻找全局最优值可能的位置；然后会逐渐缩小搜索范围和步长，来寻找更精确的最优值。</p><p><strong>随机搜索：</strong>　在搜索范围内随机选取样本点而不是测试上界和下界之间之间的所有值。它的理论依据是如果样本点足够大，那么通过随机采样也能大概率找到全局最优值或者近似值、。</p><p><strong>贝叶斯优化算法：</strong>贝叶斯优化算法通过对目标函数的形状进行学习，找到使目标函数向全局最优值提升的参数。具体来说，它的学习目标函数形状的方法是，首先根据先验分布，假设一个搜集函数；然后每一次使用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布；最后，算法测试由后验分布给出的全局最优最可能出现的位置的点。对于贝叶斯算法，有一个需要注意的地方，一旦找到一个局部最优值，它就会在该区域不断采样，所以很容易陷入局部最优值。为了弥补这个缺陷，贝叶斯优化算法会在探索和利用直接找到一个平衡点，“探索”就是在还未取样的区域获取采样点；而“利用”则是根据后验分布在最可能出现的全局区域最值进行采样。</p><h4 id="7-在模型评估过程中，过拟合现象和欠拟合现象具体是指什么"><a href="#7-在模型评估过程中，过拟合现象和欠拟合现象具体是指什么" class="headerlink" title="　7. 在模型评估过程中，过拟合现象和欠拟合现象具体是指什么?"></a>　7. 在模型评估过程中，过拟合现象和欠拟合现象具体是指什么?</h4><p><strong>过拟合</strong>是指模型对于训练数据拟合过当的情况，反应到评估指标上，就是模型在训练集上表现很好，但是在测试集和新数据上表现较差。 <strong>欠拟合</strong>是指模型在训练和预测时表现都不好的情况。</p><p><img src="/2019/03/10/model-evaluation/underfitting.png" alt="IMG"></p><h4 id="8-常见的降低过拟合和欠拟合风险的方法"><a href="#8-常见的降低过拟合和欠拟合风险的方法" class="headerlink" title="8. 常见的降低过拟合和欠拟合风险的方法"></a>8. 常见的降低过拟合和欠拟合风险的方法</h4><h5 id="降低过拟合风险的方法"><a href="#降低过拟合风险的方法" class="headerlink" title="　降低过拟合风险的方法"></a>　降低过拟合风险的方法</h5><p>（１）　获取更多的数据集，或者通过一定的规则来扩充训练数据，　如在图像分类问题上，可以通过平移、旋转、缩放等方法来扩充数据；更进一步地，可以通过对抗生成网络来合成大量的新训练数据。</p><p>（２）降低模型的复杂度。当数据较少，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。例如，在神经网络中减少网络层数、神经元个数等；在决策树模型中降低树的深度、进行剪枝等。</p><p>（３）正则化方法。　给模型的参数加上一定的正则约束，比如，将权值的大小加入到损失函数中。以L2正则化为例：$C = C<em>0 + \frac{\lambda}{2n}\dot\sum</em>{i}{w_i}^2$。 这样在优化原来目标函数$C_0$　的同时，也能避免权值过大带来的过拟合风险。</p><p>（４）集成学习方法。集成学习是把多个模型集成在一起，来降低模型的过拟合风险。</p><h5 id="降低欠拟合风险的方法"><a href="#降低欠拟合风险的方法" class="headerlink" title="　降低欠拟合风险的方法"></a>　降低欠拟合风险的方法</h5><p>（１）添加新特征。　当特征不足或者现有特征与样本标签的相关性不强时，　模型容易出现欠拟合。</p><p>（２）增加模型复杂度。简单模型的学习能力较差，通过增加模型的复杂度可以使模型拥有较强的拟合能力。例如，在线性模型中添加高次项，在神经网络中添加网络层数或神经元个数等。</p><p>（３）减小正则化洗漱。正则化是用来防止过拟合的，但当模型中出现欠拟合现象时，则需要有针对性地减少正则化系数。</p><p><em>补充：添加特征时，通过挖掘“上下文特征”、“ID 类特征”、“组合特征”等新的特征，往往能取得更好的效果，在深度学习中，有很多模型可以帮助完成特征工程，如因子分解机、梯度提升策略树、Deep-crossing等都可以成为丰富特征的方法</em></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>pytorch API</title>
    <link href="/2019/03/08/Pytorch-API/"/>
    <url>/2019/03/08/Pytorch-API/</url>
    
    <content type="html"><![CDATA[<p>Pytorch API 汇总整理</p><a id="more"></a><h3 id="1-import-torch"><a href="#1-import-torch" class="headerlink" title="1. import torch"></a>1. import torch</h3><p>import &amp; vision</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch print(torch.__version__)</code></pre><h3 id="2-Tensor-type-🌟"><a href="#2-Tensor-type-🌟" class="headerlink" title="2. Tensor type 🌟"></a>2. Tensor type 🌟</h3><p>Pytorch 给出了 9 种 CPU Tensor 类型和 9 种 GPU Tensor 类型。Pytorch 中默认的数据类型是 torch.FloatTensor, 即 torch.Tensor 等同于 torch.FloatTensor。</p><div class="table-container"><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>32-bit floating point</td><td>torch.float32 or torch.float</td><td>torch.FloatTensor</td><td>torch.cuda.FloatTensor</td></tr><tr><td>64-bit floating point</td><td>torch.float64 or torch.double</td><td>torch.DoubleTensor</td><td>torch.cuda.DoubleTensor</td></tr><tr><td>16-bit floating point</td><td>torch.float16 or torch.half</td><td>torch.HalfTensor</td><td>torch.cuda.HalfTensor</td></tr><tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td>torch.ByteTensor</td><td>torch.cuda.ByteTensor</td></tr><tr><td>8-bit integer (signed)</td><td>torch.int8</td><td>torch.CharTensor</td><td>torch.cuda.CharTensor</td></tr><tr><td>16-bit integer (signed)</td><td>torch.int16 or torch.short</td><td>torch.ShortTensor</td><td>torch.cuda.ShortTensor</td></tr><tr><td>32-bit integer (signed)</td><td>torch.int32 or torch.int</td><td>torch.IntTensor</td><td>torch.cuda.IntTensor</td></tr><tr><td>64-bit integer (signed)</td><td>torch.int64 or torch.long</td><td>torch.LongTensor</td><td>torch.cuda.LongTensor</td></tr><tr><td>Boolean</td><td>torch.bool</td><td>torch.BoolTensor</td><td>torch.cuda.BoolTensor</td></tr></tbody></table></div><h5 id="设置默认Tensor-类型"><a href="#设置默认Tensor-类型" class="headerlink" title="设置默认Tensor 类型"></a>设置默认Tensor 类型</h5><p>Pytorch 可以通过 <code>set_default_tensor_type</code> 函数<strong>设置默认使用的Tensor类型</strong>， 在局部使用完后如果需要其他类型，则还需要重新设置会所需的类型 </p><pre><code class="hljs elm"><span class="hljs-title">torch</span>.set_default_tensor_<span class="hljs-keyword">type</span>(&#x27;torch.<span class="hljs-type">DoubleTensor</span>&#x27;)</code></pre><h5 id="CPU-GPU-互转"><a href="#CPU-GPU-互转" class="headerlink" title="CPU/GPU 互转"></a>CPU/GPU 互转</h5><p>CPU Tensor 和 GPU Tensor 的区别在于， 前者存储在内存中，而后者存储在显存中。两者之间的转换可以通过 <code>.cpu()</code>、<code>.cuda()</code>和 <code>.to(device)</code> 来完成  </p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)  <span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.rand(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>a = a.cuda() <span class="hljs-comment"># CPU -&gt; GPU</span><span class="hljs-meta">&gt;&gt;&gt; </span>a.type()<span class="hljs-string">&#x27;torch.cuda.FloatTensor&#x27;</span><span class="hljs-meta">&gt;&gt;&gt; </span>a = a.cpu() <span class="hljs-comment"># GPU -&gt; CPU</span><span class="hljs-meta">&gt;&gt;&gt; </span>a.type()<span class="hljs-string">&#x27;torch.FloatTensor&#x27;</span><span class="hljs-meta">&gt;&gt;&gt; </span>a = a.to(device) <span class="hljs-comment"># to device</span><span class="hljs-meta">&gt;&gt;&gt; </span>a.type()<span class="hljs-string">&#x27;torch.cuda.FloatTensor&#x27;</span></code></pre><h5 id="判定-Tensor-类型的几种方式"><a href="#判定-Tensor-类型的几种方式" class="headerlink" title="判定 Tensor 类型的几种方式:"></a>判定 Tensor 类型的几种方式:</h5><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>atensor([[<span class="hljs-number">0.6065</span>, <span class="hljs-number">0.0122</span>, <span class="hljs-number">0.4473</span>],        [<span class="hljs-number">0.5937</span>, <span class="hljs-number">0.5530</span>, <span class="hljs-number">0.4663</span>]], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>a.is_cuda  <span class="hljs-comment"># 可以显示是否在显存中</span><span class="hljs-literal">True</span><span class="hljs-meta">&gt;&gt;&gt; </span>a.dtype   <span class="hljs-comment"># Tensor 内部data的类型</span>torch.float32<span class="hljs-meta">&gt;&gt;&gt; </span>a.type()<span class="hljs-string">&#x27;torch.cuda.FloatTensor&#x27;</span>  <span class="hljs-comment"># 可以直接显示 Tensor 类型 = is_cuda + dtype</span></code></pre><h5 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h5><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>atensor([[<span class="hljs-number">0.6065</span>, <span class="hljs-number">0.0122</span>, <span class="hljs-number">0.4473</span>],        [<span class="hljs-number">0.5937</span>, <span class="hljs-number">0.5530</span>, <span class="hljs-number">0.4663</span>]], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>a.type(torch.DoubleTensor)   <span class="hljs-comment"># 使用 type() 函数进行转换</span>tensor([[<span class="hljs-number">0.6065</span>, <span class="hljs-number">0.0122</span>, <span class="hljs-number">0.4473</span>],        [<span class="hljs-number">0.5937</span>, <span class="hljs-number">0.5530</span>, <span class="hljs-number">0.4663</span>]], dtype=torch.float64)<span class="hljs-meta">&gt;&gt;&gt; </span>a = a.double()  <span class="hljs-comment"># 直接使用 int()、long() 、float() 、和 double() 等直接进行数据类型转换进行</span>tensor([[<span class="hljs-number">0.6065</span>, <span class="hljs-number">0.0122</span>, <span class="hljs-number">0.4473</span>],        [<span class="hljs-number">0.5937</span>, <span class="hljs-number">0.5530</span>, <span class="hljs-number">0.4663</span>]], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>, dtype=torch.float64)<span class="hljs-meta">&gt;&gt;&gt; </span>b = torch.randn(<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>b.type_as(a)  <span class="hljs-comment"># 使用 type_as 函数, 并不需要明确具体是哪种类型</span>tensor([[ <span class="hljs-number">0.2129</span>,  <span class="hljs-number">0.1877</span>, <span class="hljs-number">-0.0626</span>,  <span class="hljs-number">0.4607</span>, <span class="hljs-number">-1.0375</span>],        [ <span class="hljs-number">0.7222</span>, <span class="hljs-number">-0.3502</span>,  <span class="hljs-number">0.1288</span>,  <span class="hljs-number">0.6786</span>,  <span class="hljs-number">0.5062</span>],        [<span class="hljs-number">-0.4956</span>, <span class="hljs-number">-0.0793</span>,  <span class="hljs-number">0.7590</span>, <span class="hljs-number">-1.0932</span>, <span class="hljs-number">-0.1084</span>],        [<span class="hljs-number">-2.2198</span>,  <span class="hljs-number">0.3827</span>,  <span class="hljs-number">0.2735</span>,  <span class="hljs-number">0.5642</span>,  <span class="hljs-number">0.6771</span>]], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>,       dtype=torch.float64)</code></pre><h5 id="numpy-array-与-torch-Tensor-互转"><a href="#numpy-array-与-torch-Tensor-互转" class="headerlink" title="numpy array 与　torch Tensor　互转"></a>numpy array 与　torch Tensor　互转</h5><pre><code class="hljs python">torch.Tensor 与 np.ndarray 转换<span class="hljs-comment"># torch.Tensor -&gt; np.ndarray.</span>ndarray = tensor.cpu().numpy()<span class="hljs-comment"># np.ndarray -&gt; torch.Tensor.</span>tensor = torch.from_numpy(ndarray).float()tensor = torch.from_numpy(ndarray.copy()).float()  <span class="hljs-comment"># If ndarray has negative stride</span></code></pre><h5 id="Tensor-相关信息获取"><a href="#Tensor-相关信息获取" class="headerlink" title="Tensor 相关信息获取"></a>Tensor 相关信息获取</h5><pre><code class="hljs python">t.size()/t、.shape   <span class="hljs-comment"># 两者等价， 返回 t 的形状, 可以使用 t.size()[1] 或 t.size(1) 查看列数</span>t.numel() / t.nelement()  <span class="hljs-comment"># 两者等价, 返回 tensor 中元素总个数</span>t.item()  <span class="hljs-comment"># 取出单个 tensor 的值</span>t.dim()  <span class="hljs-comment"># 维度</span></code></pre><h3 id="3-Tensor-Create"><a href="#3-Tensor-Create" class="headerlink" title="3. Tensor Create"></a>3. Tensor Create</h3><h5 id="最基本的Tensor创建方式"><a href="#最基本的Tensor创建方式" class="headerlink" title="最基本的Tensor创建方式"></a>最基本的Tensor创建方式</h5><pre><code class="hljs python">troch.Tensor(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># 会使用默认的类型创建 Tensor, </span>                   <span class="hljs-comment"># 可以通过 torch.set_default_tensor_type(&#x27;torch.DoubleTensor&#x27;) 进行修改</span>torch.DoubleTensor(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># 指定类型创建 Tensor</span>torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])  <span class="hljs-comment"># 通过 list 创建 Tensor</span>                                <span class="hljs-comment"># 将 Tensor转换为list可以使用: t.tolist()</span>torch.from_numpy(np.array([<span class="hljs-number">2</span>, <span class="hljs-number">3.3</span>]) ) <span class="hljs-comment"># 通过 numpy array 创建 tensor</span></code></pre><h5 id="确定初始值的方式创建"><a href="#确定初始值的方式创建" class="headerlink" title="确定初始值的方式创建"></a>确定初始值的方式创建</h5><pre><code class="hljs python">torch.ones(sizes)  <span class="hljs-comment"># 全 1 Tensor     </span>torch.zeros(sizes)  <span class="hljs-comment"># 全 0 Tensor</span>torch.eye(sizes)  <span class="hljs-comment"># 对角线为1，不要求行列一致</span>torch.full(sizes, value) <span class="hljs-comment"># 指定 value</span></code></pre><h5 id="分布"><a href="#分布" class="headerlink" title="分布"></a>分布</h5><pre><code class="hljs python">torch.rand(sizes)  <span class="hljs-comment"># 均匀分布   </span>torch.randn(sizes)   <span class="hljs-comment"># 标准分布</span><span class="hljs-comment"># 正态分布: 返回一个张量，包含从给定参数 means, std 的离散正态分布中抽取随机数。 </span><span class="hljs-comment"># 均值 means 是一个张量，包含每个输出元素相关的正态分布的均值 -&gt; 以此张量的均值作为均值</span><span class="hljs-comment"># 标准差 std 是一个张量，包含每个输出元素相关的正态分布的标准差 -&gt; 以此张量的标准差作为标准差。 </span><span class="hljs-comment"># 均值和标准差的形状不须匹配，但每个张量的元素个数须相同</span>torch.normal(mean=torch.arange(<span class="hljs-number">1.</span>, <span class="hljs-number">11.</span>), std=torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">-0.1</span>))tensor([<span class="hljs-number">-0.1987</span>,  <span class="hljs-number">3.1957</span>,  <span class="hljs-number">3.5459</span>,  <span class="hljs-number">2.8150</span>,  <span class="hljs-number">5.5398</span>,  <span class="hljs-number">5.6116</span>,  <span class="hljs-number">7.5512</span>,  <span class="hljs-number">7.8650</span>,         <span class="hljs-number">9.3151</span>, <span class="hljs-number">10.1827</span>])torch.uniform(<span class="hljs-keyword">from</span>,to) <span class="hljs-comment"># 均匀分布 </span>torch.arange(s, e, steps)  <span class="hljs-comment"># 从 s 到 e，步长为 step</span>torch.linspace(s, e, num)   <span class="hljs-comment"># 从 s 到 e, 均匀切分为 num 份</span><span class="hljs-comment"># ! 注意linespace和arange的区别，前者的最后一个参数是生成的Tensor中元素的数量，而后者的最后一个参数是步长。</span>torch.randperm(m) <span class="hljs-comment"># 0 到 m-1 的随机序列</span><span class="hljs-comment"># ! shuffle 操作</span>tensor[torch.randperm(tensor.size(<span class="hljs-number">0</span>))]</code></pre><h5 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h5><p>Pytorch 有几种不同的复制方式，注意区分</p><div class="table-container"><table><thead><tr><th>Operation</th><th>New/Shared memory</th><th>Still in computation graph</th></tr></thead><tbody><tr><td>tensor.clone()</td><td>New</td><td>Yes</td></tr><tr><td>tensor.detach()</td><td>Shared</td><td>No</td></tr><tr><td>tensor.detach.clone()</td><td>New</td><td>No</td></tr></tbody></table></div><h3 id="4-索引、比较、排序"><a href="#4-索引、比较、排序" class="headerlink" title="4. 索引、比较、排序"></a>4. 索引、比较、排序</h3><h5 id="索引操作"><a href="#索引操作" class="headerlink" title="索引操作"></a>索引操作</h5><pre><code class="hljs python">a.item() <span class="hljs-comment">#　从只包含一个元素的张量中提取值</span>a[row, column]   <span class="hljs-comment"># row 行， cloumn 列</span>a[index]   <span class="hljs-comment"># 第index 行</span>a[:,index]   <span class="hljs-comment"># 第 index 列</span>a[<span class="hljs-number">0</span>, <span class="hljs-number">-1</span>]  <span class="hljs-comment"># 第零行， 最后一个元素</span>a[:index]  <span class="hljs-comment"># 前 index 行</span>a[:row, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>]  <span class="hljs-comment"># 前 row 行， 0和1列</span>a[a&gt;<span class="hljs-number">1</span>]  <span class="hljs-comment"># 选择 a &gt; 1的元素， 等价于 a.masked_select(a&gt;1)</span>torch.nonzero(a) <span class="hljs-comment"># 选择非零元素的坐标，并返回</span>a.clamp(x, y)  <span class="hljs-comment"># 对 Tensor 元素进行限制， 小于x用x代替， 大于y用y代替</span>torch.where(condition, x, y)  <span class="hljs-comment"># 满足condition 的位置输出x， 否则输出y</span><span class="hljs-meta">&gt;&gt;&gt; </span>atensor([[ <span class="hljs-number">6.</span>, <span class="hljs-number">-2.</span>],        [ <span class="hljs-number">8.</span>,  <span class="hljs-number">0.</span>]])<span class="hljs-meta">&gt;&gt;&gt; </span>torch.where(a &gt; <span class="hljs-number">1</span>, torch.full_like(a, <span class="hljs-number">1</span>), a)  <span class="hljs-comment"># 大于1 的部分直接用1代替， 其他保留原值</span>tensor([[ <span class="hljs-number">1.</span>, <span class="hljs-number">-2.</span>],        [ <span class="hljs-number">1.</span>,  <span class="hljs-number">0.</span>]])<span class="hljs-comment">#　得到非零元素</span>torch.nonzero(tensor)               <span class="hljs-comment"># 非零元素的索引</span>torch.nonzero(tensor == <span class="hljs-number">0</span>)          <span class="hljs-comment"># 零元素的索引</span>torch.nonzero(tensor).size(<span class="hljs-number">0</span>)       <span class="hljs-comment"># 非零元素的个数</span>torch.nonzero(tensor == <span class="hljs-number">0</span>).size(<span class="hljs-number">0</span>)  <span class="hljs-comment"># 零元素的个数</span></code></pre><h5 id="比较操作"><a href="#比较操作" class="headerlink" title="比较操作"></a>比较操作</h5><pre><code class="hljs python">gt &gt;    lt &lt;     ge &gt;=     le &lt;=   eq ==    ne != topk(input, k) -&gt; (Tensor, LongTensor)sort(input) -&gt; (Tensor, LongTensor)max/min =&gt; max(tensor)      max(tensor, dim)    max(tensor1, tensor2)</code></pre><p>sort 函数接受两个参数, 其中 参数 0 为按照行排序、1为按照列排序: True 为降序， False 为升序， 返回值有两个， 第一个是排序结果， 第二个是排序序号</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch<span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>atensor([[<span class="hljs-number">-1.8500</span>, <span class="hljs-number">-0.2005</span>,  <span class="hljs-number">1.4475</span>],        [<span class="hljs-number">-1.7795</span>, <span class="hljs-number">-0.4968</span>, <span class="hljs-number">-1.8965</span>],        [ <span class="hljs-number">0.5798</span>, <span class="hljs-number">-0.1554</span>,  <span class="hljs-number">1.6395</span>]])<span class="hljs-meta">&gt;&gt;&gt; </span>a.sort(<span class="hljs-number">0</span>, <span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>] tensor([[ <span class="hljs-number">0.5798</span>, <span class="hljs-number">-0.1554</span>,  <span class="hljs-number">1.6395</span>],        [<span class="hljs-number">-1.7795</span>, <span class="hljs-number">-0.2005</span>,  <span class="hljs-number">1.4475</span>],        [<span class="hljs-number">-1.8500</span>, <span class="hljs-number">-0.4968</span>, <span class="hljs-number">-1.8965</span>]])<span class="hljs-meta">&gt;&gt;&gt; </span>a.sort(<span class="hljs-number">0</span>, <span class="hljs-literal">True</span>)[<span class="hljs-number">1</span>]tensor([[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>],        [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],        [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])<span class="hljs-meta">&gt;&gt;&gt; </span>a.sort(<span class="hljs-number">1</span>, <span class="hljs-literal">True</span>)[<span class="hljs-number">1</span>]tensor([[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],        [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>],        [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]])<span class="hljs-meta">&gt;&gt;&gt; </span>a.sort(<span class="hljs-number">1</span>, <span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]tensor([[ <span class="hljs-number">1.4475</span>, <span class="hljs-number">-0.2005</span>, <span class="hljs-number">-1.8500</span>],        [<span class="hljs-number">-0.4968</span>, <span class="hljs-number">-1.7795</span>, <span class="hljs-number">-1.8965</span>],        [ <span class="hljs-number">1.6395</span>,  <span class="hljs-number">0.5798</span>, <span class="hljs-number">-0.1554</span>]])</code></pre><h3 id="5-Element-wise-和-归并操作"><a href="#5-Element-wise-和-归并操作" class="headerlink" title="5. Element-wise 和 归并操作"></a>5. Element-wise 和 归并操作</h3><p>Element-wise：输出的 Tensor 形状与原始的形状一致</p><pre><code class="hljs python">abs / sqrt / div / exp / fmod / log / pow...cos / sin / asin / atan2 / cosh...ceil / round / floor / truncclamp(input, min, max)sigmoid / tanh...</code></pre><p>归并操作：输出的 Tensor 形状小于原始的 Tensor形状</p><pre><code class="hljs python">mean/sum/median/mode   <span class="hljs-comment"># 均值/和/ 中位数/众数</span>norm/dist  <span class="hljs-comment"># 范数/距离</span>std/var  <span class="hljs-comment"># 标准差/方差</span>cumsum/cumprd <span class="hljs-comment"># 累加/累乘</span></code></pre><h3 id="6-变形操作"><a href="#6-变形操作" class="headerlink" title="6. 变形操作"></a>6. 变形操作</h3><h5 id="view-resize-reshape-调整Tensor的形状"><a href="#view-resize-reshape-调整Tensor的形状" class="headerlink" title="view/resize/reshape  调整Tensor的形状"></a>view/resize/reshape  调整Tensor的形状</h5><ul><li>元素总数必须相同  </li><li>view 和 reshape 可以使用 -1 自动计算维度</li><li>共享内存</li></ul><p>!!!  <code>view()</code> 操作是需要 Tensor 在内存中连续的， 这种情况下需要使用 <code>contiguous()</code> 操作先将内存变为连续。 对于reshape 操作， 可以看做是 <code>Tensor.contiguous().view()</code>.</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.Tensor(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>atensor([[<span class="hljs-number">6.0000e+00</span>, <span class="hljs-number">8.0000e+00</span>],        [<span class="hljs-number">1.0000e+00</span>, <span class="hljs-number">1.8367e-40</span>]])<span class="hljs-meta">&gt;&gt;&gt; </span>a.resize(<span class="hljs-number">4</span>, <span class="hljs-number">1</span>)tensor([[<span class="hljs-number">6.0000e+00</span>],        [<span class="hljs-number">8.0000e+00</span>],        [<span class="hljs-number">1.0000e+00</span>],        [<span class="hljs-number">1.8367e-40</span>]])</code></pre><h5 id="transpose-permute-各维度之间的变换，"><a href="#transpose-permute-各维度之间的变换，" class="headerlink" title="transpose / permute  各维度之间的变换，"></a>transpose / permute  各维度之间的变换，</h5><p>transpose 可以将指定的两个维度的元素进行转置， permute 则可以按照指定的维度进行维度变换</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>xtensor([[[<span class="hljs-number">-0.9699</span>, <span class="hljs-number">-0.3375</span>, <span class="hljs-number">-0.0178</span>]],        [[ <span class="hljs-number">1.4260</span>, <span class="hljs-number">-0.2305</span>, <span class="hljs-number">-0.2883</span>]]])<span class="hljs-meta">&gt;&gt;&gt; </span>x.shapetorch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>])<span class="hljs-meta">&gt;&gt;&gt; </span>x.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># shape =&gt; torch.Size([1, 2, 3])</span>tensor([[[<span class="hljs-number">-0.9699</span>, <span class="hljs-number">-0.3375</span>, <span class="hljs-number">-0.0178</span>],         [ <span class="hljs-number">1.4260</span>, <span class="hljs-number">-0.2305</span>, <span class="hljs-number">-0.2883</span>]]])<span class="hljs-meta">&gt;&gt;&gt; </span>x.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># shape =&gt; torch.Size([1, 2, 3])</span>tensor([[[<span class="hljs-number">-0.9699</span>, <span class="hljs-number">-0.3375</span>, <span class="hljs-number">-0.0178</span>],         [ <span class="hljs-number">1.4260</span>, <span class="hljs-number">-0.2305</span>, <span class="hljs-number">-0.2883</span>]]])<span class="hljs-meta">&gt;&gt;&gt; </span></code></pre><h5 id="squeeze-dim-unsquence-dim"><a href="#squeeze-dim-unsquence-dim" class="headerlink" title="squeeze(dim) / unsquence(dim)"></a>squeeze(dim) / unsquence(dim)</h5><p>处理size为1的维度， 前者用于去除size为1的维度， 而后者则是将指定的维度的size变为1</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>atensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]) <span class="hljs-comment"># shape =&gt; torch.Size([3])</span><span class="hljs-meta">&gt;&gt;&gt; </span>a.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># shape =&gt; torch.Size([1, 3])</span><span class="hljs-meta">&gt;&gt;&gt; </span>a.unqueeze(<span class="hljs-number">0</span>).squeeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># shape =&gt; torch.Size([3])</span></code></pre><h5 id="expand-expand-as-repeat复制元素来扩展维度"><a href="#expand-expand-as-repeat复制元素来扩展维度" class="headerlink" title="expand / expand_as / repeat复制元素来扩展维度"></a>expand / expand_as / repeat复制元素来扩展维度</h5><p>有时需要采用复制的形式来扩展 Tensor 的维度， 这时可以使用 <code>expand</code>， <code>expand()</code> 函数将 size 为 1的维度复制扩展为指定大小， 也可以用 <code>expand_as()</code>函数指定为 示例 Tensor 的维度。</p><p>!! <code>expand</code> 扩大 tensor 不需要分配新内存，只是仅仅新建一个 tensor 的视图，其中通过将 stride 设为0，一维将会扩展位更高维。</p><p><code>repeat</code> 沿着指定的维度重复 tensor。 不同于 <code>expand()</code>，复制的是 tensor 中的数据。</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>atensor([[[<span class="hljs-number">0.3094</span>],         [<span class="hljs-number">0.4812</span>]],        [[<span class="hljs-number">0.0950</span>],         [<span class="hljs-number">0.8652</span>]]])<span class="hljs-meta">&gt;&gt;&gt; </span>a.expand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>) <span class="hljs-comment"># 将第2维的维度由1变为3， 则复制该维的元素，并扩展为3</span>tensor([[[<span class="hljs-number">0.3094</span>, <span class="hljs-number">0.3094</span>, <span class="hljs-number">0.3094</span>],         [<span class="hljs-number">0.4812</span>, <span class="hljs-number">0.4812</span>, <span class="hljs-number">0.4812</span>]],        [[<span class="hljs-number">0.0950</span>, <span class="hljs-number">0.0950</span>, <span class="hljs-number">0.0950</span>],         [<span class="hljs-number">0.8652</span>, <span class="hljs-number">0.8652</span>, <span class="hljs-number">0.8652</span>]]])<span class="hljs-meta">&gt;&gt;&gt; </span>a.repeat(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># 将第二位复制一次</span>tensor([[[<span class="hljs-number">0.3094</span>],         [<span class="hljs-number">0.4812</span>],         [<span class="hljs-number">0.3094</span>],         [<span class="hljs-number">0.4812</span>]],        [[<span class="hljs-number">0.0950</span>],         [<span class="hljs-number">0.8652</span>],         [<span class="hljs-number">0.0950</span>],         [<span class="hljs-number">0.8652</span>]]])</code></pre><h5 id="使用切片操作扩展多个维度"><a href="#使用切片操作扩展多个维度" class="headerlink" title="使用切片操作扩展多个维度"></a>使用切片操作扩展多个维度</h5><pre><code class="hljs fortran">b = a[:,<span class="hljs-keyword">None</span>, <span class="hljs-keyword">None</span>,:] # <span class="hljs-keyword">None</span> 处的维度为１</code></pre><h3 id="7-组合与分块"><a href="#7-组合与分块" class="headerlink" title="7. 组合与分块"></a>7. 组合与分块</h3><p><strong>组合操作</strong> 是将不同的 Tensor 叠加起来。 主要有 <code>cat()</code> 和 <code>torch.stack()</code> 两个函数，cat 即 concatenate 的意思， 是指沿着已有的数据的某一维度进行拼接， 操作后的数据的总维数不变， 在进行拼接时， 除了拼接的维度之外， 其他维度必须相同。 而<code>torch. stack()</code> 函数会新增一个维度， 并按照指定的维度进行叠加。</p><pre><code class="hljs shell">torch.cat(list_of_tensors, dim=0)　  # k 个 (m,n) -&gt; (k*m, n)torch.stack(list_of_tensors, dim=0)   # k 个 (m,n) -&gt; (k*m*n)</code></pre><p><strong>分块操作</strong> 是指将 Tensor 分割成不同的子 Tensor，主要有 <code>torch.chunk()</code> 与 <code>torch.split()</code> 两个函数，前者需要指定分块的数量，而后者则需要指定每一块的大小，以整形或者list来表示。</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.Tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>], [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]])<span class="hljs-meta">&gt;&gt;&gt; </span>torch.chunk(a, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)(tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>]]), tensor([[<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]]))<span class="hljs-meta">&gt;&gt;&gt; </span>torch.chunk(a, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)(tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>],        [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>]]), tensor([[<span class="hljs-number">3.</span>],        [<span class="hljs-number">6.</span>]]))<span class="hljs-meta">&gt;&gt;&gt; </span>torch.split(a, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)(tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],        [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]]),)<span class="hljs-meta">&gt;&gt;&gt; </span>torch.split(a, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], <span class="hljs-number">1</span>)(tensor([[<span class="hljs-number">1.</span>],        [<span class="hljs-number">4.</span>]]), tensor([[<span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],        [<span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]]))</code></pre><h3 id="8-linear-algebra"><a href="#8-linear-algebra" class="headerlink" title="8. linear algebra"></a>8. linear algebra</h3><pre><code class="hljs python">trace  <span class="hljs-comment"># 对角线元素之和(矩阵的迹)</span>diag  <span class="hljs-comment"># 对角线元素</span>triu/tril  <span class="hljs-comment"># 矩阵的上三角/下三角</span>addmm/addbmm/addmv/addr/badbmm...  <span class="hljs-comment"># 矩阵运算</span>t <span class="hljs-comment"># 转置</span>dor/cross <span class="hljs-comment"># 内积/外积</span>inverse <span class="hljs-comment"># 矩阵求逆</span>svd  <span class="hljs-comment"># 奇异值分解</span>torch.mm(tensor1, tensor2)   <span class="hljs-comment"># 矩阵乘法  (m*n) * (n*p) -&gt; (m*p)</span>torch.bmm(tensor1, tensor2) <span class="hljs-comment"># batch的矩阵乘法: (b*m*n) * (b*n*p) -&gt; (b*m*p).</span>torch.mv(tensor, vec) <span class="hljs-comment">#　矩阵向量乘法 (m*n) * (n) = (m)</span>tensor1 * tensor2 <span class="hljs-comment"># Element-wise multiplication.</span></code></pre><h3 id="9-基本机制"><a href="#9-基本机制" class="headerlink" title="9. 基本机制"></a>9. 基本机制</h3><h5 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h5><p>不同形状的 Tensor 进行计算时， 可以自动扩展到较大的相同形状再进行计算。 广播机制的前提是一个 Tensor  至少有一个维度，且从尾部遍历 Tensor 时，两者维度必须相等， 其中七个要么是1， 要么不存在</p><h5 id="向量化操作"><a href="#向量化操作" class="headerlink" title="向量化操作"></a>向量化操作</h5><p>可以在同一时间进行批量地并行计算，例如矩阵运算，以达到更高的计算效率的一种方式:</p><h5 id="共享内存机制"><a href="#共享内存机制" class="headerlink" title="共享内存机制"></a>共享内存机制</h5><p>(1) 直接通过 Tensor 来初始化另一个 Tensor， 或者通过 Tensor 的组合、分块、索引、变形来初始化另一个Tensor， 则这两个 Tensor 共享内存:</p><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>b = a<span class="hljs-meta">&gt;&gt;&gt; </span>c = a.view(<span class="hljs-number">6</span>)<span class="hljs-meta">&gt;&gt;&gt; </span>b[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>] = <span class="hljs-number">0</span><span class="hljs-meta">&gt;&gt;&gt; </span>c[<span class="hljs-number">3</span>] = <span class="hljs-number">4</span><span class="hljs-meta">&gt;&gt;&gt; </span>atensor([[ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.3898</span>, <span class="hljs-number">-0.7641</span>],        [ <span class="hljs-number">4.0000</span>,  <span class="hljs-number">0.6859</span>, <span class="hljs-number">-1.5179</span>]])</code></pre><p>(2) 对于一些操作通过加后缀  “_”  实现 inplace 操作， 如 <code>add_()</code> 和 <code>resize_()</code> 等， 这样操作只要被执行， 本身的 Tensor 就会被改变。</p><pre><code class="hljs angelscript">&gt;&gt;&gt; atensor([[ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.3898</span>, <span class="hljs-number">-0.7641</span>],        [ <span class="hljs-number">4.0000</span>,  <span class="hljs-number">0.6859</span>, <span class="hljs-number">-1.5179</span>]])&gt;&gt;&gt; a.add_(a)tensor([[ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.7796</span>, <span class="hljs-number">-1.5283</span>],        [ <span class="hljs-number">8.0000</span>,  <span class="hljs-number">1.3719</span>, <span class="hljs-number">-3.0358</span>]])</code></pre><p>(3) Tensor与 Numpy 可以高效的完成转换， 并且转换前后的变量共享内存。在进行 Pytorch 不支持的操作的时候， 甚至可以曲线救国， 将 Tensor 转换为 Numpy 类型，操作后再转化为 Tensor</p><pre><code class="hljs clean"># tensor &lt;--&gt; numpyb = a.numpy() # tensor -&gt; numpya = torch.from_numpy(a) # numpy -&gt; tensor</code></pre><p>!!! 需要注意的是，<code>torch.tensor()</code> 总是会进行数据拷贝，新 tensor 和原来的数据不再共享内存。所以如果你想共享内存的话，建议使用 <code>torch.from_numpy()</code> 或者 <code>tensor.detach()</code> 来新建一个 tensor, 二者共享内存。</p><h3 id="10-nn"><a href="#10-nn" class="headerlink" title="10. nn"></a>10. nn</h3><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F</code></pre><h5 id="pad-填充"><a href="#pad-填充" class="headerlink" title="pad 填充"></a>pad 填充</h5><pre><code class="hljs python">nn.ConstantPad2d(padding, value)</code></pre><h5 id="卷积和反卷积"><a href="#卷积和反卷积" class="headerlink" title="卷积和反卷积"></a>卷积和反卷积</h5><pre><code class="hljs python">nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, groups=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>)nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, output_padding=<span class="hljs-number">0</span>, groups=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>, dilation=<span class="hljs-number">1</span>)</code></pre><pre><code class="hljs python"><span class="hljs-comment">#　最常用的两种卷积层设计 3x3 &amp; 1x1</span>conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>)conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">True</span>)</code></pre><h5 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h5><pre><code class="hljs python">nn.MaxPool2d(kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, return_indices=<span class="hljs-literal">False</span>, ceil_mode=<span class="hljs-literal">False</span>)nn.AvgPool2d(kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, ceil_mode=<span class="hljs-literal">False</span>, count_include_pad=<span class="hljs-literal">True</span>)nn.AdaptiveMaxPool2d(output_size, return_indices=<span class="hljs-literal">False</span>)nn.AdaptiveAvgPool2d(output_size)  <span class="hljs-comment"># global avg pool: output_size=1</span>nn.MaxUnpool2d(kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>)</code></pre><h5 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h5><pre><code class="hljs python">nn.Linear(in_features, out_features, bias=<span class="hljs-literal">True</span>)</code></pre><h5 id="防止过拟合相关层"><a href="#防止过拟合相关层" class="headerlink" title="防止过拟合相关层"></a>防止过拟合相关层</h5><pre><code class="hljs python">nn.Dropout2d(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)nn.AlphaDropout(p=<span class="hljs-number">0.5</span>)nn.BatchNorm2d(num_features, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)</code></pre><h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><pre><code class="hljs python">nn.Softplus(beta=<span class="hljs-number">1</span>, threshold=<span class="hljs-number">20</span>)nn.Tanh()nn.ReLU(inplace=<span class="hljs-literal">False</span>)    nn.ReLU6(inplace=<span class="hljs-literal">False</span>)nn.LeakyReLU(negative_slope=<span class="hljs-number">0.01</span>, inplace=<span class="hljs-literal">False</span>)nn.PReLU(num_parameters=<span class="hljs-number">1</span>, init=<span class="hljs-number">0.25</span>)nn.SELU(inplace=<span class="hljs-literal">False</span>)nn.ELU(alpha=<span class="hljs-number">1.0</span>, inplace=<span class="hljs-literal">False</span>)</code></pre><h5 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h5><pre><code class="hljs python">nn.RNNCell(input_size, hidden_size, bias=<span class="hljs-literal">True</span>, nonlinearity=<span class="hljs-string">&#x27;tanh&#x27;</span>)nn.RNN(*args, **kwargs)nn.LSTMCell(input_size, hidden_size, bias=<span class="hljs-literal">True</span>)nn.LSTM(*args, **kwargs)nn.GRUCell(input_size, hidden_size, bias=<span class="hljs-literal">True</span>)nn.GRU(*args, **kwargs)</code></pre><h5 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h5><pre><code class="hljs python">nn.Embedding(num_embeddings, embedding_dim, padding_idx=<span class="hljs-literal">None</span>, max_norm=<span class="hljs-literal">None</span>, norm_type=<span class="hljs-number">2</span>, scale_grad_by_freq=<span class="hljs-literal">False</span>, sparse=<span class="hljs-literal">False</span>, _weight=<span class="hljs-literal">None</span>)</code></pre><h5 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h5><pre><code class="hljs python">nn.Sequential(*args)</code></pre><h5 id="loss-functon"><a href="#loss-functon" class="headerlink" title="loss functon"></a>loss functon</h5><pre><code class="hljs python">nn.BCELoss(weight=<span class="hljs-literal">None</span>, size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)nn.CrossEntropyLoss(weight=<span class="hljs-literal">None</span>, size_average=<span class="hljs-literal">True</span>, ignore_index=<span class="hljs-number">-100</span>, reduce=<span class="hljs-literal">True</span>)<span class="hljs-comment"># CrossEntropyLoss 等价于 log_softmax + NLLLoss</span>nn.L1Loss(size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)nn.KLDivLoss(size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)nn.MSELoss(size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)nn.NLLLoss(weight=<span class="hljs-literal">None</span>, size_average=<span class="hljs-literal">True</span>, ignore_index=<span class="hljs-number">-100</span>, reduce=<span class="hljs-literal">True</span>)nn.NLLLoss2d(weight=<span class="hljs-literal">None</span>, size_average=<span class="hljs-literal">True</span>, ignore_index=<span class="hljs-number">-100</span>, reduce=<span class="hljs-literal">True</span>)nn.SmoothL1Loss(size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)nn.SoftMarginLoss(size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)nn.TripletMarginLoss(margin=<span class="hljs-number">1.0</span>, p=<span class="hljs-number">2</span>, eps=<span class="hljs-number">1e-06</span>, swap=<span class="hljs-literal">False</span>, size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)nn.CosineEmbeddingLoss(margin=<span class="hljs-number">0</span>, size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)</code></pre><h5 id="functional"><a href="#functional" class="headerlink" title="functional"></a>functional</h5><pre><code class="hljs python">nn.functional <span class="hljs-comment"># nn中的大多数layer，在functional中都有一个与之相对应的函数。</span>              <span class="hljs-comment"># nn.functional中的函数和nn.Module的主要区别在于，</span>              <span class="hljs-comment"># 用nn.Module实现的layers是一个特殊的类，都是由 class layer(nn.Module)定义，</span>              <span class="hljs-comment"># 会自动提取可学习的参数。而nn.functional中的函数更像是纯函数，</span>              <span class="hljs-comment"># 由def function(input)定义。</span></code></pre><h5 id="init"><a href="#init" class="headerlink" title="init"></a>init</h5><pre><code class="hljs python">torch.nn.init.uniformtorch.nn.init.normaltorch.nn.init.kaiming_uniformtorch.nn.init.kaiming_normaltorch.nn.init.xavier_normaltorch.nn.init.xavier_uniformtorch.nn.init.sparse</code></pre><h5 id="net"><a href="#net" class="headerlink" title="net"></a>net</h5><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">net_name</span>(<span class="hljs-params">nn.Module</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span>        super(net_name, self).__init__()        self.layer_name = xxxx    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span>         x = self.layer_name(x)                <span class="hljs-keyword">return</span> xnet.parameters()   <span class="hljs-comment"># 获取参数 </span>net.named_parameters  <span class="hljs-comment"># 获取参数及名称</span>net.zero_grad()  <span class="hljs-comment"># 网络所有梯度清零, grad 在反向传播过程中是累加的(accumulated)，</span>                 <span class="hljs-comment"># 这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以反向传播之前需把梯度清零。</span></code></pre><h3 id="11-optim-gt-form-torch-import-optim"><a href="#11-optim-gt-form-torch-import-optim" class="headerlink" title="11. optim -&gt; form torch import optim"></a>11. optim -&gt; form torch import optim</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optimoptim.SGD(params, lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0</span>, dampening=<span class="hljs-number">0</span>, weight_decay=<span class="hljs-number">0</span>, nesterov=<span class="hljs-literal">False</span>)optim.ASGD(params, lr=<span class="hljs-number">0.01</span>, lambd=<span class="hljs-number">0.0001</span>, alpha=<span class="hljs-number">0.75</span>, t0=<span class="hljs-number">1000000.0</span>, weight_decay=<span class="hljs-number">0</span>)optim.LBFGS(params, lr=<span class="hljs-number">1</span>, max_iter=<span class="hljs-number">20</span>, max_eval=<span class="hljs-literal">None</span>, tolerance_grad=<span class="hljs-number">1e-05</span>, tolerance_change=<span class="hljs-number">1e-09</span>, history_size=<span class="hljs-number">100</span>, line_search_fn=<span class="hljs-literal">None</span>)optim.RMSprop(params, lr=<span class="hljs-number">0.01</span>, alpha=<span class="hljs-number">0.99</span>, eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>, momentum=<span class="hljs-number">0</span>, centered=<span class="hljs-literal">False</span>)optim.Rprop(params, lr=<span class="hljs-number">0.01</span>, etas=(<span class="hljs-number">0.5</span>, <span class="hljs-number">1.2</span>), step_sizes=(<span class="hljs-number">1e-06</span>, <span class="hljs-number">50</span>))optim.Adadelta(params, lr=<span class="hljs-number">1.0</span>, rho=<span class="hljs-number">0.9</span>, eps=<span class="hljs-number">1e-06</span>, weight_decay=<span class="hljs-number">0</span>)optim.Adagrad(params, lr=<span class="hljs-number">0.01</span>, lr_decay=<span class="hljs-number">0</span>, weight_decay=<span class="hljs-number">0</span>, initial_accumulator_value=<span class="hljs-number">0</span>)optim.Adam(params, lr=<span class="hljs-number">0.001</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>, amsgrad=<span class="hljs-literal">False</span>)optim.Adamax(params, lr=<span class="hljs-number">0.002</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>)optim.SparseAdam(params, lr=<span class="hljs-number">0.001</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>)optim.Optimizer(params, defaults)optimizer.zero_grad()  <span class="hljs-comment"># 等价于 net.zero_grad() </span>optimizer.step()</code></pre><h3 id="12-learning-rate"><a href="#12-learning-rate" class="headerlink" title="12.  learning rate"></a>12.  learning rate</h3><pre><code class="hljs python"><span class="hljs-comment"># Reduce learning rate when validation accuarcy plateau.</span>scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="hljs-string">&#x27;max&#x27;</span>, patience=<span class="hljs-number">5</span>, verbose=<span class="hljs-literal">True</span>)<span class="hljs-comment"># Cosine annealing learning rate.</span>scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=<span class="hljs-number">80</span>)<span class="hljs-comment"># Reduce learning rate by 10 at given epochs.</span>scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class="hljs-number">50</span>, <span class="hljs-number">70</span>], gamma=<span class="hljs-number">0.1</span>)<span class="hljs-comment"># Learning rate warmup by 10 epochs.</span>scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=<span class="hljs-keyword">lambda</span> t: t / <span class="hljs-number">10</span>)<span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>):    scheduler.step()    train(...); val(...)</code></pre><h3 id="12-save-and-load-model"><a href="#12-save-and-load-model" class="headerlink" title="12. save and load model"></a>12. save and load model</h3><pre><code class="hljs python">torch.save(model.state_dict(), <span class="hljs-string">&#x27;xxxx_params.pth&#x27;</span>)model.load_state_dict(t.load(<span class="hljs-string">&#x27;xxxx_params.pth&#x27;</span>))torch.save(model, <span class="hljs-string">&#x27;xxxx.pth&#x27;</span>)model.torch.load(<span class="hljs-string">&#x27;xxxx.pth&#x27;</span>)all_data = dict(    optimizer = optimizer.state_dict(),    model = model.state_dict(),    info = <span class="hljs-string">u&#x27;model and optim parameter&#x27;</span>)t.save(all_data, <span class="hljs-string">&#x27;xxx.pth&#x27;</span>)all_data = t.load(<span class="hljs-string">&#x27;xxx.pth&#x27;</span>)all_data.keys()</code></pre><h3 id="13-torchvision"><a href="#13-torchvision" class="headerlink" title="13. torchvision"></a>13. torchvision</h3><h5 id="models"><a href="#models" class="headerlink" title="models"></a>models</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> modelsresnet34 = models.resnet34(pretrained=<span class="hljs-literal">True</span>, num_classes=<span class="hljs-number">1000</span>)</code></pre><h5 id="data-augmentation"><a href="#data-augmentation" class="headerlink" title="data augmentation"></a>data augmentation</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<span class="hljs-comment"># transforms.CenterCrop           transforms.Grayscale           transforms.ColorJitter          </span><span class="hljs-comment"># transforms.Lambda               transforms.Compose             transforms.LinearTransformation </span><span class="hljs-comment"># transforms.FiveCrop             transforms.Normalize           transforms.functional           </span><span class="hljs-comment"># transforms.Pad                  transforms.RandomAffine        transforms.RandomHorizontalFlip  </span><span class="hljs-comment"># transforms.RandomApply          transforms.RandomOrder         transforms.RandomChoice         </span><span class="hljs-comment"># transforms.RandomResizedCrop    transforms.RandomCrop          transforms.RandomRotation        </span><span class="hljs-comment"># transforms.RandomGrayscale      transforms.RandomSizedCrop     transforms.RandomVerticalFlip   </span><span class="hljs-comment"># transforms.ToTensor             transforms.Resize              transforms.transforms                                           </span><span class="hljs-comment"># transforms.TenCrop              transforms.Scale               transforms.ToPILImage</span></code></pre><h5 id="自定义-dataset"><a href="#自定义-dataset" class="headerlink" title="自定义 dataset"></a>自定义 dataset</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">my_data</span>(<span class="hljs-params">Dataset</span>):</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, image_path, annotation_path, transform=None</span>):</span>        <span class="hljs-comment"># 初始化， 读取数据集</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span>        <span class="hljs-comment"># 获取数据集的总大小</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span>(<span class="hljs-params">self, id</span>):</span>        <span class="hljs-comment"># 对于制定的 id, 读取该数据并返回    </span></code></pre><p><strong>datasets</strong></p><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, Dataloader<span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">as</span> transformstransform = transforms.Compose([        transforms.ToTensor(), <span class="hljs-comment"># convert to Tensor</span>        transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))]) <span class="hljs-comment"># normalization</span>dataset = ImageFolder(root, transform=transform, target_transform=<span class="hljs-literal">None</span>, loader=default_loader)dataloader = DataLoader(dataset, <span class="hljs-number">2</span>, collate_fn=my_collate_fn, num_workers=<span class="hljs-number">1</span>,shuffle=<span class="hljs-literal">True</span>)<span class="hljs-keyword">for</span> batch_datas, batch_labels <span class="hljs-keyword">in</span> dataloader:    ...</code></pre><h5 id="img-process"><a href="#img-process" class="headerlink" title="img process"></a>img process</h5><pre><code class="hljs python">img = make_grid(next(dataiter)[<span class="hljs-number">0</span>], <span class="hljs-number">4</span>) save_image(img, <span class="hljs-string">&#x27;a.png&#x27;</span>)</code></pre><h5 id="data-Visualization"><a href="#data-Visualization" class="headerlink" title="data Visualization"></a>data Visualization</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToPILImageshow = ToPILImage()  <span class="hljs-comment"># 可以把Tensor转成Image，方便可视化</span>(data, label) = trainset[<span class="hljs-number">100</span>]show((data + <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>).resize((<span class="hljs-number">100</span>, <span class="hljs-number">100</span>))  <span class="hljs-comment"># 应该会自动乘以 255 的</span></code></pre><h3 id="14-Code-Samples"><a href="#14-Code-Samples" class="headerlink" title="14. Code Samples"></a>14. Code Samples</h3><pre><code class="hljs python"><span class="hljs-comment"># torch.device object used throughout this script</span>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> use_cuda <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)model = MyRNN().to(device)<span class="hljs-comment"># train</span>total_loss = <span class="hljs-number">0</span><span class="hljs-keyword">for</span> input, target <span class="hljs-keyword">in</span> train_loader:    input, target = input.to(device), target.to(device)    hidden = input.new_zeros(*h_shape)  <span class="hljs-comment"># has the same device &amp; dtype as `input`</span>    ...  <span class="hljs-comment"># get loss and optimize</span>    total_loss += loss.item()           <span class="hljs-comment"># get Python number from 1-element Tensor</span><span class="hljs-comment"># evaluate</span><span class="hljs-keyword">with</span> torch.no_grad():                   <span class="hljs-comment"># operations inside don&#x27;t track history</span>    <span class="hljs-keyword">for</span> input, target <span class="hljs-keyword">in</span> test_loader:        ...</code></pre><h3 id="15-jit-amp-torchscript"><a href="#15-jit-amp-torchscript" class="headerlink" title="15. jit &amp; torchscript"></a>15. jit &amp; torchscript</h3><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.jit <span class="hljs-keyword">import</span> script, tracetorch.jit.trace(model, torch.rand(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)) 　<span class="hljs-comment"># export model</span><span class="hljs-meta">@torch.jit.script</span></code></pre><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;torch/torch.h&gt;</span></span><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;torch/script.h&gt;</span></span><span class="hljs-meta"># img blob -&gt; img tensor</span>torch::Tensor img_tensor = torch::from_blob(image.data, &#123;<span class="hljs-number">1</span>, image.rows, image.cols, <span class="hljs-number">3</span>&#125;, torch::kByte);img_tensor = img_tensor.permute(&#123;<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>&#125;);img_tensor = img_tensor.toType(torch::kFloat);img_tensor = img_tensor.div(<span class="hljs-number">255</span>);<span class="hljs-meta"># load model</span><span class="hljs-built_in">std</span>::<span class="hljs-built_in">shared_ptr</span>&lt;torch::jit::script::Module&gt; <span class="hljs-keyword">module</span> = torch::jit::load(<span class="hljs-string">&quot;resnet.pt&quot;</span>);<span class="hljs-meta"># forward</span>torch::Tensor output = <span class="hljs-keyword">module</span>-&gt;forward(&#123;img_tensor&#125;).toTensor();</code></pre><h3 id="16-onnx"><a href="#16-onnx" class="headerlink" title="16. onnx"></a>16. onnx</h3><pre><code class="hljs python">torch.onnx.export(model, dummy data, xxxx.proto) <span class="hljs-comment"># exports an ONNX formatted</span>model = onnx.load(<span class="hljs-string">&quot;alexnet.proto&quot;</span>)               <span class="hljs-comment"># load an ONNX model</span>onnx.checker.check_model(model)                  <span class="hljs-comment"># check that the model</span>onnx.helper.printable_graph(model.graph)         <span class="hljs-comment"># print a human readable　representation of the graph</span></code></pre><h3 id="17-Distributed-Training"><a href="#17-Distributed-Training" class="headerlink" title="17. Distributed Training"></a>17. Distributed Training</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist          <span class="hljs-comment"># distributed communication</span><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> Process       <span class="hljs-comment"># memory sharing processes</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The next step of machine learning</title>
    <link href="/2019/03/08/The-next-step-of-machine-learning/"/>
    <url>/2019/03/08/The-next-step-of-machine-learning/</url>
    
    <content type="html"><![CDATA[<p>🔍 The next step of machine learning (机器学习下一步的可能的发展方向)</p><a id="more"></a><h3 id="The-next-step-of-machine-learning："><a href="#The-next-step-of-machine-learning：" class="headerlink" title="The next step of machine learning："></a>The next step of machine learning：</h3><h5 id="1-Anomaly-Detection（让机器知道我不知道）"><a href="#1-Anomaly-Detection（让机器知道我不知道）" class="headerlink" title="1.Anomaly Detection（让机器知道我不知道）"></a>1.Anomaly Detection（让机器知道我不知道）</h5><h5 id="2-Explainable-AI（可解释性AI）"><a href="#2-Explainable-AI（可解释性AI）" class="headerlink" title="2.Explainable AI（可解释性AI）"></a>2.Explainable AI（可解释性AI）</h5><h5 id="3-Adversarial-attack（对抗攻击）"><a href="#3-Adversarial-attack（对抗攻击）" class="headerlink" title="3.Adversarial attack（对抗攻击）"></a>3.Adversarial attack（对抗攻击）</h5><h5 id="4-Life-long-Learning（终身学习）"><a href="#4-Life-long-Learning（终身学习）" class="headerlink" title="4.Life-long Learning（终身学习）"></a>4.Life-long Learning（终身学习）</h5><h5 id="5-Meta-learning-Learning-to-learn（学会如何学习）"><a href="#5-Meta-learning-Learning-to-learn（学会如何学习）" class="headerlink" title="5.Meta-learning/Learning to learn（学会如何学习）"></a>5.Meta-learning/Learning to learn（学会如何学习）</h5><h5 id="6-Few-shot-learning-Zero-shot-Learning（小样本学习）"><a href="#6-Few-shot-learning-Zero-shot-Learning（小样本学习）" class="headerlink" title="6.Few-shot learning/Zero-shot Learning（小样本学习）"></a>6.Few-shot learning/Zero-shot Learning（小样本学习）</h5><h5 id="7-how-to-use-reinforcement-Learning-effectively？（有效使用增强学习）"><a href="#7-how-to-use-reinforcement-Learning-effectively？（有效使用增强学习）" class="headerlink" title="7.how to use reinforcement Learning  effectively？（有效使用增强学习）"></a>7.how to use reinforcement Learning  effectively？（有效使用增强学习）</h5><h5 id="8-Network-Compression（神经网络压缩）"><a href="#8-Network-Compression（神经网络压缩）" class="headerlink" title="8.Network Compression（神经网络压缩）"></a>8.Network Compression（神经网络压缩）</h5><h5 id="9-Unsupervised-Domain-Adaptation（领域自适应）"><a href="#9-Unsupervised-Domain-Adaptation（领域自适应）" class="headerlink" title="9.Unsupervised Domain Adaptation（领域自适应）"></a>9.Unsupervised Domain Adaptation（领域自适应）</h5><h5 id="10-Weak-Supervision-unsupervised-learning-微监督-无监督"><a href="#10-Weak-Supervision-unsupervised-learning-微监督-无监督" class="headerlink" title="10.Weak Supervision/unsupervised learning(微监督/无监督)"></a>10.Weak Supervision/unsupervised learning(微监督/无监督)</h5><h5 id="11-Video-understanding-（视频理解）"><a href="#11-Video-understanding-（视频理解）" class="headerlink" title="11.Video understanding （视频理解）"></a>11.Video understanding （视频理解）</h5><h5 id="12-Graph-network（图网络）"><a href="#12-Graph-network（图网络）" class="headerlink" title="12.Graph network（图网络）"></a>12.Graph network（图网络）</h5><h5 id="13-Transfer-learning（迁移学习）"><a href="#13-Transfer-learning（迁移学习）" class="headerlink" title="13.Transfer learning（迁移学习）"></a>13.Transfer learning（迁移学习）</h5>]]></content>
    
    
    
    <tags>
      
      <tag>Explore</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Learning Toolbox</title>
    <link href="/2018/09/08/Deep-Learning-Toolbox/"/>
    <url>/2018/09/08/Deep-Learning-Toolbox/</url>
    
    <content type="html"><![CDATA[<h2 id="🔧-Deep-Learning-toolbox"><a href="#🔧-Deep-Learning-toolbox" class="headerlink" title="🔧 Deep Learning toolbox"></a>🔧 Deep Learning toolbox</h2><a id="more"></a><h4 id="📚-Datasets-search"><a href="#📚-Datasets-search" class="headerlink" title="📚 Datasets search"></a>📚 Datasets search</h4><ol><li><p><a href="https://www.kaggle.com"><strong>Kaggle</strong></a> </p></li><li><p><a href="https://toolbox.google.com/datasetsearch"><strong>Google Datasets Search Engine</strong></a></p></li><li><p><a href="https://msropendata.com"><strong>Microsoft Datasets</strong></a></p></li><li><p><a href="https://www.visualdata.io"><strong>Computer Vision Datasets</strong></a></p></li><li><p><a href="https://github.com/awesomedata/awesome-public-datasets"><strong>Github awesomedata</strong></a></p></li><li><p><a href="https://archive.ics.uci.edu/ml/datasets.html"><strong>UCI Machine Learning Repository.</strong></a></p></li><li><p><a href="https://registry.opendata.aws"><strong>Amazon Datasets</strong></a></p></li><li><p><strong>Government Datasets:</strong> <a href="https://data.europa.eu/euodp/data/dataset"><strong>EU</strong></a>                 <a href="https://www.data.gov/"><strong>US</strong></a>                 <a href="https://catalogue.data.govt.nz/dataset"><strong>NZL</strong></a>                <a href="https://data.gov.in/"><strong>IND</strong></a>  </p></li></ol><h4 id="🔍-Visualizing-neural-network-architectures"><a href="#🔍-Visualizing-neural-network-architectures" class="headerlink" title="🔍 Visualizing neural network architectures"></a>🔍 Visualizing neural network architectures</h4><ol><li><a href="https://github.com/lutzroeder/Netron"><strong>Netron:</strong></a>  now supports <strong>ONNX</strong>, <strong>Keras</strong>, <strong>CoreML</strong>, <strong>Caffe2</strong>, <strong>Mxnet</strong>, <strong>Pytorch</strong> and <strong>Tensorflow</strong>.</li><li><a href="https://ethereon.github.io/netscope/#/editor"><strong>Netscope:</strong></a>  or  <strong>GraphViz</strong>:   <strong>Caffe</strong>       </li><li><a href="https://github.com/tensorflow/tensorboard"><strong>TensorBoard:</strong></a>   <strong>Tensorflow</strong>  </li><li><a href="https://github.com/szagoruyko/pytorchviz"><strong>Graphviz:</strong></a>   <strong>Pytorch</strong>   </li><li><a href="https://github.com/Murugan-natarajan/mxnet/blob/7cd06109643664442045457a6c318c26d1c728ae/docs/how_to/visualize_graph.md"><strong>mxnet.viz:</strong></a>  <strong>mxnet</strong>   </li><li><a href="https://keras.io/utils/#print_summary"><strong>model.summary():</strong></a>    <strong>keras</strong></li></ol><h4 id="🏷-Lable-Tool"><a href="#🏷-Lable-Tool" class="headerlink" title="🏷 Lable Tool"></a>🏷 Lable Tool</h4><ol><li><p><a href="https://github.com/opencv/cvat"><strong>CVAT:</strong></a>Computer Vision Annotation Tool (CVAT) is a web-based tool which helps to annotate video and images for Computer Vision algorithms</p></li><li><p><a href="https://github.com/wkentaro/labelme"><strong>Labelme:</strong></a> Image Polygonal Annotation with Python</p></li><li><a href="https://github.com/tzutalin/labelImg"><strong>LabelImg</strong></a>：LabelImg is a graphical image annotation tool and label object bounding boxes in images </li><li><a href="https://github.com/AlexeyAB/Yolo_mark"><strong>Yolo_mark:</strong></a>GUI for marking bounded boxes of objects in images for training neural network Yolo v3 and v2</li><li><a href="https://github.com/cvhciKIT/sloth"><strong>Sloth:</strong></a>Sloth is a tool for labeling image and video data for computer vision research.</li><li><a href="http://www.cs.columbia.edu/~vondrick/vatic/"><strong>Vatic:</strong></a>  vatic is a <strong>free, online, interactive video annotation tool</strong> for computer vision research that crowdsources work to Amazon’s Mechanical Turk.</li><li><a href="https://github.com/Microsoft/VoTT/"><strong>VoTT:</strong></a>Visual Object Tagging Tool: An electron app for building end to end Object Detection Models from Images and Videos.</li><li><a href="https://github.com/fidler-lab/polyrnn-pp-pytorch"><strong>Polygon-RNN++:</strong></a> Polygon-RNN++ allows you to train new Polygon-RNN++ models, and run our demo tool on local machines. </li></ol>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Toolbox</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>git常见问题</title>
    <link href="/2018/08/17/git%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <url>/2018/08/17/git%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h4 id="1-Git-基本流程"><a href="#1-Git-基本流程" class="headerlink" title="1. Git 基本流程"></a>1. Git 基本流程</h4><p><img src="/2018/08/17/git%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/gitflow.png" alt></p><h4 id="2-git-commit-规范"><a href="#2-git-commit-规范" class="headerlink" title="2. git commit 规范:"></a>2. git commit 规范:</h4><p>常用的 Git commit 规范，没有强制的规定，主要是翻阅以前的日志会更清晰。</p><p><strong>基本格式:</strong></p><pre><code class="hljs shell">git commit -m &quot;type: description&quot;</code></pre><p><strong>一个比较详细的格式:</strong></p><pre><code class="hljs shell">&lt;type&gt;: (If applied, this commit will...) &lt;subject&gt; (Max 50 char)|&lt;----  Using a Maximum Of 50 Characters  ----&gt;|Explain why this change is being made|&lt;----   Try To Limit Each Line to a Maximum Of 72 Characters   ----&gt;|Provide links or keys to any relevant tickets, articles or other resourcesExample: Github issue #23</code></pre><p><strong>type</strong> 是 commit 的类别，只允许如下几种标识：</p><ul><li><strong>fix: 修复bug</strong></li><li><strong>feat/add:</strong> 新功能 (new feature) </li><li>update: 更新</li><li>refactor : 某个已有功能重构</li><li>perf : 性能优化</li><li>style : 代码格式改变 (formatting, missing semi colons, etc; no code change)</li><li>test: 增加测试代码 (adding or refactoring tests; no production code change)</li><li>docs : 文档改变  (changes to documentation)</li><li>revert: 撤销上一次的commit</li><li>build: 构建工具或构建过程等的变动，如：关联包升级等</li><li><strong>chore</strong>    (updating grunt tasks etc; no production code change)</li></ul><p><strong>description</strong> 是对本次提交的简短描述：</p><ul><li>不超过50个字符。</li><li>推荐以动词开头，如： 设置、修改、增加、删减、撤销等</li></ul><p>参考: <a href="https://gist.github.com/adeekshith/cd4c95a064977cdc6c50">https://gist.github.com/adeekshith/cd4c95a064977cdc6c50</a></p><h4 id="3-git-提交空文件夹"><a href="#3-git-提交空文件夹" class="headerlink" title="3. git 提交空文件夹"></a>3. git 提交空文件夹</h4><p>在空文件夹下建立 .gitkeep</p><pre><code class="hljs shell">touch .gitkeep</code></pre><p>内容如下:</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> Ignore everything <span class="hljs-keyword">in</span> this directory </span>* <span class="hljs-meta">#</span><span class="bash"> Except this file !.gitkeep </span></code></pre><h4 id="4-删除-mac-下面自动生成的-DS-store"><a href="#4-删除-mac-下面自动生成的-DS-store" class="headerlink" title="4. 删除 mac 下面自动生成的 .DS_store"></a>4. 删除 mac 下面自动生成的 .DS_store</h4><p>删除所有隐藏.DS_store文件，打开命令行窗口</p><pre><code class="hljs cpp">sudo find . -name <span class="hljs-string">&quot;.DS_Store&quot;</span> -depth -exec rm &#123;&#125; \;</code></pre><p>设置不再产生选项, 执行如下命令</p><pre><code class="hljs shell">defaults write com.apple.desktopservices DSDontWriteNetworkStores true</code></pre><h4 id="5-通过修改-hosts-提高-github-访问速度"><a href="#5-通过修改-hosts-提高-github-访问速度" class="headerlink" title="5. 通过修改 hosts 提高 github 访问速度"></a>5. 通过修改 hosts 提高 github 访问速度</h4><p>先去 <strong>IPAddress.com 或者 <a href="http://tool.chinaz.com/dns">http://tool.chinaz.com/dns</a></strong> 网站，查询3个与GitHub相关网址对应的IP地址：</p><p>​    1、github.com</p><p>​    2、assets-cdn.github.com</p><p>​    3、github.global.ssl.fastly.net</p><p>页面上会查看到这3个地址对应的 IP 地址，把查询到的IP和地址加到 hosts 文件下, 比如:</p><pre><code class="hljs shell">13.250.177.223   github.com</code></pre><p>我的系统下hosts文件不能直接保存，那就 copy 到其它路径下，修改保存后再 ctrl+x，ctrl+v 回去；</p><p>再清一下系统DNS缓存，</p><p>win: cmd命令打开DOS窗口，输入</p><pre><code class="hljs shell">ipconfig/flushdns</code></pre><p>ubuntu 下:</p><p>安装并重新启动 nscd 守护程序。</p><pre><code class="hljs shell">sudo aptitude install nscdsudo /etc/init.d/nscd restart</code></pre><h4 id="6-如何设置不被追踪的文件"><a href="#6-如何设置不被追踪的文件" class="headerlink" title="6. 如何设置不被追踪的文件"></a>6. 如何设置不被追踪的文件</h4><p>​    有些文件是不想被追踪的， 可以修改  <strong>.git/info/exclude</strong> 文件， 以 # 开头，添加规则即可。</p><p>这里提供一个比较常见的 .gitignore 文件, 将其命名为 .gitignore 然后放到根目录即可。</p><h4 id="7-server-certificate-verification-failed-CAfile-etc-ssl-certs-ca-certificates-crt-CRLfile-none"><a href="#7-server-certificate-verification-failed-CAfile-etc-ssl-certs-ca-certificates-crt-CRLfile-none" class="headerlink" title="7. server certificate verification failed. CAfile:/etc/ssl/certs/ca-certificates.crt CRLfile: none"></a>7. server certificate verification failed. CAfile:/etc/ssl/certs/ca-certificates.crt CRLfile: none</h4><pre><code class="hljs routeros"><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">GIT_SSL_NO_VERIFY</span>=1</code></pre>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>cmake使用</title>
    <link href="/2018/08/17/cmake%E4%BD%BF%E7%94%A8/"/>
    <url>/2018/08/17/cmake%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>cmake 基本使用</p><a id="more"></a><h3 id="一-实例项目"><a href="#一-实例项目" class="headerlink" title="一. 实例项目"></a>一. 实例项目</h3><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> project1  <span class="hljs-comment"># use project2/project3 instead of project1</span></span><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> build</span><span class="hljs-meta">$</span><span class="bash"> cmake ..</span><span class="hljs-meta">$</span><span class="bash"> make</span><span class="hljs-meta">$</span><span class="bash"> ./hello <span class="hljs-comment"># </span></span>Hello World.</code></pre><ul><li><strong>project 1:</strong>  使用cmake 编译项目</li><li><strong>project 2:</strong> 使用动态链接库</li><li><strong>project 3：</strong> 将src和include 分别放到不同的文件夹下</li><li><strong>project4:</strong> 使用第三方库</li></ul><h3 id="二-基本语法"><a href="#二-基本语法" class="headerlink" title="二. 基本语法"></a>二. 基本语法</h3><h5 id="1-CMake-基本使用"><a href="#1-CMake-基本使用" class="headerlink" title="1. CMake 基本使用"></a>1. CMake 基本使用</h5><pre><code class="hljs cmake"><span class="hljs-keyword">cmake_minimum_required</span> (VERSION <span class="hljs-number">2.8</span>)<span class="hljs-keyword">project</span> (Demo1)<span class="hljs-keyword">add_executable</span>(Demo main.cpp)</code></pre><h5 id="2-多个文件同时编译"><a href="#2-多个文件同时编译" class="headerlink" title="2. 多个文件同时编译"></a>2. 多个文件同时编译</h5><pre><code class="hljs cmake"><span class="hljs-comment"># 使用 aux_source_directory(&lt;dir&gt; &lt;variable&gt;)</span><span class="hljs-keyword">aux_source_directory</span>(src DIR_SRCS)<span class="hljs-keyword">add_library</span>(xxxxx SHARED <span class="hljs-variable">$&#123;SRC_FILES&#125;</span>)<span class="hljs-comment"># glob</span><span class="hljs-keyword">file</span>(GLOB_RECURSE SRC_FILES src/*.cpp) <span class="hljs-keyword">add_library</span>(xxxxx SHARED <span class="hljs-variable">$&#123;SRC_FILES&#125;</span>)<span class="hljs-comment"># (3)多个文件同时写</span><span class="hljs-keyword">set</span>(SRC_FILES    src/util/xxx.cpp    src/util/xxxxx_xxxxxxx.cpp    src/xxxx.cpp    )<span class="hljs-keyword">add_library</span>(xxxxx SHARED <span class="hljs-variable">$&#123;SRC_FILES&#125;</span>)</code></pre><h5 id="3-生成动态库或者共享库"><a href="#3-生成动态库或者共享库" class="headerlink" title="3. 生成动态库或者共享库"></a>3. 生成动态库或者共享库</h5><pre><code class="hljs cmake"><span class="hljs-comment"># 静态库</span><span class="hljs-keyword">add_library</span>(slzheliib STATIC libStatic.cpp)<span class="hljs-comment"># 共享库</span><span class="hljs-keyword">add_library</span>(dlib SHARED libShared.cpp)</code></pre><h5 id="4-使用第三方包-以-opencv-和-FFTW-为例"><a href="#4-使用第三方包-以-opencv-和-FFTW-为例" class="headerlink" title="4. 使用第三方包(以 opencv 和 FFTW 为例)"></a>4. 使用第三方包(以 opencv 和 FFTW 为例)</h5><pre><code class="hljs cmake"><span class="hljs-comment"># 编译: add_executable(Demo demo.cpp)</span><span class="hljs-keyword">find_package</span>( OpenCV REQUIRED)<span class="hljs-keyword">if</span> (OpenCV_FOUND)    <span class="hljs-keyword">include_directories</span>(<span class="hljs-variable">$&#123;OpenCV_INCLUDE_DIRS&#125;</span>)    <span class="hljs-keyword">target_link_libraries</span>(Demo <span class="hljs-variable">$&#123;Opencv_LIBS&#125;</span>)<span class="hljs-keyword">endif</span> (OpenCV_FOUND)</code></pre><pre><code class="hljs cmake"><span class="hljs-comment"># 另外一种方式(通过 xxx.cmake 进行)　</span><span class="hljs-comment"># cmake相当于上一种方式中的find_package和include_directories</span><span class="hljs-comment"># 这里以 FFTW 为例</span><span class="hljs-keyword">include</span>(cmake/FindFFTW.cmake)<span class="hljs-keyword">target_link_libraries</span>(udwt-gumbel <span class="hljs-variable">$&#123;OpenCV_LIBS&#125;</span> <span class="hljs-variable">$&#123;FFTW_LIBRARIES&#125;</span>)</code></pre><h5 id="5-如何优化编译选项-Debug-Release模式-："><a href="#5-如何优化编译选项-Debug-Release模式-：" class="headerlink" title="5. 如何优化编译选项(Debug/Release模式)："></a>5. 如何优化编译选项(Debug/Release模式)：</h5><p>​    Debug通常称为调试版本，它包含调试信息，并且不作任何优化，便于程序员调试程序。Release 称为发布版本，它往往是进行了各种优化，使得程序在代码大小和运行速度上都是最优的，以便用户很好地使用。</p><ul><li>在CMakeLists.txt下加入</li></ul><pre><code class="hljs cmake"><span class="hljs-comment"># cmake -DCMAKE_BUILD_TYPE=Debug/Release ..</span><span class="hljs-keyword">SET</span>(CMAKE_BUILD_TYPE <span class="hljs-string">&quot;Release&quot;</span>)<span class="hljs-keyword">if</span> (CMAKE_BUILD_TYPE <span class="hljs-keyword">STREQUAL</span> <span class="hljs-string">&quot;Debug&quot;</span>)    <span class="hljs-keyword">set</span>(CMAKE_CXX_FLAGS_DEBUG <span class="hljs-string">&quot;$ENV&#123;CXXFLAGS&#125; -O0 -Wall -g -ggdb&quot;</span>)    <span class="hljs-keyword">message</span>(STATUS <span class="hljs-string">&quot;CMAKE_BUILD_TYPE = Debug&quot;</span>)<span class="hljs-keyword">else</span>()    <span class="hljs-keyword">set</span>(CMAKE_CXX_FLAGS_RELEASE <span class="hljs-string">&quot;$ENV&#123;CXXFLAGS&#125; -O3 -Wall&quot;</span>)    <span class="hljs-keyword">message</span>(STATUS <span class="hljs-string">&quot;CMAKE_BUILD_TYPE = Release&quot;</span>)<span class="hljs-keyword">endif</span>()</code></pre><p>​        <strong>CMake中有一个变量 CMAKE_BUILD_TYPE ,可以的取值是None、Debug、Release、RelWithDebInfo和MinSizeRel。当这个变量值为Debug的时候,CMake会使用变量 CMAKE_CXX_FLAGS_DEBUG 和 CMAKE_C_FLAGS_DEBUG 中的字符串作为编译选项生成 Makefile</strong></p><div class="table-container"><table><thead><tr><th>CMAKE_BUILD_TYPE</th><th>对应的c编译选项变量</th><th>对应的c++编译选项变量</th></tr></thead><tbody><tr><td>None</td><td>CMAKE_C_FLAGS</td><td>CMAKE_CXX_FLAGS</td></tr><tr><td>Debug</td><td>CMAKE_C_FLAGS_DEBUG</td><td>CMAKE_CXX_FLAGS_DEBUG</td></tr><tr><td>Release</td><td>CMAKE_C_FLAGS_RELEASE</td><td>CMAKE_CXX_FLAGS_RELEASE</td></tr><tr><td>RelWithDebInfo</td><td>CMAKE_C_FLAGS_RELWITHDEBINFO</td><td>CMAKE_CXX_FLAGS_RELWITHDEBINFO</td></tr><tr><td>MinSizeRel</td><td>CMAKE_C_FLAGS_MINSIZEREL</td><td>CMAKE_CXX_FLAGS_MINSIZEREL</td></tr></tbody></table></div><p>​    如果将优化程度调到最高需要设置<code>-O3</code>，最低的是<code>-O0</code>即不做优化，添加调试信息的参数是<code>-g  -ggdb</code>，如果不添加这个参数，调试信息就不会被包含在生成的二进制中。</p><p>（1）<code>-O</code>，<code>-O1</code> 这两个命令的效果是一样的，目的都是在不影响编译速度的前提下，尽量采用一些优化算法降低代码大小和可执行代码的运行速度。 </p><p>（2）<code>-O2</code> 该优化选项会牺牲部分编译速度，除了执行<code>-O1</code>所执行的所有优化之外，还会采用几乎所有的目标配置支持的优化算法，用以提高目标代码的运行速度。 </p><p>（3） <code>-O3</code> 该选项除了执行<code>-O2</code>所有的优化选项之外，一般都是采取很多向量化算法，提高代码的并行执行程度，利用现代CPU中的流水线，Cache等。这个选项会提高执行代码的大小，当然会降低目标代码的执行时间。</p><p>（4）<code>-Os</code> 这个优化标识和<code>-O3</code>有异曲同工之妙，当然两者的目标不一样，<code>-O3</code>的目标是宁愿增加目标代码的大小，也要拼命的提高运行速度，但是这个选项是在<code>-O2</code>的基础之上，尽量的降低目标代码的大小，这对于存储容量很小的设备来说非常重要。为了降低目标代码大小，会禁用下列优化选项，一般就是压缩内存中的对齐空白(alignment padding)</p><p>（5）<code>-Ofast</code> 该选项将不会严格遵循语言标准，除了启用所有的<code>-O3</code>优化选项之外，也会针对某些语言启用部分优化。如：<code>-ffast-math</code> .</p><p>（6）<code>-Og</code>: 该标识会精心挑选部分与<code>-g</code>选项不冲突的优化选项，当然就能提供合理的优化水平，同时产生较好的可调试信息和对语言标准的遵循程度。</p><h5 id="6-设定使用-C-11"><a href="#6-设定使用-C-11" class="headerlink" title="6. 设定使用 C++11"></a>6. 设定使用 C++11</h5><pre><code class="hljs cmake"><span class="hljs-comment"># Use C++11</span><span class="hljs-keyword">set</span>(CMAKE_CXX_STANDARD <span class="hljs-number">11</span>)<span class="hljs-keyword">set</span>(CMAKE_CXX_STANDARD_REQUIRED <span class="hljs-keyword">ON</span>)<span class="hljs-keyword">message</span>(STATUS <span class="hljs-string">&quot;C++11 support has been enabled by default.&quot;</span>)</code></pre><h5 id="7-自定义编译选项-以-OpenMP-和-SSE为例"><a href="#7-自定义编译选项-以-OpenMP-和-SSE为例" class="headerlink" title="7. 自定义编译选项(以 OpenMP 和 SSE为例)"></a>7. 自定义编译选项(以 OpenMP 和 SSE为例)</h5><pre><code class="hljs cmake"><span class="hljs-comment"># Use OpenMP</span><span class="hljs-keyword">option</span>(USE_OPENMP      <span class="hljs-string">&quot;Set to ON to build use openmp&quot;</span>  <span class="hljs-keyword">ON</span>)<span class="hljs-keyword">if</span> (USE_OPENMP)    <span class="hljs-keyword">find_package</span>(OpenMP QUIET)    <span class="hljs-keyword">if</span> (OPENMP_FOUND)        <span class="hljs-keyword">message</span>(STATUS <span class="hljs-string">&quot;Use OpenMP&quot;</span>)        <span class="hljs-keyword">add_definitions</span>(-DUSE_OPENMP)        <span class="hljs-keyword">set</span>(CMAKE_C_FLAGS <span class="hljs-string">&quot;$&#123;CMAKE_C_FLAGS&#125; $&#123;OpenMP_C_FLAGS&#125;&quot;</span>)        <span class="hljs-keyword">set</span>(CMAKE_CXX_FLAGS <span class="hljs-string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; $&#123;OpenMP_CXX_FLAGS&#125;&quot;</span>)        <span class="hljs-keyword">set</span>(CMAKE_EXE_LINKER_FLAGS <span class="hljs-string">&quot;$&#123;CMAKE_EXE_LINKER_FLAGS&#125; $&#123;OpenMP_EXE_LINKER_FLAGS&#125;&quot;</span>)    <span class="hljs-keyword">endif</span>()<span class="hljs-keyword">endif</span>()<span class="hljs-comment"># Use SSE</span><span class="hljs-keyword">option</span>(USE_SSE         <span class="hljs-string">&quot;Set to ON to build use SSE&quot;</span>  <span class="hljs-keyword">ON</span>)<span class="hljs-keyword">if</span> (USE_SSE)    <span class="hljs-keyword">add_definitions</span>(-DUSE_SSE)    <span class="hljs-keyword">message</span>(STATUS <span class="hljs-string">&quot;Use SSE&quot;</span>)    <span class="hljs-keyword">set</span>(CMAKE_CXX_FLAGS <span class="hljs-string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -msse4.1&quot;</span>)<span class="hljs-keyword">endif</span>()</code></pre><h5 id="8-添加版本号"><a href="#8-添加版本号" class="headerlink" title="8. 添加版本号"></a>8. 添加版本号</h5><pre><code class="hljs cmake"><span class="hljs-comment"># Version (DEMO is Project Name)</span><span class="hljs-keyword">set</span> (DEMO_VERSION_MAJOR <span class="hljs-number">0</span>)<span class="hljs-keyword">set</span> (DEMO_VERSION_MINOR <span class="hljs-number">1</span>)<span class="hljs-keyword">message</span>(STATUS <span class="hljs-string">&quot;PROJECT VERSION IS $&#123;DEMO_VERSION_MAJOR&#125;.$&#123;DEMO_VERSION_MINOR&#125;&quot;</span>)</code></pre><h5 id="9-设置子文件夹"><a href="#9-设置子文件夹" class="headerlink" title="9. 设置子文件夹"></a>9. 设置子文件夹</h5><p>在主项目的 <code>CMakeLists.txt</code> 中, 可以使用 <code>add_subdirectory</code> 来添加 子文件夹, 然后在子文件夹中也要有一个 <code>CMakeLists.txt</code> 文件.</p><pre><code class="hljs cmake"><span class="hljs-keyword">add_subdirectory</span>(benchmark)<span class="hljs-keyword">add_subdirectory</span>(src)<span class="hljs-keyword">add_subdirectory</span>(tools)</code></pre><p>项目的基本目录为:</p><pre><code class="hljs cmake">.├── src│   ├── CMakeLists.txt│   └── xxx.cpp├── tools│   ├── CMakeLists.txt│   └── xxx.cpp├── benchmark│   ├── CMakeLists.txt│   └── xxx.cpp├── main.cpp└── CMakeLists.txt</code></pre><h5 id="10-测试"><a href="#10-测试" class="headerlink" title="10. 测试"></a>10. 测试</h5><pre><code class="hljs cmake"><span class="hljs-keyword">enable_testing</span>()<span class="hljs-keyword">add_test</span> (test_5_2 Demo <span class="hljs-number">5</span> <span class="hljs-number">2</span>)<span class="hljs-keyword">set_tests_properties</span> (test_5_2 PROPERTIES PASS_REGULAR_EXPRESSION <span class="hljs-string">&quot;is 25&quot;</span>)<span class="hljs-keyword">add_test</span> (test_2_10 Demo <span class="hljs-number">2</span> <span class="hljs-number">10</span>)<span class="hljs-keyword">set_tests_properties</span> (test_2_10 PROPERTIES PASS_REGULAR_EXPRESSION <span class="hljs-string">&quot;is 1024&quot;</span>)</code></pre><p>set_tests_properties 中的 <code>PASS_REGULAR_EXPRESSION</code> 用来测试输出是否包含后面跟着的字符串。</p><h5 id="11-设置安装与与生成安装包"><a href="#11-设置安装与与生成安装包" class="headerlink" title="11.设置安装与与生成安装包"></a>11.设置安装与与生成安装包</h5><p>生成的 Demo 文件将会被复制到 <code>/usr/local/bin</code> 中，而 demo.h 和则会被复制到 <code>/usr/local/include</code> 中。顺带一提的是，这里的 <code>/usr/local/</code> 是默认安装到的根目录，可以通过修改 <code>CMAKE_INSTALL_PREFIX</code> 变量的值来指定这些文件应该拷贝到哪个根目录。</p><pre><code class="hljs cmake"><span class="hljs-keyword">install</span> (TARGETS Demo DESTINATION bin) <span class="hljs-keyword">install</span> (FILES demo.h DESTINATION <span class="hljs-keyword">include</span>)</code></pre><h5 id="12-指定输出目录"><a href="#12-指定输出目录" class="headerlink" title="12. 指定输出目录"></a>12. 指定输出目录</h5><pre><code class="hljs cmake"><span class="hljs-keyword">SET</span>(EXECUTABLE_OUTPUT_PATH <span class="hljs-variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/bin)       <span class="hljs-comment"># 设置可执行文件的输出目录</span><span class="hljs-keyword">SET</span>(LIBRARY_OUTPUT_PATH <span class="hljs-variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/lib)           <span class="hljs-comment"># 设置库文件的输出目录</span></code></pre><h5 id="13-使用第三方软件-但是不安装-以opencv为例"><a href="#13-使用第三方软件-但是不安装-以opencv为例" class="headerlink" title="13.  使用第三方软件,  但是不安装(以opencv为例)"></a>13.  使用第三方软件,  但是不安装(以opencv为例)</h5><p>将所需的头文件放在  <code>include</code>  文件夹中, 将需要的 共享库 <code>*.so</code> 放在 <code>lib</code> 文件夹中.</p><pre><code class="hljs cmake"><span class="hljs-keyword">include_directories</span>(<span class="hljs-keyword">include</span>)<span class="hljs-keyword">set</span>(opencv_lib   <span class="hljs-variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/lib/libopencv_highgui.so.<span class="hljs-number">3.3</span>   <span class="hljs-variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/lib/libopencv_videoio.so.<span class="hljs-number">3.3</span>   <span class="hljs-variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/lib/libopencv_imgcodecs.so.<span class="hljs-number">3.3</span>   <span class="hljs-variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/lib/libopencv_imgproc.so.<span class="hljs-number">3.3</span>   <span class="hljs-variable">$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;</span>/lib/libopencv_core.so.<span class="hljs-number">3.3</span>)<span class="hljs-keyword">target_link_libraries</span>(demo <span class="hljs-variable">$&#123;opencv_lib&#125;</span>)</code></pre><h3 id="三-Cmake常见问题"><a href="#三-Cmake常见问题" class="headerlink" title="三. Cmake常见问题"></a>三. Cmake常见问题</h3><h5 id="1-Question-make-Nothing-to-be-done-for-all-Solution"><a href="#1-Question-make-Nothing-to-be-done-for-all-Solution" class="headerlink" title="1. Question : make: Nothing to be done for `all` Solution"></a>1. <strong>Question</strong> : <strong>make: Nothing to be done for `all` Solution</strong></h5><p>这句提示是说明你已经编译好了，而且没有对代码进行任何改动。若想重新编译，可以先删除以前编译产生的目标文件，然后使用 make clean，然后再使用 make。</p><h5 id="2-Question-gcc-make-cmake-的联系与区别"><a href="#2-Question-gcc-make-cmake-的联系与区别" class="headerlink" title="2. Question: gcc/make/cmake 的联系与区别"></a>2. Question: gcc/make/cmake 的联系与区别</h5><p>​      gcc是GNU Compiler Collection（就是GNU编译器套件），也可以简单认为是编译器，它可以编译很多种编程语言（括C、C++、Objective-C、Fortran、Java等等）。<strong>当你的程序只有一个源文件时，直接就可以用gcc命令编译它</strong>。但是当你的程序包含很多个源文件时，用gcc命令逐个去编译时，你就很容易混乱而且工作量大。这时可以使用make 工具，<strong>make工具可以看成是一个智能的批处理工具，它本身并没有编译和链接的功能，而是用类似于批处理的方式—— 通过调用makefile文件中用户指定的命令来进行编译和链接的</strong>。简单的说就像一首歌的乐谱，make工具就像指挥家，指挥家根据乐谱指挥整个乐团怎么样演奏，make工具就根据makefile中的命令进行编译和链接的。makefile命令中就包含了调用gcc（也可以是别的编译器）去编译某个源文件的命令。</p><p>​        makefile在一些简单的工程完全可以人工手写，但是当工程非常大的时候，手写makefile也是非常麻烦的，如果换了个平台makefile又要重新修改。这时候就出现了Cmake这个工具，<strong>cmake就可以更加简单的生成makefile文件给上面那个make用</strong>。当然cmake还有其他功能，就是<strong>可以跨平台</strong>生成对应平台能用的makefile，你不用再自己去修改了。可是cmake根据什么生成makefile呢？它又要<strong>根据一个叫CMakeLists.txt文件（学名：组态档）去生成makefile</strong>。到最后CMakeLists.txt文件谁写啊？亲，是你自己手写的。三者的关系可以如下面的简图所示：</p><p><img src="/2018/08/17/cmake%E4%BD%BF%E7%94%A8/cmake.png" alt="p4107"></p><h5 id="3-常见的路径"><a href="#3-常见的路径" class="headerlink" title="3. 常见的路径"></a>3. 常见的路径</h5><p><strong>PROJECT_SOURCE_DIR:</strong> 含有 <code>project()</code> 指令的<code>CMakeLists.txt</code> 文件夹。</p><p><strong>CMAKE_CURRENT_SOURCE_DIR:</strong> 目前正在处理的CMakeLists.txt 所在位置。</p><h5 id="4-指定输出路径"><a href="#4-指定输出路径" class="headerlink" title="4. 指定输出路径"></a>4. 指定输出路径</h5><pre><code class="hljs cmake"><span class="hljs-keyword">SET</span>(EXECUTABLE_OUTPUT_PATH <span class="hljs-variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/../bin)       <span class="hljs-comment">#设置可执行文件的输出目录</span><span class="hljs-keyword">SET</span>(LIBRARY_OUTPUT_PATH <span class="hljs-variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/../lib)           <span class="hljs-comment">#设置库文件的输出目录</span></code></pre><h3 id="四-参考链接"><a href="#四-参考链接" class="headerlink" title="四. 参考链接"></a>四. 参考链接</h3><p>[0] <a href="https://cmake.org">CMake官方网站</a></p><p>[1] <a href="https://www.hahack.com/codes/cmake/">CMake 入门实战</a></p><p>[2]  <a href="https://cmake.org/cmake-tutorial/">CMake-tutorial</a></p><p>[3] 《CMake Pratice》</p><p>[4]  <a href="https://cmake.org/mailing-lists/">cmake-maillist</a></p><p>[5]  <a href="https://github.com/onqtam/awesome-cmake">awesome-camke</a></p><p>[6] 《Mastering CMake》</p>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>gcc和g++使用</title>
    <link href="/2018/08/17/gcc%E5%92%8Cg++%E4%BD%BF%E7%94%A8/"/>
    <url>/2018/08/17/gcc%E5%92%8Cg++%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>gcc/g++ 和 makefile 的基本用法</p><a id="more"></a><h4 id="gcc-g-流程"><a href="#gcc-g-流程" class="headerlink" title="gcc/g++流程"></a>gcc/g++流程</h4><p><strong>预处理</strong>　－&gt;  <strong>编译</strong>　－&gt;　<strong>汇编</strong>　－&gt;  <strong>链接</strong></p><h4 id="常见选项"><a href="#常见选项" class="headerlink" title="常见选项"></a>常见选项</h4><h6 id="目录选项"><a href="#目录选项" class="headerlink" title="目录选项"></a>目录选项</h6><p><strong>-L[dir]</strong>     　指定搜索目录</p><p>默认会在<code>/lib</code>和<code>/usr/lib</code>和<code>/usr/local/lib</code>三个目录进行搜索</p><p>👉  动态添加方法　<code>pkg-config [pkg_name] —libs</code></p><p><strong>-l[libname]</strong>      指定程序要链接的库，-l参数紧接着就是库名</p><p>例如： 库名是<code>m</code>，　库文件名　<code>libm.so</code>　 链接　<code>-lm</code></p><h6 id="头文件设置"><a href="#头文件设置" class="headerlink" title="头文件设置"></a>头文件设置</h6><p><strong>-I</strong>            指定头文件目录</p><p><code>/usr/include</code>目录一般是不用指定的，gcc知道去那里找，但是如果头文件不在 <code>/usr/icnclude</code> 里我们就要用-I参数指定了，比如头文件放在 <code>/myinclude</code>目录里，那编译命令行就要加上    <code>-I /myinclude</code></p><p><strong>—include [lib]</strong>      用来包含头文件</p><p>一般情况下包含头文件都在源码里用<code>＃include xxxxxx</code>实现， <code>--include</code>  参数很少用。</p><h6 id="连接器选项"><a href="#连接器选项" class="headerlink" title="连接器选项"></a>连接器选项</h6><p><strong>-share</strong>        共享库:尽量使用动态库</p><p><strong>-static</strong> 　    静态库: 禁止使用动态库</p><h6 id="警告选项"><a href="#警告选项" class="headerlink" title="警告选项"></a>警告选项</h6><p><strong>-w</strong>            禁止所有警告信息</p><p><strong>-Wall</strong>             开启大部分警告提示</p><p><strong>-Werror</strong>        视警告为错误</p><h6 id="指定优化级别"><a href="#指定优化级别" class="headerlink" title="指定优化级别"></a>指定优化级别</h6><p>包含<code>-O1</code>/<code>-O2</code>  和<code>-O3</code>等级别，推荐使用<code>-O3</code>　进行优化</p><h6 id="生成位置无关目标码"><a href="#生成位置无关目标码" class="headerlink" title="生成位置无关目标码"></a>生成位置无关目标码</h6><p><strong>-fpic</strong>            适用于共享库</p><p><strong>-fPIC</strong>　          适用于动态链接库</p><h6 id="语言选项"><a href="#语言选项" class="headerlink" title="语言选项"></a>语言选项</h6><p><strong>-ansi</strong>         #　支持符合ＡＮＳＩ标准的Ｃ程序</p><p><strong>-std=c99</strong>      # c99标准</p><p><strong>-std=C++11</strong>    #　支持　c++11　标准  &lt;-</p><h6 id="常见环境变量"><a href="#常见环境变量" class="headerlink" title="常见环境变量"></a>常见环境变量</h6><p><strong>PKG_CONFIG_PATH</strong>：用来指定pkg-config用到的pc文件的路径，默认是/usr/lib/pkgconfig，pc文件是文本文件，扩展名是.pc，里面定义开发包的安装路径，Libs参数和Cflags参数等等。</p><p><strong>CC</strong>：用来指定c编译器。</p><p><strong>CXX</strong>：用来指定cxx编译器。</p><p><strong>LIBS</strong>：跟上面的—libs作用差不多。</p><p><strong>CFLAGS</strong>:跟上面的—cflags作用差不多。</p><p><strong>CC，CXX，LIBS，CFLAGS</strong> 手动编译时一般用不上，在做configure时有时用到，一般情况下不用管。</p><p>环境变量设定方法：<strong>export  ENV_NAME=xxx</strong></p><h4 id="makefile-基本格式"><a href="#makefile-基本格式" class="headerlink" title="makefile 基本格式"></a>makefile 基本格式</h4><pre><code class="hljs cmake"><span class="hljs-comment"># 指定编译器</span><span class="hljs-comment"># 设置编译选项</span><span class="hljs-comment"># 库 &amp; 头文件</span><span class="hljs-comment"># 变量设置</span><span class="hljs-comment"># 语句：</span><span class="hljs-comment"># 编译目标：依赖文件</span><span class="hljs-comment">#     运行脚本</span><span class="hljs-comment"># 清理脚本</span></code></pre><h4 id="makefile-模板"><a href="#makefile-模板" class="headerlink" title="makefile 模板"></a>makefile 模板</h4><pre><code class="hljs makefile">CC = g++ CFLAGS += -g -O3 -WallINC += -I. `pkg-config --cflags opencv`　LIBS += `pkg-config --libs opencv`　　TARGET = main.binOBJS += main.o \        config.o <span class="hljs-section">all:<span class="hljs-variable">$(TARGET)</span></span><span class="hljs-variable">$(TARGET)</span>:<span class="hljs-variable">$(OBJS)</span>    <span class="hljs-variable">$(CC)</span> <span class="hljs-variable">$(INC)</span> <span class="hljs-variable">$(CFLAGS)</span> <span class="hljs-variable">$(OBJS)</span> -o <span class="hljs-variable">$(TARGET)</span> <span class="hljs-variable">$(LIBS)</span><span class="hljs-variable">$(OBJS)</span>:%.o:%.cpp    <span class="hljs-variable">$(CC)</span> <span class="hljs-variable">$(INC)</span> <span class="hljs-variable">$(CFLAGS)</span> -c <span class="hljs-variable">$&lt;</span> -o <span class="hljs-variable">$@</span><span class="hljs-meta"><span class="hljs-meta-keyword">.PHONY</span>:clean</span><span class="hljs-section">clean:　</span>    rm -r *.o <span class="hljs-variable">$(TARGET)</span></code></pre><p>参考:</p><p>&lt;跟我一起写 Makefile&gt; 陈皓</p>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Linux基础知识</title>
    <link href="/2018/08/17/Linux%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    <url>/2018/08/17/Linux%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<p>Linux 一些最最最基本的常识</p><a id="more"></a><h4 id="1-GNU-vs-BSD"><a href="#1-GNU-vs-BSD" class="headerlink" title="1. GNU vs BSD"></a>1. GNU vs BSD</h4><h5 id="典型的类-unix-操作系统包括两个版本："><a href="#典型的类-unix-操作系统包括两个版本：" class="headerlink" title="典型的类 unix 操作系统包括两个版本："></a>典型的类 unix 操作系统包括两个版本：</h5><ul><li>BSD 版本（ BSDs &amp; Mac OS）</li><li>GNU 版本（Linux）</li></ul><h5 id="Linux内核-vs-发行版"><a href="#Linux内核-vs-发行版" class="headerlink" title="Linux内核 vs 发行版"></a>Linux内核 vs 发行版</h5><p>​        linux内核是一种开放源码的操作系统，提供硬件抽象层、硬盘及文件系统控制及多任务功能的系统核心程序。<br>​        linux发行版基于linux内核源码，将Linux系统的内核与外围实用程序、软件和文档包装起来，并提供一些系统安装界面和系统配置、设定与管理工具，就构成了一种发行版本(distribution)。</p><p>各种发行版可以主要分为两大系统：</p><ul><li>以 RPM 方式安装软件的系统，包括Red Hat， Fedora， SuSE 等。</li><li>以 Debian 的 dpkg方式安装的软件的系统，包括 Debian， Ubuntu，B2D 等。</li></ul><h4 id="2-文件的目录"><a href="#2-文件的目录" class="headerlink" title="2. 文件的目录"></a>2. 文件的目录</h4><pre><code class="hljs shell">/              根目录├── bin     存放用户二进制文件  # ├── boot    存放内核引导配置文件  #├── dev     存放设备文件  # ├── etc     存放系统配置文件  # ├── home    用户主目录  # ├── lib     动态共享库├── lost+found  文件系统恢复时的恢复文件├── media   可卸载存储介质挂载点├── mnt     文件系统临时挂载点  # ├── opt     附加的应用程序包├── proc    系统内存的映射目录，提供内核与进程信息  # ├── root    root 用户主目录  #├── sbin    存放系统二进制文件  # ├── srv     存放服务相关数据├── sys     sys 虚拟文件系统挂载点├── tmp     存放临时文件├── usr     存放用户应用程序└── var     存放邮件、系统日志等变化文件   #</code></pre><h4 id="3-软连接和硬链接"><a href="#3-软连接和硬链接" class="headerlink" title="3. 软连接和硬链接"></a>3. 软连接和硬链接</h4><h5 id="硬链接"><a href="#硬链接" class="headerlink" title="硬链接"></a>硬链接</h5><p>硬链接就是同一个文件使用了多个别名，他们有共同的 inode和 data block块。硬链接可由命令 <code>ln</code> 创建硬链接：</p><pre><code class="hljs ebnf"><span class="hljs-attribute">ln oldfile newfile</span></code></pre><ul><li>只能对已存在的文件进行创建；不能对目录进行创建，只可对文件创建；</li><li>删除一个硬链接文件并不影响其他有相同 inode 号的文件。</li></ul><h5 id="软链接"><a href="#软链接" class="headerlink" title="软链接"></a>软链接</h5><p>软链接就是一个普通文件，只是数据块内容是另一文件的路径名的指向。软链接有着自己的 inode 号以及用户数据块。可以使用<code>ln -s</code> 创建软链接：</p><pre><code class="hljs shell">ln -s oldfile newfile</code></pre><ul><li>软链接有自己的文件属性及权限等；</li><li>可对不存在的文件或目录创建软链接；软链接可对文件或目录创建；</li><li>创建软链接时，链接计数 <strong>i_nlink</strong> 不会增加；</li><li>删除软链接并不影响被指向的文件，但若被指向的原文件被删除，则相关软连接被称为死链接（即 <strong>dangling link</strong>，若被指向路径文件被重新创建，死链接可恢复为正常的软链接）。</li></ul><p><img src="/2018/08/17/Linux%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/hard_soft_link.png" alt="hard link vs soft link"></p><h4 id="4-环境变量"><a href="#4-环境变量" class="headerlink" title="4. 环境变量"></a>4. 环境变量</h4><p>查看环境变量：<code>echo $PATH</code></p><p> 将指定路径到系统路径(一次性)： <code>export PATH=$PATH:/path/to/dir</code>  </p><p>将指定路径到系统路径(长期)：</p><pre><code class="hljs shell">vim ~/.bashrcexport PATH=$PATH:/path/to/dirsource ~/.bashrc</code></pre><h4 id="5-常见的开源许可证选择"><a href="#5-常见的开源许可证选择" class="headerlink" title="5. 常见的开源许可证选择"></a>5. 常见的开源许可证选择</h4><p><img src="/2018/08/17/Linux%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/license.png" alt="License"></p><p>[参考资料]</p><ol><li><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-hardandsymb-links/">https://www.ibm.com/developerworks/cn/linux/l-cn-hardandsymb-links/</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Linux常见问题</title>
    <link href="/2018/08/17/Linux%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <url>/2018/08/17/Linux%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<p>主要用来记录一些 ubuntu(linux) 使用过程中碰到的问题</p><a id="more"></a><h4 id="1-如何挂载磁盘"><a href="#1-如何挂载磁盘" class="headerlink" title="1. 如何挂载磁盘"></a>1. 如何挂载磁盘</h4><p>（１） 找到未分配的磁盘，　这里是　<code>/dev/sdb</code></p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo fdisk -l  <span class="hljs-comment"># 找到未分配的磁盘</span></span>Disk /dev/sdb: 1.8 TiB, 2000398934016 bytes, 3907029168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytesDisklabel type: dosDisk identifier: 0x76738fb3</code></pre><p>（２）对该磁盘使用fdisk 命令建立分区:</p><pre><code class="hljs shell">sudo fdisk /dev/sdb<span class="hljs-meta">#</span><span class="bash"> m -&gt; n -&gt; 一路回车　－&gt; w(保存退出)</span><span class="hljs-meta">$</span><span class="bash"> sudo fdisk -l <span class="hljs-comment">#　此时可以发现设备</span></span>Device     Boot Start        End    Sectors  Size Id Type/dev/sdb1        2048 3907029167 3907027120  1.8T 83 Linux</code></pre><p>（３）格式化分区，　并建立文件系统</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo mkfs -t ext4 /dev/sdb1  <span class="hljs-comment"># -t　指定文件系统格式</span></span></code></pre><p>（４）挂载磁盘到指定位置</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> mount /dev/sdb1 /home/data/</span><span class="hljs-meta">#</span><span class="bash"> 区分 sdb和sdb1, sdb表示磁盘，　sdb1则表示该磁盘上的分区</span></code></pre><h4 id="2-如何查询端口所运行的程序"><a href="#2-如何查询端口所运行的程序" class="headerlink" title="2. 如何查询端口所运行的程序"></a>2. 如何查询端口所运行的程序</h4><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo netstat -antp | grep 7000</span>tcp6       0      0 :::7000                 :::*                    LISTEN      12464/frps      <span class="hljs-meta">$</span><span class="bash"> ps 12464</span>  PID TTY      STAT   TIME COMMAND12464 pts/0    Sl     0:00 ./frps<span class="hljs-meta">$</span><span class="bash"> lsof -i:7000   <span class="hljs-comment"># lsof:list open files -&gt; -i </span></span>COMMAND   PID   USER   FD   TYPE  DEVICE SIZE/OFF NODE NAMEfrps    12464 ubuntu    3u  IPv6 7542760      0t0  TCP *:afs3-fileserver (LISTEN)<span class="hljs-meta">$</span><span class="bash"> ps -fe | grep 12464</span>ubuntu   12464  6446  0 00:29 pts/0    00:00:00 ./frpsubuntu   12707  6446  0 00:32 pts/0    00:00:00 grep --color=auto 12464</code></pre><h4 id="3-gdm3、lightdm-和-kdm-的认识"><a href="#3-gdm3、lightdm-和-kdm-的认识" class="headerlink" title="3. gdm3、lightdm 和 kdm 的认识"></a>3. gdm3、lightdm 和 kdm 的认识</h4><p>Q:  当我的 win10/mac 电脑想要控制 ubuntu 电脑时，一切操作无误的情况下，在点击 “远程协助” 按钮时，进去后过一会儿就会出现“连接已断开”的提示。<br>S： 在打开向日葵客户端的情况下，打开命令 ubuntu 行窗口</p><pre><code class="hljs shell">sudo apt-get updatesudo apt-get upgradesudo install lightdm</code></pre><p>重启Ubuntu系统即可远程成功</p><p>R： 了解一下gdm3、lightdm、kdm</p><p>维基百科：显示管理器向用户显示登录屏幕。 当用户成功输入用户名和密码的有效组合时，会话开始。</p><ul><li>gdm3 是 gdm的继承者，它是GNOME显示管理器。 更新的gdm3 使用了最小的gnome-shell 版本，并提供了与GNOME3会话相同的外观和感觉。</li><li>lightDM，即：Light Display Manager，是一个全新的、轻量的Linux桌面的桌面显示管理器</li><li>kdm 是 kde 管理器的显示。 但在 KDE5中，它被否决为 SDDM，它更适合作为显示管理器，因此在默认情况下，它是在屏幕。</li></ul><p>简单理解，这三个只是不同版本的显示管理器而已，当你的 ubuntu 系统安装了多个显示管理器时，(以 lightdm 切换到 gdm3 为例）可以用 <code>sudo dpkg-reconfigure gdm3</code> 来进行切换。</p><h4 id="4-xrandr-使用"><a href="#4-xrandr-使用" class="headerlink" title="4. xrandr 使用"></a>4. xrandr 使用</h4><p>装了一个侧屏， 需要使用 xrandr 来进行相关设置：</p><ul><li><p>xrandr : 列出可用的显示设备</p><pre><code class="hljs shell">zhaozhichao@zhaozhichao-MS-7B24:~/Desktop/sany/classification$ xrandrScreen 0: minimum 8 x 8, current 3000 x 1920, maximum 32767 x 32767DP-0 connected primary 1920x1080+1080+520 (normal left inverted right x axis y axis) 598mm x 336mm   1920x1080     60.00*+   1600x900      60.00     1280x1024     75.02    60.02     1152x864      75.00     1024x768      75.03    60.00     800x600       75.00    60.32     640x480       75.00    59.94  DP-1 disconnected (normal left inverted right x axis y axis)HDMI-0 disconnected (normal left inverted right x axis y axis)DP-2 disconnected (normal left inverted right x axis y axis)DP-3 disconnected (normal left inverted right x axis y axis)DP-4 connected 1080x1920+0+0 left (normal left inverted right x axis y axis) 598mm x 336mm   1920x1080     60.00*+   1600x900      60.00     1280x1024     75.02    60.02     1152x864      75.00     1024x768      75.03    60.00     800x600       75.00    60.32     640x480       75.00    59.94  DP-5 disconnected (normal left inverted right x axis y axis)USB-C-0 disconnected (normal left inverted right x axis y axis)</code></pre></li><li><p>设置分辨率</p><pre><code class="hljs shell">xrandr --output eDP1 --mode 1280x1024_60.00</code></pre></li><li><p>双屏设置</p><pre><code class="hljs shell">xrandr --output DP-4 --left-of  DP-0 --auto // DP-4 作为 DP-0 的左屏幕显示                                              //  --left-of                                                // --right-of</code></pre></li><li><p>屏幕克隆</p><pre><code class="hljs shell">xrandr --output VGA-0 --same-as DVI-D-0 --auto</code></pre></li><li><p>设置 左旋转/右旋转/正向</p><pre><code class="hljs shell">xrandr --output DP-0 --rotate normal // left、right、normal</code></pre></li><li><p>设置主屏幕</p><pre><code class="hljs shell">xrandr --output HDMI2 --auto --primary</code></pre></li></ul><h4 id="5-U盘-硬盘-mount-故障"><a href="#5-U盘-硬盘-mount-故障" class="headerlink" title="5. U盘/硬盘 mount 故障"></a>5. U盘/硬盘 mount 故障</h4><pre><code class="hljs applescript">$ sudo mount /dev/sdb1 /mnt$MFTMirr <span class="hljs-keyword">does</span> <span class="hljs-keyword">not</span> match $MFT (<span class="hljs-built_in">record</span> <span class="hljs-number">0</span>).Failed <span class="hljs-keyword">to</span> mount &#x27;/dev/sdb1&#x27;: Input/output <span class="hljs-keyword">error</span>NTFS <span class="hljs-keyword">is</span> either inconsistent, <span class="hljs-keyword">or</span> there <span class="hljs-keyword">is</span> a hardware fault, <span class="hljs-keyword">or</span> <span class="hljs-keyword">it</span>&#x27;s aSoftRAID/FakeRAID hardware. In <span class="hljs-keyword">the</span> <span class="hljs-keyword">first</span> case <span class="hljs-built_in">run</span> chkdsk /f <span class="hljs-keyword">on</span> Windows<span class="hljs-keyword">then</span> reboot <span class="hljs-keyword">into</span> Windows twice. The usage <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> /f parameter <span class="hljs-keyword">is</span> veryimportant! If <span class="hljs-keyword">the</span> device <span class="hljs-keyword">is</span> a SoftRAID/FakeRAID <span class="hljs-keyword">then</span> <span class="hljs-keyword">first</span> <span class="hljs-built_in">activate</span><span class="hljs-keyword">it</span> <span class="hljs-keyword">and</span> mount a different device under <span class="hljs-keyword">the</span> /dev/mapper/ directory, (e.g./dev/mapper/nvidia_eahaabcc1). Please see <span class="hljs-keyword">the</span> &#x27;dmraid&#x27; documentation<span class="hljs-keyword">for</span> more details.</code></pre><p>S： 利用 ntfsprogs utility 包里的工具 ntfsfix 修理一下，感觉应该是把类似链接号什么的修理好：</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo ntfsfix /dev/sdb1</span>Mounting volume... FAILEDAttempting to correct errors...Processing $MFT and $MFTMirr...Reading $MFT... OKReading $MFTMirr... OKComparing $MFTMirr to $MFT... FAILEDCorrecting differences in $MFTMirr record 0...OKProcessing of $MFT and $MFTMirr completed successfully.Setting required flags on partition... OKGoing to empty the journal ($LogFile)... OKNTFS volume version is 3.1.NTFS partition /dev/sdb1 was processed successfully.</code></pre><h4 id="6-区分-profile-和-bashrc"><a href="#6-区分-profile-和-bashrc" class="headerlink" title="6. 区分 profile 和 bashrc"></a>6. 区分 profile 和 bashrc</h4><div class="table-container"><table><thead><tr><th></th><th>生效时间</th><th>针对用户</th></tr></thead><tbody><tr><td>/etc/profile</td><td>重启生效</td><td>所有用户</td></tr><tr><td>/etc/bashrc</td><td>重新打开一个 bash 生效</td><td>所有用户</td></tr><tr><td>~/.bash_profile或 ~/.profile</td><td>重启生效</td><td>当前用户</td></tr><tr><td>~/.bashrc</td><td>重新打开一个 bash 生效</td><td>当前用户</td></tr></tbody></table></div><h4 id="7-etc-ld-so-conf-与ldconfig"><a href="#7-etc-ld-so-conf-与ldconfig" class="headerlink" title="7. /etc/ld.so.conf 与ldconfig"></a>7. /etc/ld.so.conf 与ldconfig</h4><p><strong>ldconfig命令</strong> 的用途主要是在<strong>默认搜寻目录/lib和/usr/lib以及动态库配置文件/etc/ld.so.conf内所列的目录下，搜索出可共享的动态链接库（格式如lib*.so*）,进而创建出动态装入程序(ld.so)所需的连接和缓存文件</strong><br>往 <code>/lib</code> 和 <code>/usr/lib</code> 里面加东西，是不用修改 <code>/etc/ld.so.conf</code> 的，但是完了之后要调一下 <code>ldconfig</code>，不然这个 library 会找不到。<br>想往上面两个目录以外加东西的时候，一定要修改 <code>/etc/ld.so.conf</code>，然后再调用 <code>ldconfig</code>，不然也会找不到。比较常规的操作是，自己生成了一个动态链接库，然后将路径添加到 <code>/etc/ld.so.conf</code>， 然后执行 <code>ldconfig</code>。 这样就可以在系统中调用该动态链接库了。</p><h4 id="8-NVIDIA-相关软件的知识"><a href="#8-NVIDIA-相关软件的知识" class="headerlink" title="8. NVIDIA 相关软件的知识"></a>8. NVIDIA 相关软件的知识</h4><p><strong>GPU</strong>:  硬件， 主流的是 Nvidia 的 GPU(现在流行的是 RTX 2080Ti)， 深度学习本身需要大量计算。 GPU 的并行计算能力， 在过去几年里恰当了满足了深度学习的需求。 AMD 的 GPU 基本没有什么支持， 可以不用考虑。<br><strong>NVIDIA Driver</strong>: 硬件接口， 没有显卡驱动， 就不能识别 GPU 硬件， 不能调用其计算资源。<br><strong>CUDA</strong>: 是 NVIDIA 推出的只能用于自家GPU的并行计算框架。只有安装这个框架才能够进行复杂的并行计算。主流的深度学习框架也都是基于CUDA进行GPU并行加速的，几乎无一例外。<br><strong>cudnn</strong>: 针对深度卷积神经网络的加速库。<br><strong>Tensorflow、pytorch、mxnet、paddle</strong>: 在 CUDA 和 cudnn 之上的深度学习框架。</p><h4 id="9-显卡安装的知识"><a href="#9-显卡安装的知识" class="headerlink" title="9. 显卡安装的知识"></a>9. 显卡安装的知识</h4><p>(1)  <strong>硬件</strong>：插上 GPU<br>(2)  <strong>驱动</strong>:  [去官网下载对应的驱动程序]</p><p>a. 首先屏蔽 nouveau</p><pre><code class="hljs shell">sudo vim /etc/modprobe.d/blacklist-nouveau.conf</code></pre><p>在其中加入</p><pre><code class="hljs shell">blacklist nouveau</code></pre><p>b. 按照 ctrl + alt + F1 进入 tty1, 然后使用如下命令关掉 X server</p><pre><code class="hljs awk">sudo <span class="hljs-regexp">/etc/i</span>nit.d/lightdm stop</code></pre><p>c. 安装对应的驱动程序<br>d. 重启 X server</p><pre><code class="hljs shell">sudo /etc/init.d/light restart</code></pre><p>（3） <strong>CUDA &amp; cudnn</strong></p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> Download CUDA   https://developer.nvidia.com/cuda-downloads</span><span class="hljs-meta">$</span><span class="bash"> chmod a+x cuda-repo-ubuntu1804-10-1-local-10.1.168-418.67_1.0-1_amd64.deb </span><span class="hljs-meta">$</span><span class="bash"> sudo dpkg -i ./cuda-repo-ubuntu1804-10-1-local-10.1.168-418.67_1.0-1_amd64.deb </span><span class="hljs-meta">$</span><span class="bash"> sudo apt-key add /var/cuda-repo-10-1-local-10.1.168-418.67/7fa2af80.pub</span><span class="hljs-meta">$</span><span class="bash"> sudo apt-get update</span><span class="hljs-meta">$</span><span class="bash"> sudo apt-get install cuda-10-1 -y</span><span class="hljs-meta">#</span><span class="bash"> Download cudnn from https://developer.nvidia.com/cudnn</span><span class="hljs-meta">$</span><span class="bash"> sudo cp cuda/include/cudnn.h /usr/<span class="hljs-built_in">local</span>/cuda/include</span><span class="hljs-meta">$</span><span class="bash"> sudo cp cuda/lib64/libcudnn* /usr/<span class="hljs-built_in">local</span>/cuda/lib64</span><span class="hljs-meta">$</span><span class="bash"> sudo chmod a+r /usr/<span class="hljs-built_in">local</span>/cuda/include/cudnn.h /usr/<span class="hljs-built_in">local</span>/cuda/lib64/libcudnn*</span></code></pre><p>别忘记 执行以下 sudo ldconfig</p><h4 id="10-ubuntu-循环登录问题"><a href="#10-ubuntu-循环登录问题" class="headerlink" title="10. ubuntu 循环登录问题"></a>10. ubuntu 循环登录问题</h4><p>这个问题是一个很常见的问题， 我这次碰到的情况是新增加的账户不能正常登陆, 这里是由于useradd的时候没有关联到对应的/home下的文件夹所致, 处理如下:</p><p>（1）使用: <code>userdel -r 用户名</code> 删除用户</p><p>（2）使用  <code>useradd -m 用户名</code> 添加用户 -&gt;  会在/home目录下创建同名文件夹</p><p>（3）添加用户到sudoer</p><ul><li><code>sudo visudo</code></li><li>在<code>%sudo</code>行下面<code>user ALL=(ALL) ALL</code></li></ul><h4 id="11-linux-productivity-tools"><a href="#11-linux-productivity-tools" class="headerlink" title="11. linux productivity tools"></a>11. linux productivity tools</h4><p>朋友分享了一份 linux productivity tools， 用空去学习学习</p><p>参考网址为：<a href="https://news.ycombinator.com/item?id=23229241">https://news.ycombinator.com/item?id=23229241</a></p><h4 id="12-linux-开机自启动"><a href="#12-linux-开机自启动" class="headerlink" title="12. linux 开机自启动"></a>12. linux 开机自启动</h4><p><strong>方式一. 使用自带开机脚本</strong></p><p>使用 <code>/etc/rc.local</code> 文件，在 ubuntu18.04 中可以自己新建这个文件。</p><pre><code class="hljs awk">vim <span class="hljs-regexp">/etc/</span>rc.local</code></pre><p>文件的具体内容如下所示，将开机自启动命令加在 exit 0 之前。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">!/bin/sh -e</span><span class="hljs-meta">#</span><span class="hljs-meta">#</span><span class="bash"> rc.local</span><span class="hljs-meta">#</span><span class="hljs-meta">#</span><span class="bash"> This script is executed at the end of each multiuser runlevel.</span><span class="hljs-meta">#</span><span class="bash"> Make sure that the script will <span class="hljs-string">&quot;exit 0&quot;</span> on success or any other</span><span class="hljs-meta">#</span><span class="bash"> value on error.</span><span class="hljs-meta">#</span><span class="hljs-meta">#</span><span class="bash"> In order to <span class="hljs-built_in">enable</span> or <span class="hljs-built_in">disable</span> this script just change the execution</span><span class="hljs-meta">#</span><span class="bash"> bits.</span><span class="hljs-meta">#</span><span class="hljs-meta">#</span><span class="bash"> By default this script does nothing.</span>exit 0</code></pre><p>修改执行权限即可</p><pre><code class="hljs shell">chmod +x /etc/rc.local</code></pre><p><strong>方式二. 添加开机脚本</strong></p><p>(1) 在　<code>/etc/init.d/</code> 下新建一个文件 auto_start.sh，　内容如下所示，讲待执行命令填写在 exit 0 之前。　</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">!/bin/bash  </span><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">command</span> content      </span>exit 0</code></pre><p>(2) 更改脚本权限:</p><pre><code class="hljs shell">sudo chmod +x auto_start.sh</code></pre><p>(3) 将脚本添加到启动脚本, 执行如下指令即可，在这里90表明一个优先级，越高表示执行的越晚。</p><pre><code class="hljs shell">cd /etc/init.d/  sudo update-rc.d auto_start.sh defaults 90</code></pre><p>Note:　可以使用如下命令移除开机脚本：</p><pre><code class="hljs shell">sudo update-rc.d -f new_service.sh remove</code></pre><h4 id="13-NVIDIA-常见命令"><a href="#13-NVIDIA-常见命令" class="headerlink" title="13. NVIDIA 常见命令"></a>13. NVIDIA 常见命令</h4><h6 id="nvidia-smi"><a href="#nvidia-smi" class="headerlink" title="nvidia-smi"></a>nvidia-smi</h6><p>在进行深度学习实验时，GPU 的实时状态监测十分有必要。今天详细解读一下 <code>nvidia-smi</code> 命令。</p><pre><code class="hljs shell">Fri Aug  2 10:10:08 2019       +-----------------------------------------------------------------------------+| NVIDIA-SMI 410.48                 Driver Version: 410.48                    ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  GeForce RTX 208...  Off  | 00000000:01:00.0  On |                  N/A || 31%   43C    P8    12W / 250W |    223MiB / 10981MiB |      4%      Default |+-------------------------------+----------------------+----------------------+                                                                               +-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID   Type   Process name                             Usage      ||=============================================================================||    0      1614      G   /usr/lib/xorg/Xorg                            14MiB ||    0      4804      G   /usr/lib/xorg/Xorg                           207MiB |+-----------------------------------------------------------------------------+</code></pre><p>上图是服务器上 GeForce GTX 1080 Ti 的信息，下面一一解读参数。 上面的表格中的信息与下面的四个框的信息是一一对应的：</p><p><strong>GPU</strong>：GPU 编号；            <strong>Name</strong>：GPU 型号；  <strong>Persistence-M</strong>：持续模式的状态。<br><strong>Fan</strong>：风扇转速(0~100%)  <strong>Temp</strong>：温度(摄氏度)  <strong>Perf</strong>：性能状态，从P0(小)到P12(大)　<strong>Pwr:Usage/Cap</strong>：能耗</p><p><strong>Bus-Id</strong>：GPU总线　　　　<strong>Disp.A</strong>：Display Active，表示GPU的显示是否初始化；<br><strong>Memory Usage</strong>：显存使用率； </p><p><strong>Volatile GPU-Util</strong>：浮动的GPU利用率　　　<strong>Uncorr. ECC</strong>：Error Correcting Code，错误检查与纠正<br><strong>Compute M</strong>：compute mode，计算模式。</p><p><strong>下方的 Processes 表示每个进程对 GPU 的显存使用率</strong></p><h6 id="nvidia-smi-L"><a href="#nvidia-smi-L" class="headerlink" title="nvidia-smi -L"></a>nvidia-smi -L</h6><p>第二个命令：<code>nvidia-smi -L</code>, 该命令用于列出所有可用的 NVIDIA 设备信息。</p><h6 id="watch-n1-nvidia-smi"><a href="#watch-n1-nvidia-smi" class="headerlink" title="watch -n1 nvidia-smi"></a>watch -n1 nvidia-smi</h6><p>每1秒检查一次GPU的使用情况</p><h6 id="查看CUDA-和-cudnn-版本信息"><a href="#查看CUDA-和-cudnn-版本信息" class="headerlink" title="查看CUDA　和 cudnn　版本信息"></a>查看CUDA　和 cudnn　版本信息</h6><p>查看 CUDA 版本：</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> cat /usr/<span class="hljs-built_in">local</span>/cuda/version.txt</span>CUDA Version 10.0.130</code></pre><p>也可以使用如下命令:</p><pre><code class="hljs shell">nvcc --version</code></pre><p>查看 CUDNN 版本：</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> cat /usr/<span class="hljs-built_in">local</span>/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span>rep CUDNN_MAJOR -A 2<span class="hljs-meta">#</span><span class="bash">define CUDNN_MAJOR 7</span><span class="hljs-meta">#</span><span class="bash">define CUDNN_MINOR 5</span><span class="hljs-meta">#</span><span class="bash">define CUDNN_PATCHLEVEL 0</span>--<span class="hljs-meta">#</span><span class="bash">define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)</span><span class="hljs-meta">#</span><span class="bash">include <span class="hljs-string">&quot;driver_types.h&quot;</span></span></code></pre>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ubuntu常见的软件安装方式</title>
    <link href="/2018/06/17/ubuntu%E5%B8%B8%E8%A7%81%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F/"/>
    <url>/2018/06/17/ubuntu%E5%B8%B8%E8%A7%81%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<p>ubuntu 一些安装命令的区分， 比如 apt-get、apt、snap、dpkg 等。</p><a id="more"></a><h4 id="1-apt-vs-apt-get"><a href="#1-apt-vs-apt-get" class="headerlink" title="1. apt vs apt-get"></a>1. apt vs apt-get</h4><p>　　最常用的 Linux 包管理命令分散在 <code>apt-get</code>、<code>apt-cache</code> 和<code>apt-config</code> 这三条命令当中。<code>apt</code> 命令的引入就是<strong>为了解决命令过于分散的问题</strong>，它包括了 apt-get 命令出现以来使用最广泛的功能选项，以及 apt-cache 和 apt-config 命令中很少用到的功能。在使用 apt 命令时，用户不必再由 apt-get 转到 apt-cache 或 apt-config，而且 apt 更加结构化，并为用户提供了管理软件包所需的必要选项。</p><p><strong>简单来说就是：apt = apt-get、apt-cache 和 apt-config 中最常用命令选项的集合。</strong></p><h6 id="apt与apt-get之间的区别"><a href="#apt与apt-get之间的区别" class="headerlink" title="apt与apt-get之间的区别"></a>apt与apt-get之间的区别</h6><p>　　通过 apt 命令，用户可以在同一地方集中得到所有必要的工具，apt 的主要目的是提供一种以「让终端用户满意」的方式来处理 Linux 软件包的有效方式。</p><p>　　apt 具有更精减但足够的命令选项，而且参数选项的组织方式更为有效。除此之外，它默认启用的几个特性对最终用户也非常有帮助。例如，可以在使用 apt 命令安装或删除程序时看到进度条，apt 还会在更新存储库数据库时提示用户可升级的软件包个数。</p><p>　　如果你使用 apt 的其它命令选项，也可以实现与使用 apt-get 时相同的操作。</p><h6 id="apt和apt-get命令之间的区别"><a href="#apt和apt-get命令之间的区别" class="headerlink" title="apt和apt-get命令之间的区别"></a>apt和apt-get命令之间的区别</h6><p>​      虽然 apt 与 apt-get 有一些类似的命令选项，但它并不能完全向下兼容 apt-get 命令。也就是说，可以用 apt 替换部分 apt-get 系列命令，但不是全部。</p><div class="table-container"><table><thead><tr><th style="text-align:left">apt 命令</th><th style="text-align:left">取代的命令</th><th style="text-align:left">命令的功能</th></tr></thead><tbody><tr><td style="text-align:left">apt install</td><td style="text-align:left">apt-get install</td><td style="text-align:left">安装软件包</td></tr><tr><td style="text-align:left">apt remove</td><td style="text-align:left">apt-get remove</td><td style="text-align:left">移除软件包</td></tr><tr><td style="text-align:left">apt purge</td><td style="text-align:left">apt-get purge</td><td style="text-align:left">移除软件包及配置文件</td></tr><tr><td style="text-align:left">apt update</td><td style="text-align:left">apt-get update</td><td style="text-align:left">刷新存储库索引</td></tr><tr><td style="text-align:left">apt upgrade</td><td style="text-align:left">apt-get upgrade</td><td style="text-align:left">升级所有可升级的软件包</td></tr><tr><td style="text-align:left">apt autoremove</td><td style="text-align:left">apt-get autoremove</td><td style="text-align:left">自动删除不需要的包</td></tr><tr><td style="text-align:left">apt full-upgrade</td><td style="text-align:left">apt-get dist-upgrade</td><td style="text-align:left">在升级软件包时自动处理依赖关系</td></tr><tr><td style="text-align:left">apt search</td><td style="text-align:left">apt-cache search</td><td style="text-align:left">搜索应用程序</td></tr><tr><td style="text-align:left">apt show</td><td style="text-align:left">apt-cache show</td><td style="text-align:left">显示安装细节</td></tr></tbody></table></div><p>当然，apt 还有一些自己的命令：</p><div class="table-container"><table><thead><tr><th style="text-align:left">新的apt命令</th><th style="text-align:left">命令的功能</th></tr></thead><tbody><tr><td style="text-align:left">apt list</td><td style="text-align:left">列出包含条件的包（已安装，可升级等）</td></tr><tr><td style="text-align:left">apt edit-sources</td><td style="text-align:left">编辑源列表</td></tr></tbody></table></div><p>需要大家注意的是：apt 命令也还在不断发展， 因此，你可能会在将来的版本中看到新的选项。</p><h6 id="apt-get已弃用？"><a href="#apt-get已弃用？" class="headerlink" title="apt-get已弃用？"></a>apt-get已弃用？</h6><p>​      目前还没有任何 Linux 发行版官方放出 apt-get 将被停用的消息，至少它还有比 apt 更多、更细化的操作功能。对于低级操作，仍然需要 apt-get。</p><h6 id="我应该使用apt还是apt-get？"><a href="#我应该使用apt还是apt-get？" class="headerlink" title="我应该使用apt还是apt-get？"></a>我应该使用apt还是apt-get？</h6><p>​      既然两个命令都有用，那么我该使用 apt 还是 apt-get 呢？作为一个常规 Linux 用户，建议大家尽快适应并开始首先使用 apt。不仅因为广大 Linux 发行商都在推荐 apt，更主要的还是它提供了 Linux 包管理的必要选项。</p><p>​     最重要的是，apt 命令选项更少更易记，因此也更易用，所以没理由继续坚持 apt-get。</p><h6 id="dpkg"><a href="#dpkg" class="headerlink" title="dpkg"></a>dpkg</h6><p>dpkg是用来安装.deb文件,但不会解决模块的依赖关系,且不会关心ubuntu的软件仓库内的软件,可以用于安装本地的deb文件。</p><h6 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h6><p>最后结大家提供两点使用上的建议：</p><ul><li>apt 可以看作 apt-get 和 apt-cache 命令的子集, 可以为包管理提供必要的命令选项。</li><li>apt-get 虽然没被弃用，但作为普通用户，还是应该首先使用 apt。</li></ul><p>以上原文链接：<a href="https://www.sysgeek.cn/apt-vs-apt-get/">https://www.sysgeek.cn/apt-vs-apt-get/</a></p><h4 id="2-apt-基本命令"><a href="#2-apt-基本命令" class="headerlink" title="2. apt 基本命令"></a>2. apt 基本命令</h4><pre><code class="hljs shell">sudo apt install [xxx.deb] # 安装sudo apt --fix-broken install　　#　修复依赖<span class="hljs-meta">#</span><span class="bash"> apt update vs upgrade</span>sudo apt update　　# 只检查，不更新（已安装的软件包是否有可用的更新，给出汇总报告）sudo apt upgrade #  更新已安装的软件包sudo apt remove     # 删除已安装的软件包（保留配置文件）sudo apt purge      # 删除已安装包（不保留配置文件)，删除软件包，同时删除相应依赖软件包。                         # 如果想要彻底清理软件，　建议使用　purge 命令sudo apt clean    # 此命令会将 /var/cache/apt/archives/ 下的 所有 deb 删掉，                 # 相当于清理下载的软件安装包。sudo apt autoremove   # 删除已安装的软件包（保留配置文件），不会删除依赖软件包，且保留配置文件。（这个命令容易导致系统无法进入系统桌面, 在桌面版的Ubuntu系统下尽量不要使用</code></pre><h4 id="3-常见的软件编译安装方法"><a href="#3-常见的软件编译安装方法" class="headerlink" title="3. 常见的软件编译安装方法"></a>3. 常见的软件编译安装方法</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> download</span><span class="hljs-meta">$</span><span class="bash"> git <span class="hljs-built_in">clone</span> [github/path/to/packge]</span><span class="hljs-meta">#</span><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> [package]</span><span class="hljs-meta">$</span><span class="bash"> mkdir build</span><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">cd</span> build</span><span class="hljs-meta">#</span><span class="bash"> build &amp; install</span><span class="hljs-meta">$</span><span class="bash"> cmake ..</span><span class="hljs-meta">$</span><span class="bash"> make </span><span class="hljs-meta">$</span><span class="bash"> sudo make install</span></code></pre><h4 id="４-snap"><a href="#４-snap" class="headerlink" title="４. snap"></a>４. snap</h4><p>　　Snap是Ubuntu母公司Canonical于2016年4月发布　Ubuntu16.04　时候引入的一种安全的、易于管理的、沙盒化的软件包格式，与传统的dpkg/apt有着很大的区别。Snap可以让开发者将他们的软件更新包随时发布给用户，而不必等待发行版的更新周期；　其次Snap应用可以同时安装多个版本的软件，比如安装Python2.7和Python3.3。　snap软件包一般安装在/snap目录下。</p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> sudo snap install [snap_name]　<span class="hljs-comment">#　安装snap包</span></span><span class="hljs-meta">$</span><span class="bash"> sudo snap find [snap_name] <span class="hljs-comment">#　搜索安装的snap包</span></span><span class="hljs-meta">$</span><span class="bash"> sudo snap refresh [snap_name] <span class="hljs-comment">#　更新snap包</span></span><span class="hljs-meta">$</span><span class="bash"> sudo snap remove [snap_name] <span class="hljs-comment">#　删除 snap 包</span></span><span class="hljs-meta">$</span><span class="bash"> snap list <span class="hljs-comment">#　列出已经安装的snap包</span></span></code></pre><h4 id="5-Python-packgae-安装方式"><a href="#5-Python-packgae-安装方式" class="headerlink" title="5. Python packgae 安装方式"></a>5. Python packgae 安装方式</h4><h6 id="pip-pip3"><a href="#pip-pip3" class="headerlink" title="pip/pip3"></a>pip/pip3</h6><p>比较常见的安装python package 的方法    </p><h6 id="conda"><a href="#conda" class="headerlink" title="conda"></a>conda</h6><p>​    conda的包管理比较好理解了，这部分功能与pip类似。例如，如果需要安装scipy， 可以使用：<code>conda install scipy</code>。conda会从从远程搜索scipy的相关信息和依赖项目。例如，scipy 依赖于numpy，一次如果你只安装 scipy，则conda还会安装 numpy。你还可以同时安装多个包。类似 <code>conda install numpy scipy pandas</code> 的命令会同时安装所有这些包。另外还可以通过添加版本号（例如 <code>conda install numpy=1.10</code>）来指定所需的包版本。</p><p>​    Conda的大部分命令都很直观，要卸载安装包，可以使用 <code>conda remove package_name</code>。要更新安装包，可以使用 <code>conda update package_name</code>, 要更新环境中所有包，可以使用 <code>conda update -all</code>。要查看已经安装的包，可以使用 <code>conda list</code>, 最新版的conda是从site-packages文件夹中搜索已经安装的包，不依赖于pip，因此可以显示出通过各种方式安装的包。 最后，如果我们不知道要找的包的确切名称，可以尝试使用<code>conda search search_term</code> 尝试进行搜索。例如，要安装 Beatiful Soup，但是我们不清楚确切的包名称，因此可以尝试<code>conda search beatifulsoup</code>。</p><h6 id="apt-get-install-Python-package"><a href="#apt-get-install-Python-package" class="headerlink" title="apt-get install Python package"></a>apt-get install Python package</h6><p>(1) 安装python2 package</p><pre><code class="hljs shell">sudo apt-get install python-[package]</code></pre><p>(2) 安装python3 package</p><pre><code class="hljs shell">sudo apt-get install python3-[package]</code></pre><h4 id="6-apt-get-和-yum-区别"><a href="#6-apt-get-和-yum-区别" class="headerlink" title="6. apt-get 和 yum 区别"></a>6. apt-get 和 yum 区别</h4><p>一般来说著名的linux系统基本上分两大类：</p><ol><li>RedHat系列：Redhat、CentOS、Fedora等</li><li>Debian系列：Debian、Ubuntu等</li></ol><h6 id="RedHat-系列"><a href="#RedHat-系列" class="headerlink" title="RedHat 系列"></a>RedHat 系列</h6><ol><li>常见的安装包格式 rpm包,安装rpm包的命令是“rpm -参数” </li><li>包管理工具 yum </li><li>支持tar包</li></ol><h6 id="Debian系列"><a href="#Debian系列" class="headerlink" title="Debian系列"></a>Debian系列</h6><ol><li>常见的安装包格式 deb包,安装deb包的命令是“dpkg -参数” </li><li>包管理工具 apt-get </li><li>支持tar包</li></ol>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Linux_commands</title>
    <link href="/2018/06/05/Linux-commands/"/>
    <url>/2018/06/05/Linux-commands/</url>
    
    <content type="html"><![CDATA[<p>Linux 常见命令：基本文件和目录操作、文件的创建和查阅、搜索命令、磁盘的压缩和解压缩、系统信息、进程管理、用户权限与用户管理、网络传输、安装卸载、开关机、窗口管理等。</p><a id="more"></a><h4 id="1-基本文件与目录操作"><a href="#1-基本文件与目录操作" class="headerlink" title="1. 基本文件与目录操作"></a>1. 基本文件与目录操作</h4><pre><code class="hljs shell">ls    # 列出文件   [-a] 列出全部文件(包括隐藏文件)      [-l]  列出长数据串，包含属性与权限等信息      # 查看当前文件下的目录个数  find ./ | wc -lcd dir  # 切换到指定目录 cd ..   # 切换到上一级目录          cd ~    # 切换到自己的主文件夹       cd -    # 切换到刚才目录pwd    # 显示当前目录mkdir [dirname]    # 创建新文件夹      rm [file]  #  删除文件或者目录     [-r]  删除目录    [-f] 强制cp [srcfile] [dstfile]   # 复制文件         [-r] 用于复制目录  mv [srcfile] [dstfile]   # 将文件移动到指定目录，不使用 [-r] 即可移动目录，同一目录则为重命名tree . # 显示目录树     -L   指定显示最大层数  sudo apt-get install tree</code></pre><h4 id="2-文件创建与查阅"><a href="#2-文件创建与查阅" class="headerlink" title="2. 文件创建与查阅"></a>2. 文件创建与查阅</h4><pre><code class="hljs shell">touch filename #  创建新文件   cat file    # 正向查看文件全部内容     tac file    # 反向查看文件全部内容    [-n] 带行号显示文件head -n file  # 查看文件前n行      tail -n file  # 查看file的后n行less file   # 按页查看文件[Page Down]    下翻一页  [Page Up]  上翻一页    /  向下查询   ?  向上查询   q 退出more file  # 按页查看文件 [Enter]   向下一行    [Space]向下一页    q退出 /查询    :f   显示文件名和当前行数</code></pre><h4 id="3-搜索命令"><a href="#3-搜索命令" class="headerlink" title="3. 搜索命令"></a>3. 搜索命令</h4><pre><code class="hljs shell">locate [filename]    # 系统全局范围内定位文件,  从/var.lib/mlocate数据库中查找文件，                     # 但是该数据库每天更新一次，所以可以首先使用sudo updatedb更新一下数据库，                     # 才能查询最近的文件        whereis [commands]    搜索命令位置(命令+帮助文档)   which  搜索命令位置(命令+别名)find  [search_file]  [search_condition]      搜索文件，应该避免大范围搜索<span class="hljs-meta">#</span><span class="bash"> 结合通配符使用： * 任意内容 ? 任意一个字符 []    任意一个[]内的内容 </span>grep [search_string]  [search_file]     -v 反向搜索  -i 忽略大小写</code></pre><h4 id="4-磁盘与压缩-解压缩命令-F"><a href="#4-磁盘与压缩-解压缩命令-F" class="headerlink" title="4. 磁盘与压缩/解压缩命令 [F]"></a>4. 磁盘与压缩/解压缩命令 [F]</h4><h5 id="1-查看磁盘空间"><a href="#1-查看磁盘空间" class="headerlink" title="(1) 查看磁盘空间"></a>(1) 查看磁盘空间</h5><pre><code class="hljs shell">du -h --max-depth=1 #　显示当前目录磁盘占用, --max-depth=1 显示级别df -h  # 检查linux服务器的文件系统的磁盘空间占用情况<span class="hljs-meta">#</span><span class="bash"> -h 以人们较易读的容量格式 (G/M) 显示</span></code></pre><h5 id="2-压缩与解压缩"><a href="#2-压缩与解压缩" class="headerlink" title="(2) 压缩与解压缩"></a>(2) 压缩与解压缩</h5><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> .zip</span>zip  压缩文件名.zip  源文件   # 压缩 zip 文件  [-r] 压缩目录unzip 压缩文件.zip   # 解压<span class="hljs-meta">#</span><span class="bash"> .tar.gz</span>tar -zcvf file.tar.gz 源目录  压缩  [-c] 打包  [-v] 显示过程 [-f] 指定打包后的文件名tar -zxvf file.tar.gz    解压<span class="hljs-meta">#</span><span class="bash"> .tar.bz2</span>tar -jcvf file.tar.gz 源目录   压缩  [-c] 打包  [-v] 显示过程 [-f] 指定打包后的文件名tar -jxcf file.tar.gz   解压</code></pre><h4 id="5-系统信息"><a href="#5-系统信息" class="headerlink" title="5. 系统信息"></a>5. 系统信息</h4><h5 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h5><pre><code class="hljs shell">date   #  显示当前时间和日期    cal   # 显示当月日历       uptime  # 显示系统开机时间</code></pre><h5 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h5><pre><code class="hljs shell">lsb_release [-a]    # 查看linux 版本   uname   [-a]  # 查看内核版本  last # 查看登录信息whoami   # 查看当前用户名lsof  # 列出进程调用或者打开的文件信息vmstat [刷新延时  刷新次数]  # 查看系统资源 w # 查看用户登录信息</code></pre><h5 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h5><pre><code class="hljs shell">cat /proc/cpuinfo     #  查看CPU 信息  cat /proc/meminfo     # 查看内存信息df   # 显示磁盘占用情况    [-h] 按照人们常见的 KB,MB,GB 格式显示du   # 显示目录空间占用情况    [-h] 按照人们常见的 KB,MB,GB 格式显示   [--max-depth]  指定显示目录深度nvidia-smi  #  显示nvidia 显卡的运行情况</code></pre><h4 id="6-进程管理"><a href="#6-进程管理" class="headerlink" title="6. 进程管理"></a>6. 进程管理</h4><pre><code class="hljs shell">ps    # 显示当前活动进程   [aux]  以BSD系统格式显示  [-ef] 以linux标准格式显示top    # 显示正在运行的进程  kill pid   # 杀死进程id  [-9]  强制杀死killall [进程名]  # 按照进程名杀死进程    pkill [进程名]  # 按照进程名杀死进程kill %[job num]  # 按照工作号杀死进程bg     # 列出已停止或者后台的进程       fg     # 将最近的作业带到前台   fg  n  # 将作业n带到前台 [command] &amp;  #  把命令放入后台，并在后台执行  jobs  # 显示后台进程[command];[Enter];[ctrl] + z  # 把命令放在后台，并暂停</code></pre><h4 id="7-文件权限与用户管理-F"><a href="#7-文件权限与用户管理-F" class="headerlink" title="7. 文件权限与用户管理  [F]"></a>7. 文件权限与用户管理  [F]</h4><h5 id="1-更改文件权限-："><a href="#1-更改文件权限-：" class="headerlink" title="(1) 更改文件权限 ："></a>(1) 更改文件权限 ：</h5><pre><code class="hljs shell">chmod [u|g|o][+|-|=][r|w|x]   file|dir    chmod octal file|dir<span class="hljs-meta">#</span><span class="bash"> u 用户    g  所属群组   o 其他      r=<span class="hljs-built_in">read</span>(4)     w=write(2)      x=execute(1)</span></code></pre><h5 id="2-用户和用户组管理"><a href="#2-用户和用户组管理" class="headerlink" title="(2) 用户和用户组管理"></a>(2) 用户和用户组管理</h5><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> user</span>useradd -m username   # 新建用户   [-m] 建立用户家目录passwd username       # 为新建用户设置密码su [username]         # 切换用户， 缺省则为 root 用户userdel -r username   # 删除用户  [-r] 删除用户家目录more /etc/passwd  # 查看所用用户及其权限</code></pre><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> groups</span>groups                           # 查看用户所属于的组usermod -G groupNmame username   # 将用户加入到组usermod -g groupName username    # 变更用户所属的根组more /etc/group  # 查看所有用户组及其权限</code></pre><pre><code class="hljs shell">chown username dirOrFile  # 更改文件的拥有者   -R 文件夹</code></pre><h5 id="3-环境变量"><a href="#3-环境变量" class="headerlink" title="(3) 环境变量"></a>(3) 环境变量</h5><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> bashrc与profile都用于保存用户的环境信息</span><span class="hljs-meta">#</span><span class="bash"> profile只能在登入的时候执行一次；</span><span class="hljs-meta">#</span><span class="bash"> bashrc 在每次执行 <span class="hljs-built_in">source</span> 时都会使用它一次</span>source ~/.bashrc</code></pre><h4 id="8-网络传输-F"><a href="#8-网络传输-F" class="headerlink" title="8. 网络传输 [F]"></a>8. 网络传输 [F]</h4><pre><code class="hljs shell">netstat         # 查看网络状态 -t TCP 端口  -u UDP 协议端口   -l 在监听状态  -a 所有端口lsof -i:[Port]  # 列出端口占用情况ipconfig        # 查看与配置网络状态route -n        # 查看路由状态traceroute IP    # 探测前往IP的路由路径ssh [-P port] user@hostname    # 以user用户身份连接到 hostname, 端口为 portwget file  # 下载file          -c 表示断点续传  -o 指定日志文件scp  -r  src/path   dst/path     #  下载和上传文件或目录   -P 端口host domain # domain -&gt; IPhost IP # IP -&gt; domainping host    #   探测指定IP或者域名的网络状况   -c 指定次数</code></pre><h4 id="9-安装卸载"><a href="#9-安装卸载" class="headerlink" title="9. 安装卸载"></a>9. 安装卸载</h4><h5 id="源代码安装"><a href="#源代码安装" class="headerlink" title="源代码安装"></a>源代码安装</h5><pre><code class="hljs shell">./configure # 软件配置和检查make # 编译make install   # 安装</code></pre><h5 id="二进制包安装-：-apt-get-ubuntu-deb-package"><a href="#二进制包安装-：-apt-get-ubuntu-deb-package" class="headerlink" title="二进制包安装 ： apt-get (ubuntu - deb package)"></a>二进制包安装 ： apt-get (ubuntu - deb package)</h5><pre><code class="hljs shell">apt-get install [package_name]      使用 apt-get安装包apt-get remove [package_name]    使用 apt-get卸载包</code></pre><h5 id="秘钥生成"><a href="#秘钥生成" class="headerlink" title="秘钥生成"></a>秘钥生成</h5><pre><code class="hljs shell">ssh-keygen -f key_name   -C &quot;description&quot;</code></pre><h4 id="11-常见的开关机命令"><a href="#11-常见的开关机命令" class="headerlink" title="11. 常见的开关机命令"></a>11. 常见的开关机命令</h4><h5 id="关机命令"><a href="#关机命令" class="headerlink" title="关机命令"></a>关机命令</h5><pre><code class="hljs shell">shutdown -h now   # 关机（保存当前正在运行的程序，相对安全）✨<span class="hljs-meta">#</span><span class="bash"> 其它： </span>halt         poweroff        init 0</code></pre><h5 id="重启命令"><a href="#重启命令" class="headerlink" title="重启命令"></a>重启命令</h5><pre><code class="hljs shell">shutdown -h now      # 重启 (保存当前正在运行的程序，相对安全)  ✨rebootinit 6</code></pre><h5 id="退出登录："><a href="#退出登录：" class="headerlink" title="退出登录："></a>退出登录：</h5><pre><code class="hljs shell">logout  # 建议每次离开服务器的时候退出登录 ✨</code></pre><h4 id="12-窗口管理命令"><a href="#12-窗口管理命令" class="headerlink" title="12.  窗口管理命令"></a>12.  窗口管理命令</h4><h5 id="标签管理"><a href="#标签管理" class="headerlink" title="标签管理"></a>标签管理</h5><pre><code class="hljs shell">[Ctrl] + [Shift]  + T   #  新建一个ternimal 标签 [Ctrl] + [Shift] + W    #  关闭 terminal 标签 Ctrl + PD / Ctrl + PU   #  切换 terminal 标签页 Alt+n   #   切换到标签页n</code></pre><h5 id="窗口管理"><a href="#窗口管理" class="headerlink" title="窗口管理"></a>窗口管理</h5><pre><code class="hljs shell">Ctrl + Shift + N    # 新建terminal窗口Ctrl + shift + Q     # 关闭 terminal 窗口  Alt + Tab    # 在窗口之间切换F11    # 窗口全屏(exit 退出)</code></pre><h4 id="13-其它"><a href="#13-其它" class="headerlink" title="13. 其它"></a>13. 其它</h4><h5 id="1-常见的热键"><a href="#1-常见的热键" class="headerlink" title="(1) 常见的热键"></a>(1) 常见的热键</h5><pre><code class="hljs shell">[Ctrl] + c  # 停止当前命令     [Ctrl] + d  # 注销当前会话     [Ctrl]+z   # 结束交互  [↑]         # 重复上一条命令         [Tab]       # 命令补齐/文件补齐</code></pre><h5 id="2-帮助命令："><a href="#2-帮助命令：" class="headerlink" title="(2) 帮助命令："></a>(2) 帮助命令：</h5><pre><code class="hljs shell">man [cmd]  # 查询命令command的说明文档   [n]  指定分类  -k 关键字[command] --help info [cmd]whatis [cmd] # 简要说明命令的作用which [cmd] # 查看程序的binary文件所在路径whereis [cmd] # 查看程序的搜索路径sudo ldconfigsource history  # 查看历史记录</code></pre><h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 重定向</span><span class="hljs-meta">&gt;</span><span class="bash">   <span class="hljs-comment"># 标准输出重定向(覆盖)  </span></span><span class="hljs-meta">2&gt;</span><span class="bash">  <span class="hljs-comment"># 标准错误输出重定向(覆盖)   </span></span><span class="hljs-meta">&gt;</span><span class="bash">&gt;  <span class="hljs-comment"># 标准输出重定向(追加) </span></span><span class="hljs-meta">2&gt;</span><span class="bash">&gt; <span class="hljs-comment"># 标准错误输出重定向(追加) </span></span>[command] &gt;&gt; [file1] 2&gt;&gt; [file2] # 将正确输出追加到file1，错误输出追加到file2<span class="hljs-meta">#</span><span class="bash"> 多命令顺序执行:     </span>;  # 无关联顺序执行   ;        与  &amp;&amp;        或  ||&amp;&amp; # 与||  # 或|   # 将命令1的正确输出作为命令2 的输入</code></pre><p>通用的参数</p><pre><code class="hljs shell">-a  # 所有 all    -l  # 长格式 long-r  # 递归  recursive-h  # 人类可读 human-t  # 类型 type</code></pre><p>参考资料：</p><ol><li><p><a href="https://fosswire.com/post/2007/08/unixlinux-command-cheat-sheet/">Linux Command cheatsheet</a></p></li><li><p><a href="http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/index.html">http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/index.html</a></p></li><li><p><a href="https://github.com/chassing/linux-sysadmin-interview-questions">https://github.com/chassing/linux-sysadmin-interview-questions</a></p></li><li><p><a href="http://www.imooc.com/course/programdetail/pid/45">http://www.imooc.com/course/programdetail/pid/45</a></p></li><li><p><a href="http://billie66.github.io/TLCL/book/">http://billie66.github.io/TLCL/book/</a></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>git</title>
    <link href="/2018/06/01/Git-usage/"/>
    <url>/2018/06/01/Git-usage/</url>
    
    <content type="html"><![CDATA[<p>Git 基本用法</p><a id="more"></a><h3 id="1-安装和配置"><a href="#1-安装和配置" class="headerlink" title="1. 安装和配置"></a>1. 安装和配置</h3><pre><code class="hljs shell">sudo apt-get install git  # 安装 git</code></pre><pre><code class="hljs shell">git config --global user.name &quot;Your Name&quot; # 设置用户名git config --global user.email &quot;email@example.com&quot;  # 设置 email<span class="hljs-meta">#</span><span class="bash">  --global 表示系统用户级别，保存于  ~/.gitconfig   </span><span class="hljs-meta">#</span><span class="bash">  缺省则为项目级别/仓库级别， 保存于当前目录下的　.git/config</span></code></pre><pre><code class="hljs shell">ssh-keygen -t rsa -b 4096 -C &quot;your_email@domain.com&quot; <span class="hljs-meta">#</span><span class="bash"> 生成秘钥对， 公钥放git, 私钥放本地(一般放置在 ~/.ssh)</span></code></pre><h3 id="2-基本用法"><a href="#2-基本用法" class="headerlink" title="2. 基本用法"></a>2. 基本用法</h3><p><img src="/2018/06/01/Git-usage/git1.jpg" alt></p><p>基本的概念：</p><p><strong>HEAD</strong>: 在 Git 中，HEAD指针是一个指向你正在工作中的本地分支的指针。</p><p><strong>master</strong>：分支并不是一个特殊分支。 它就跟其它分支完全没有区别。 之所以几乎每一个仓库都有 master 分支，是因为 git init 命令默认创建它，并且大多数人都懒得去改动它。<br><strong>origin</strong>：是当你运行 git clone 时默认的远程仓库名字。</p><p><strong>Workspace</strong>：工作区                                               <strong>Index / Stage</strong>：暂存区<br><strong>Repository</strong>：仓库区（或本地仓库）                    <strong>Remote</strong>：远程仓库</p><pre><code class="hljs shell">git init  # 当前目录下会自动生成一个.git的目录，用来跟踪管理版本库</code></pre><pre><code class="hljs shell">git add &lt;filename&gt; # 添加文件git rm &lt;filename&gt;  # 删除文件   -r 删除文件夹git commit -m &quot;wrote a commit msg&quot;  #　从暂存区提交到本地仓库git rm --cached &lt;filename&gt;  # 舍弃暂存区内容git push origin master #　从本地仓库推送到远程仓库</code></pre><pre><code class="hljs shell">git clone /path/to/repository  # 创建一个远程仓库的本地克隆版本git clone usename@host:/path/to/repository # 远程服务器上的仓库git checkout -- &lt;filename&gt;  # 替换本地改动，　将仓库中的文件替换掉工作目录中的文件git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; 　# 取回远程主机某个分支的更新，再与本地的指定分支合并</code></pre><pre><code class="hljs shell">git status  # 显示工作目录和暂存区的状态（显示所在分支、工作区和暂存区的状态）</code></pre><h3 id="3-版本回退和前进"><a href="#3-版本回退和前进" class="headerlink" title="3. 版本回退和前进"></a>3. 版本回退和前进</h3><pre><code class="hljs shell">git log # 查看提交历史(包括　ID, author, data等信息)  ＃　--pretty=oneline  一条日志只显示一行    --online 更加简洁的方式显示git reflog # 显示版本移动步数 HEAD@&#123;移动到当前版本需要的步数&#125;</code></pre><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 版本回退与前进</span>git reset --hard Index_value # 基于索引值 进行版本回退和前进<span class="hljs-meta">#</span><span class="bash"> --soft 仅仅在本地库移动 HEAD 指针  　　</span><span class="hljs-meta">#</span><span class="bash"> --mixed 在本地库移动 HEAD 指针， 并重置暂存区</span><span class="hljs-meta">#</span><span class="bash"> --hard 在本地库移动HEAD 指针，并重置暂存区和工作区</span>git reset --hard HEAD # 舍弃工作区和暂存区的修改，直接用本地库替换</code></pre><pre><code class="hljs shell">git diff filename # 默认比较的是工作区和暂存区git diff [HEAD] filename # 比较工作区和本地库[HEAD]，可以通过修改[HEAD]与历史记录进行比较</code></pre><h3 id="4-分支管理"><a href="#4-分支管理" class="headerlink" title="4. 分支管理"></a>4. 分支管理</h3><p><img src="/2018/06/01/Git-usage/gitflow.png" alt></p><p>其中涉及到的主要分支类型有：</p><ul><li><p><strong>master分支</strong>，即主分支。任何项目都必须有个这个分支。<strong>对项目进行tag或发布版本等操作，都必须在该分支上进行</strong>。</p></li><li><p><strong>develop分支</strong>，即开发分支，<strong>从master分支上检出</strong>。<strong>团队成员一般不会直接更改该分支，而是分别从该分支检出自己的feature分支，开发完成后将feature分支上的改动merge回develop分支</strong>。<strong>同时release分支由此分支检出</strong>。</p><pre><code class="hljs shell">git branch develop   # 从当前分支上新建develop分支git checkout develop    # 检出develop分支<span class="hljs-meta">#</span><span class="bash"> ... 此处可进行功能开发，并add和commit到develop分支</span>git push origin develop    # 推送develop分支到远端的origin</code></pre></li><li><p><strong>release分支</strong>，即发布分支，<strong>从develop分支上检出,该分支用作发版前的测试，可进行简单的bug修复</strong>。如果bug修复比较复杂，可merge回develop分支后由其他分支进行bug修复。此分支测试完成后，需要同时merge到master和develop分支上。</p></li><li><p><strong>feature分支</strong>，即功能分支，<strong>从develop分支上检出。团队成员中每个人都维护一个自己的feature分支，并进行开发工作，开发完成后将此分支merge回develop分支</strong>。此分支一般用来开发新功能或进行项目维护等。</p><pre><code class="hljs shell">git clone /path/to/repositorygit checkout develop　# 检出 develop分支git checkout -b feature-hu develop    # 从develop分支新建并检出feature分支... # 这里可以进行一些功能开发，并不断的add和commitgit checkout develop    # 切换回develop分支git pull origin develop    # 更新远端代码，看develop分支是否有更新（无更新）git checkout feature-hu    # 切换回feature分支git rebase develop    # 合并develop分支到feature分支，并解决冲突（无冲突）git checkout develop    # 切换回develop分支git merge --no-ff feature-hu    # 合并feature分支到develop分支git push origin develop   # 推送develop分支到远端</code></pre></li><li><p><strong>fix分支</strong>，即补丁分支，<strong>由develop分支检出，用作bug修复</strong>，bug修复完成需merge回develop分支，并将其删除。所以该分支属于临时性分支。</p></li><li><p><strong>hotfix分支</strong>，即热补丁分支。和fix分支的区别在于，<strong>该分支由master分支检出，进行线上版本的bug修复</strong>，修复完成后merge回master分支，并merge到develop分支上，merge完成后也可以将其删除，也属于临时性分支。</p><pre><code class="hljs shell">git checkout master    # 切换回master分支git checkout -b hotfix master    # 新建hotfix分支，并切换到该分支......                 # 做一些bug修复工作git checkout master    # 切换回master分支git merge --no-ff hotfix    # 合并hotfix分支，此时bug已被修复（无冲突）git tag v0.2.1    # 新建tag v0.2git push origin master    # 推送master分支代码到远端git push origin --tags    # 推送tag到远端</code></pre></li></ul><p>常见分支管理命令小结：</p><pre><code class="hljs shell">git branch -v  # 查看分支git branch &lt;branch_name&gt;   # 从当前分支新建一个分支git checkout &lt;branch_name&gt;   # 切换到指定分支git checkout -b &lt;branch_name&gt;  # 新建分支并切换到该分支 <span class="hljs-meta">#</span><span class="bash"> git checkout -b = git branch + git checkout</span>git checkout -d &lt;branch_name&gt; # 删除指定分支git push &lt;origin&gt; &lt;branch_name&gt; # 推送某个分支到远程仓库</code></pre><pre><code class="hljs nginx"><span class="hljs-attribute">git</span> merge --<span class="hljs-literal">no</span>-ff &lt;branch_name&gt; <span class="hljs-comment"># 将指定分支合并到当前分支</span>git rebase   <span class="hljs-comment"># </span></code></pre><pre><code class="hljs shell">git tag v0.2 # 为当前分支打 taggit push origin --tags # 将tag推送到远程仓库</code></pre><p><strong>如何解决冲突</strong></p><pre><code class="hljs vala"><span class="hljs-meta"># 解决冲突</span><span class="hljs-meta"># step 1: 手动修改冲突文件</span><span class="hljs-meta"># step 2： git add filename</span><span class="hljs-meta"># step 3: git commit -m &quot;commit message&quot;</span></code></pre><h3 id="5-远程操作"><a href="#5-远程操作" class="headerlink" title="5. 远程操作"></a>5. 远程操作</h3><p><img src="/2018/06/01/Git-usage/git5.png" alt></p><h5 id="团队内部协作："><a href="#团队内部协作：" class="headerlink" title="团队内部协作："></a>团队内部协作：</h5><pre><code class="hljs shell">git remote add origin https://xxxx.xxxx.xxxx.xxxx  # 指定远程库的别名git push -u 远程库名称 本地库名称git clone https://xxxx.xxxx.xxxx.xxxx   # clone 到本地库git fetch [远程地址别名][远程分支名]  #git merge [远程地址别名/远程分支名] git pull <span class="hljs-meta">#</span><span class="bash"> git fetch是将远程主机的最新内容拉到本地，用户在检查了以后决定是否合并到工作本机分支中。</span><span class="hljs-meta">#</span><span class="bash"> 而 git pull 则是将远程主机的最新内容拉下来后直接合并，</span><span class="hljs-meta">#</span><span class="bash"> 即：git pull = git fetch + git merge</span></code></pre><h5 id="跨团队协作："><a href="#跨团队协作：" class="headerlink" title="跨团队协作："></a>跨团队协作：</h5><p>需要在代码托管中心完成<code>fork</code>、<code>pull request</code>、 <code>审核</code>、<code>merge</code> 等操作</p><p><img src="/2018/06/01/Git-usage/git6.png" alt></p><h3 id="6-安装-GitLab"><a href="#6-安装-GitLab" class="headerlink" title="6. 安装 GitLab"></a>6. 安装 GitLab</h3><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 安装</span><span class="hljs-meta">$</span><span class="bash"> curl -sS http://packages.gitlab.com.cn/install/gitlab-ce/script.deb.sh | sudo bash</span><span class="hljs-meta">$</span><span class="bash"> sudo apt-get install gitlab-ce</span><span class="hljs-meta">#</span><span class="bash"> 配置</span><span class="hljs-meta">$</span><span class="bash"> gitlab-ctl reconfigure </span><span class="hljs-meta">#</span><span class="bash"> 启动</span><span class="hljs-meta">$</span><span class="bash"> gitlab-ctl start </span><span class="hljs-meta">#</span><span class="bash"> 在浏览器输入对应的IP地址即可访问gitlab后台，可能需要关闭防火墙</span><span class="hljs-meta">#</span><span class="bash"> （1）对root用户设置密码</span><span class="hljs-meta">#</span><span class="bash"> （2）登陆：用户名默认为root  </span><span class="hljs-meta">#</span><span class="bash"> （3）在后台进行添加用户等操作、设置密码等操作</span></code></pre><h3 id="7-其他"><a href="#7-其他" class="headerlink" title="7. 其他"></a>7. 其他</h3><p>（1） 如何设置不被追踪的文件<br>     有些文件是不想被追踪的， 可以修改  <code>.git/info/exclude</code> 文件， 以 # 开头，添加规则即可。</p><p>（2）server certificate verification failed. CAfile:/etc/ssl/certs/ca-certificates.crt CRLfile: none</p><pre><code class="hljs shell">export GIT_SSL_NO_VERIFY=1</code></pre><h3 id="8-参考资料"><a href="#8-参考资料" class="headerlink" title="8. 参考资料"></a>8. 参考资料</h3><ul><li><p>git 简明指南  <a href="https://rogerdudler.github.io/git-guide/index.zh.html">https://rogerdudler.github.io/git-guide/index.zh.html</a></p></li><li><p>git-scm-zh: <a href="https://git-scm.com/book/zh/v2">https://git-scm.com/book/zh/v2</a></p></li><li>常见　UI 界面: <a href="https://www.sourcetreeapp.com/">SourceTree</a>      <a href="https://tortoisegit.org/">tortoisegit</a>     <a href="https://www.git-tower.com/mac">Tower</a></li><li>gitflow <a href="https://nvie.com/posts/a-successful-git-branching-model/">https://nvie.com/posts/a-successful-git-branching-model/</a></li><li><p>gitflow 备忘清单 <a href="https://danielkummer.github.io/git-flow-cheatsheet/index.zh_CN.html">https://danielkummer.github.io/git-flow-cheatsheet/index.zh_CN.html</a></p></li><li><p>互动式学习git的网站：　<a href="https://learngitbranching.js.org/">https://learngitbranching.js.org/</a></p></li><li>图解git:  <a href="https://my.oschina.net/xdev/blog/114383">https://my.oschina.net/xdev/blog/114383</a></li><li>git magic: <a href="http://www-cs-students.stanford.edu/~blynn/gitmagic/intl/zh_cn/index.html">http://www-cs-students.stanford.edu/~blynn/gitmagic/intl/zh_cn/index.html</a></li><li>github help: <a href="https://help.github.com/en">https://help.github.com/en</a></li><li>本地搭建Git： <a href="https://about.gitlab.com/">GitLab</a> 和　<a href="https://github.com/gogs/gogs">dashboard</a></li><li><a href="https://zhuanlan.zhihu.com/p/23478654">图文详解如何利用Git+Github进行团队协作开发</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Facenet细节剖析</title>
    <link href="/2018/04/13/Facenet/"/>
    <url>/2018/04/13/Facenet/</url>
    
    <content type="html"><![CDATA[<p>paper facenet &amp; mobilefacenet 实现细节剖析</p><a id="more"></a><h3 id="人脸识别项目-细节剖析"><a href="#人脸识别项目-细节剖析" class="headerlink" title="人脸识别项目 细节剖析"></a>人脸识别项目 细节剖析</h3><h5 id="1-图片输入的大小是多少？最后的-embedding-是多少？-为什么？"><a href="#1-图片输入的大小是多少？最后的-embedding-是多少？-为什么？" class="headerlink" title="1. 图片输入的大小是多少？最后的 embedding 是多少？ 为什么？"></a>1. 图片输入的大小是多少？最后的 embedding 是多少？ 为什么？</h5><p>图片输入大小设置为 160x160。why？ 随着图片分辨率的增大，验证集的准确率会上升，但是对应的运算量也会显著上升。160 是一个相对合适的分辨率大小。</p><p>(facenet 原论文尝试了 40x40, 80x80, 112x112, 160x160, 256x256)</p><p>embedding 的大小为 128， 实验测得，当超过 128 时候，验证集的准确率已经没有上升，反而有所下降。(64 -&gt; 128 -&gt; 256 -&gt; 512)。why？ 128D向量拥有足够的容量hold 住大规模的人脸数据集(百万人脸级别)， 并且相对紧凑。</p><h5 id="2-基本原理"><a href="#2-基本原理" class="headerlink" title="2. 基本原理"></a>2. 基本原理</h5><p>选取三元组(anchor, pos, neg)，其中，x和p是同一类，x和n是不同类。那么学习的过程就是学到一种表示，对于尽可能多的三元组，使得anchor和pos的距离，小于anchor和neg的距离</p><h5 id="3-主干网络："><a href="#3-主干网络：" class="headerlink" title="3. 主干网络："></a>3. 主干网络：</h5><p>原始论文中使用的是 googlenet、github上的facenet使用的 inception resnet v1、mobilefacenet 使用的是 mobilenet v2</p><h6 id="1-GoogleNet"><a href="#1-GoogleNet" class="headerlink" title="(1) GoogleNet"></a>(1) GoogleNet</h6><p>原论文中使用的是 GoogleNet， Googlenet 的主要涉及在于以下几点：</p><ul><li>Inception Module中包含3种不同尺寸的卷积和1个最大池化，增加了网络对不同尺度的适应性。</li></ul><p>第一个分支对输入进行 1x1卷积，<strong>1x1卷积可以跨通道组织信息，提高网络的表达能力，同时可以对输出通道升维和降维</strong>。</p><p>第二个分支先使用了 1x1 卷积，然后连接 3x3 卷积，相当于进行两次特征变换。</p><p>第三个分支和第二个分支类似，先是使用了1x1 的卷积，然后连接 5x5 的卷积。</p><p>最后一个分支则是3x3 最大池化后直接使用1x1卷积。</p><p>Inception Module 的4个分支在最后通过一个聚合操作合并。</p><ul><li>去除了最后的全连接层，用全局平均池化层来取代它。</li></ul><h6 id="2-Mobilenet-v2"><a href="#2-Mobilenet-v2" class="headerlink" title="(2) Mobilenet v2"></a>(2) Mobilenet v2</h6><p>另一篇论文则使用了 mobilenetv2 (输入变成112) 结构作为主要网络：主要修改点如下所示：</p><ul><li><p>Inception 结构替换为 mobilenet v2 结构：</p><p>mobilenet 的结构要点：（1）skip-connection 跳层连接 (2) 将传统卷积分解为逐通道卷积(先进行逐通道卷积但是通道之间不相加，然后使用1x1 卷积对通道进行整合), 并在前面加一个1x1 的卷积进行升维。(3) 将最后的 ReLU 替换为 Linear(非线性在高维有溢出，但是在低维不如线性好) </p></li><li><p>用全局可分离卷积替代原有的全局池化层。why? 特征图上的中心点的感受野和边角的感受野是不同的，中心点的感受野包括了完整的图片，边角点的感知域却只有部分的图片， 不应该视为同等重要。这样会导致性能的下降。</p></li><li><p>使用 arcface 替换掉 triplet 损失函数。 </p></li><li><p>小细节：通道扩张倍数变小(facenet 是 1024， mobilenets 则是 512)；使用prelu代替relu；使用batch Normalization。</p></li></ul><h5 id="4-预处理工作："><a href="#4-预处理工作：" class="headerlink" title="4. 预处理工作："></a>4. 预处理工作：</h5><p>(1) 数据集的整理和清洗(DDM 智能猫眼[大约10W张，6千人的样子-&gt; 猫眼300台，每台20人]的真实数据 和 CASIA 人脸数据)</p><ul><li>类间过滤： 清洗掉距离与类中心小于0.5 的负样本(对一个类别，先计算类中心，然后用所有样本，与其进行比较)</li><li><p>图片的过滤：将经过人脸检测(MTCNN/FaceBoxes) 返回的人脸置信度 小于 0.75 的图片直接过滤掉</p></li><li><p>按图像数量阈值进行数据划分(人脸低于10张的过滤掉)</p></li><li>类间距离问题：有些可能是名字重合的，找出来合并在一起</li></ul><p>(2) 人脸识别前的处理：</p><ul><li><p>人脸对齐？可以有也可以没有，影响不是很大 how?( 通过仿射变换将原本的五个landmark缩放旋转到固定的位置)</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> skimage <span class="hljs-keyword">import</span> transform <span class="hljs-keyword">as</span> trans<span class="hljs-comment"># src</span>src = np.array([  [<span class="hljs-number">30.2946</span>, <span class="hljs-number">51.6963</span>],  [<span class="hljs-number">65.5318</span>, <span class="hljs-number">51.5014</span>],  [<span class="hljs-number">48.0252</span>, <span class="hljs-number">71.7366</span>],  [<span class="hljs-number">33.5493</span>, <span class="hljs-number">92.3655</span>],  [<span class="hljs-number">62.7299</span>, <span class="hljs-number">92.2041</span>] ], dtype=np.float32 )<span class="hljs-keyword">if</span> image_size[<span class="hljs-number">1</span>]==<span class="hljs-number">112</span>:  src[:,<span class="hljs-number">0</span>] += <span class="hljs-number">8.0</span>  <span class="hljs-comment"># dst</span>dst = landmark.astype(np.float32)tform = trans.SimilarityTransform()tform.estimate(dst, src)M = tform.params[<span class="hljs-number">0</span>:<span class="hljs-number">2</span>,:]<span class="hljs-comment"># warpAffine</span>warped = cv2.warpAffine(img, M, (image_size[<span class="hljs-number">1</span>],image_size[<span class="hljs-number">0</span>]), borderValue = <span class="hljs-number">0.0</span>)</code></pre></li><li><p>模糊过滤？（将检测出来的人脸使用拉普拉斯做卷积运算，然后计算方差， 将方差小于 400 过滤掉)</p></li><li>人脸置信度，这里设置置信度0.8，人脸置信度太小可能不是人脸，直接过滤掉。</li><li>对图片进行标准化处理：(x-均值)/标准差</li></ul><h5 id="5-后处理-如何进行分类？"><a href="#5-后处理-如何进行分类？" class="headerlink" title="5. 后处理(如何进行分类？)"></a>5. 后处理(如何进行分类？)</h5><p>facenet 使用欧式距离来度量是否是一个人。使用十折交叉验证，我们设定阈值为 1.24。</p><p>mobilenet 使用相似性余弦度量：阈值设定为 0.7。</p><p><img src="/2018/04/13/Facenet/3.png" alt="3"></p><h5 id="6-参数调整：-没有怎么调整参数，-使用默认参数，替换掉了数据集而已"><a href="#6-参数调整：-没有怎么调整参数，-使用默认参数，替换掉了数据集而已" class="headerlink" title="6. 参数调整：(没有怎么调整参数， 使用默认参数，替换掉了数据集而已)"></a>6. 参数调整：(没有怎么调整参数， 使用默认参数，替换掉了数据集而已)</h5><ul><li><p>优化器：<code>RMSProp</code></p><pre><code class="hljs apache"><span class="hljs-attribute">tf</span>.train.RMSPropOptimizer(learning_rate, decay=<span class="hljs-number">0</span>.<span class="hljs-number">9</span>, momentum=<span class="hljs-number">0</span>.<span class="hljs-number">9</span>, epsilon=<span class="hljs-number">1</span>.<span class="hljs-number">0</span>)</code></pre></li><li><p>初始学习率：0.1</p></li><li><p>学习率下降方式：exponential_decay</p><pre><code class="hljs yaml"><span class="hljs-comment"># Learning rate schedule: Maps an epoch number to a learning rate</span><span class="hljs-attr">0:</span>  <span class="hljs-number">0.1</span>  <span class="hljs-comment"># 初始学习率</span><span class="hljs-attr">300:</span> <span class="hljs-number">0.01</span><span class="hljs-attr">400:</span> <span class="hljs-number">0.001</span><span class="hljs-attr">1000:</span> <span class="hljs-number">0.0001</span></code></pre></li><li><p>margin α=0.2</p></li></ul><h5 id="7-why-facenet-？"><a href="#7-why-facenet-？" class="headerlink" title="7. why facenet ？"></a>7. why facenet ？</h5><ul><li><p>softmax不直接，(三元组直接优化距离)，因⽽而性能也不不好。 softmax产⽣生的特征表示向量量都很⼤大，⼀一般超过1000维。 </p></li><li><p>faceNet并没有像DeepFace和DeepID那样需要对⻬。 </p></li><li><p>faceNet得到最终表示后不不⽤用像DeepID那样需要再训练模型进⾏分类，直接计算距离就好了了，简单⽽而有效。 </p></li></ul><h5 id="8-模型的大小："><a href="#8-模型的大小：" class="headerlink" title="8. 模型的大小："></a>8. 模型的大小：</h5><p>线上模型： model_size:  (Inception resnet v2： 186M)      Param：7.5M   </p><p>线下模型：(手机端) model_size:  4.1M       param:  0.99M       </p><h5 id="8-训练技巧"><a href="#8-训练技巧" class="headerlink" title="8. 训练技巧"></a>8. 训练技巧</h5><ul><li>三元组的选择<pre><code>     在一个minibatch中，我们根据当时的 embedding，选择一次三元组，在这些三元组上计算triplet-loss,  再对embedding进行更新，不断重复，直到收敛或训练到指定迭代次数。</code></pre></li></ul>]]></content>
    
    
    <categories>
      
      <category>Face</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Hexo_blog_issue</title>
    <link href="/2018/03/04/Hexo-blog-issue/"/>
    <url>/2018/03/04/Hexo-blog-issue/</url>
    
    <content type="html"><![CDATA[<p>Hexo 部署、配置、修改等常见问题记录</p><a id="more"></a><h4 id="1-写文章、发布文章"><a href="#1-写文章、发布文章" class="headerlink" title="1. 写文章、发布文章"></a>1. 写文章、发布文章</h4><pre><code class="hljs shell">hexo new post article_title    # 新建一篇文章<span class="hljs-meta">#</span><span class="bash"> 编辑 [root]\<span class="hljs-built_in">source</span>\_posts 下的markdown文件</span>hexo g   # 生成静态网页hexo s   # 本地预览效果hexo d   # 上传到github</code></pre><p>文章开头的配置如下所示</p><pre><code class="hljs markdown">title: 标题catalog: 是否显示段落目录date: 文章日期subtitle: 子标题header-img: 顶部背景图片top: 是否置顶tags: 标签categories: 分类</code></pre><h4 id="2-添加图片"><a href="#2-添加图片" class="headerlink" title="2.  添加图片"></a>2.  添加图片</h4><ol><li><p>修改配置文件，把主页配置文件 <strong>_config.yml</strong> 里的 <strong>post_asset_folder</strong>:这个选项设置为<strong>true</strong>。</p></li><li><p>安装可以上传本地图片的插件， <code>npm install hexo-asset-image --save</code>。此时运行<code>hexo n &quot;xxxx&quot;</code>来生成md博文时，<strong>/source/_posts</strong> 文件夹内除了 <strong>xxxx.md</strong> 文件还有一个同名的文件夹。</p></li><li><p>将图片放在 <strong>xxxx</strong> 这个文件夹中，在xxxx.md中使用markdown的格式引入图片：</p><pre><code class="hljs markdown">![<span class="hljs-string">title</span>](<span class="hljs-link">xxxx/img_name.jpg</span>)</code></pre></li><li><p>修改 <code>/node_modules/hexo-asset-image/index.js</code> ， 替换为如下代码:</p><pre><code class="hljs js"><span class="hljs-meta">&#x27;use strict&#x27;</span>;<span class="hljs-keyword">var</span> cheerio = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;cheerio&#x27;</span>);<span class="hljs-comment">// http://stackoverflow.com/questions/14480345/how-to-get-the-nth-occurrence-in-a-string</span><span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">getPosition</span>(<span class="hljs-params">str, m, i</span>) </span>&#123;  <span class="hljs-keyword">return</span> str.split(m, i).join(m).length;&#125;<span class="hljs-keyword">var</span> version = <span class="hljs-built_in">String</span>(hexo.version).split(<span class="hljs-string">&#x27;.&#x27;</span>);hexo.extend.filter.register(<span class="hljs-string">&#x27;after_post_render&#x27;</span>, <span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params">data</span>)</span>&#123;  <span class="hljs-keyword">var</span> config = hexo.config;  <span class="hljs-keyword">if</span>(config.post_asset_folder)&#123;    <span class="hljs-keyword">var</span> link = data.permalink;<span class="hljs-keyword">if</span>(version.length &gt; <span class="hljs-number">0</span> &amp;&amp; <span class="hljs-built_in">Number</span>(version[<span class="hljs-number">0</span>]) == <span class="hljs-number">3</span>)   <span class="hljs-keyword">var</span> beginPos = getPosition(link, <span class="hljs-string">&#x27;/&#x27;</span>, <span class="hljs-number">1</span>) + <span class="hljs-number">1</span>;<span class="hljs-keyword">else</span>   <span class="hljs-keyword">var</span> beginPos = getPosition(link, <span class="hljs-string">&#x27;/&#x27;</span>, <span class="hljs-number">3</span>) + <span class="hljs-number">1</span>;<span class="hljs-comment">// In hexo 3.1.1, the permalink of &quot;about&quot; page is like &quot;.../about/index.html&quot;.</span><span class="hljs-keyword">var</span> endPos = link.lastIndexOf(<span class="hljs-string">&#x27;/&#x27;</span>) + <span class="hljs-number">1</span>;    link = link.substring(beginPos, endPos);    <span class="hljs-keyword">var</span> toprocess = [<span class="hljs-string">&#x27;excerpt&#x27;</span>, <span class="hljs-string">&#x27;more&#x27;</span>, <span class="hljs-string">&#x27;content&#x27;</span>];    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">var</span> i = <span class="hljs-number">0</span>; i &lt; toprocess.length; i++)&#123;      <span class="hljs-keyword">var</span> key = toprocess[i];       <span class="hljs-keyword">var</span> $ = cheerio.load(data[key], &#123;        ignoreWhitespace: <span class="hljs-literal">false</span>,        xmlMode: <span class="hljs-literal">false</span>,        lowerCaseTags: <span class="hljs-literal">false</span>,        decodeEntities: <span class="hljs-literal">false</span>      &#125;);      $(<span class="hljs-string">&#x27;img&#x27;</span>).each(<span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params"></span>)</span>&#123;<span class="hljs-keyword">if</span> ($(<span class="hljs-built_in">this</span>).attr(<span class="hljs-string">&#x27;src&#x27;</span>))&#123;<span class="hljs-comment">// For windows style path, we replace &#x27;\&#x27; to &#x27;/&#x27;.</span><span class="hljs-keyword">var</span> src = $(<span class="hljs-built_in">this</span>).attr(<span class="hljs-string">&#x27;src&#x27;</span>).replace(<span class="hljs-string">&#x27;\\&#x27;</span>, <span class="hljs-string">&#x27;/&#x27;</span>);<span class="hljs-keyword">if</span>(!<span class="hljs-regexp">/http[s]*.*|\/\/.*/</span>.test(src) &amp;&amp;   !<span class="hljs-regexp">/^\s*\//</span>.test(src)) &#123;  <span class="hljs-comment">// For &quot;about&quot; page, the first part of &quot;src&quot; can&#x27;t be removed.</span>  <span class="hljs-comment">// In addition, to support multi-level local directory.</span>  <span class="hljs-keyword">var</span> linkArray = link.split(<span class="hljs-string">&#x27;/&#x27;</span>).filter(<span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params">elem</span>)</span>&#123;<span class="hljs-keyword">return</span> elem != <span class="hljs-string">&#x27;&#x27;</span>;  &#125;);  <span class="hljs-keyword">var</span> srcArray = src.split(<span class="hljs-string">&#x27;/&#x27;</span>).filter(<span class="hljs-function"><span class="hljs-keyword">function</span>(<span class="hljs-params">elem</span>)</span>&#123;<span class="hljs-keyword">return</span> elem != <span class="hljs-string">&#x27;&#x27;</span> &amp;&amp; elem != <span class="hljs-string">&#x27;.&#x27;</span>;  &#125;);  <span class="hljs-keyword">if</span>(srcArray.length &gt; <span class="hljs-number">1</span>)srcArray.shift();  src = srcArray.join(<span class="hljs-string">&#x27;/&#x27;</span>);  $(<span class="hljs-built_in">this</span>).attr(<span class="hljs-string">&#x27;src&#x27;</span>, config.root + link + src);  <span class="hljs-built_in">console</span>.info&amp;&amp;<span class="hljs-built_in">console</span>.info(<span class="hljs-string">&quot;update link as:--&gt;&quot;</span>+config.root + link + src);&#125;&#125;<span class="hljs-keyword">else</span>&#123;<span class="hljs-built_in">console</span>.info&amp;&amp;<span class="hljs-built_in">console</span>.info(<span class="hljs-string">&quot;no src attr, skipped...&quot;</span>);<span class="hljs-built_in">console</span>.info&amp;&amp;<span class="hljs-built_in">console</span>.info($(<span class="hljs-built_in">this</span>));&#125;      &#125;);      data[key] = $.html();    &#125;  &#125;&#125;);</code></pre></li><li><p>检查 <code>_config.yml</code> 文件， 是否修改了对应的 URL：</p><pre><code class="hljs yaml"><span class="hljs-comment"># URL</span><span class="hljs-comment">## If your site is put in a subdirectory, set url as &#x27;http://example.com/child&#x27; and root as &#x27;/child/&#x27;</span><span class="hljs-attr">url:</span> <span class="hljs-string">https://polariszhao.github.io/</span>  <span class="hljs-comment"># 修改为对应的 URL</span></code></pre></li></ol><p><img src="/2018/03/04/Hexo-blog-issue/lena.bmp" alt="test_img"></p>]]></content>
    
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>issue</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
