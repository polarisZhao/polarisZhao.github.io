

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="zhichao zhao">
  <meta name="keywords" content="">
  <title>awesome-face - ÂÅáÊ¨¢ÁïÖ Âèà‰ΩïÂ¶® Êó†‰∫∫ÂÖ±‰∫´</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/tomorrow.min.css" />
    
  

  


<!-- ‰∏ªÈ¢ò‰æùËµñÁöÑÂõæÊ†áÂ∫ìÔºå‰∏çË¶ÅËá™Ë°å‰øÆÊîπ -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_pf9vaxs7x7b.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- Ëá™ÂÆö‰πâÊ†∑Âºè‰øùÊåÅÂú®ÊúÄÂ∫ïÈÉ® -->


  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.1.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>ÂÅáÊ¨¢ÁïÖÔºåÂèà‰ΩïÂ¶®ÔºåÊó†‰∫∫ÂÖ±‰∫´</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container page-header text-center fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2019-04-18 15:40" pubdate>
        April 18, 2019 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      1.7k Â≠ó
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      33
       ÂàÜÈíü
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto" id="post">
            <!-- SEO header -->
            <h1 style="display: none">awesome-face</h1>
            
            <div class="markdown-body" id="post-body">
              <p>my GitHub project awesome-face: face algorithm„ÄÅsource code„ÄÅdatasets„ÄÅconf/workshop/trans</p>
<a id="more"></a>
<h1 id="awesome-face"><a href="#awesome-face" class="headerlink" title="awesome-face"></a>awesome-face</h1><p>üî•  face releated algorithm, datasets and papers  ü§î</p>
<h2 id="üìù-Paper-Algorithm"><a href="#üìù-Paper-Algorithm" class="headerlink" title="üìù Paper / Algorithm"></a>üìù Paper / Algorithm</h2><h4 id="2D-Face-Recognition"><a href="#2D-Face-Recognition" class="headerlink" title="2D- Face Recognition"></a>2D- Face Recognition</h4><p><img src="/2019/04/18/awesome-face/face_reg.png" srcset="/img/loading.gif" alt="2d_face_reg"></p>
<p><strong>[1] DeepID1</strong>  <a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Sun_Deep_Learning_Face_2014_CVPR_paper.pdf"><strong>[paper]</strong></a> </p>
<p>Deep Learning Face Representation from Predicting 10,000 Classes</p>
<p><strong>[2] DeepID2</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.4773"><strong>[paper]</strong></a> </p>
<p>Deep Learning Face Representation by Joint Identification-Verification</p>
<p><strong>[3] DeepID2+</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.1265"><strong>[paper]</strong></a></p>
<p>Deeply learned face representations are sparse, selective, and robust</p>
<p><strong>[4] DeepIDv3</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.00873"><strong>[paper]</strong></a> </p>
<p>DeepID3: Face Recognition with Very Deep Neural Networks</p>
<p><strong>[5] Deep Face</strong> <a target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf"><strong>[paper]</strong></a> </p>
<p>Deep Face Recognition</p>
<p><strong>[6] Center Loss</strong> <a target="_blank" rel="noopener" href="http://ydwen.github.io/papers/WenECCV16.pdf"><strong>[paper]</strong></a>    <a target="_blank" rel="noopener" href="https://github.com/ydwen/caffe-face"><strong>[code]</strong></a></p>
<p>A Discriminative Feature Learning Approach for Deep Face Recognition</p>
<p><strong>[7]Marginal loss</strong> <a target="_blank" rel="noopener" href="https://www.computer.org/csdl/proceedings-article/cvprw/2017/0733c006/12OmNzayNCT"><strong>[paper]</strong></a></p>
<p>Marginal loss for deep face recognition</p>
<p><strong>[8] Range Loss</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.08976"><strong>[paper]</strong></a> </p>
<p>Range Loss for Deep Face Recognition with Long-tail</p>
<p><strong>[9]Contrastive Loss</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.4773"><strong>[paper]</strong></a></p>
<p>Deep learning face representation by joint identification-verification</p>
<p><strong>[10] FaceNet</strong>   <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.03832"><strong>[paper]</strong></a>   <a target="_blank" rel="noopener" href="https://github.com/davidsandberg/facenet">[<strong>third-party implemention</strong>]</a></p>
<p>FaceNet: A Unified Embedding for Face Recognition and Clustering</p>
<p><strong>[11] NormFace</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1704.06369.pdf"><strong>[paper]</strong></a>    <a target="_blank" rel="noopener" href="https://github.com/happynear/NormFace"><strong>[code]</strong></a></p>
<p>NormFace: L2 Hypersphere Embedding for Face Verification</p>
<p><strong>[12] COCO Loss:</strong>    <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.00870.pdf"><strong>[paper]</strong></a>   <a target="_blank" rel="noopener" href="https://github.com/sciencefans/coco_loss">[<strong>code</strong>]</a></p>
<p>Rethinking Feature Discrimination and Polymerization for Large-scale Recognition</p>
<p><strong>[13] Large-Margin Softmax Loss</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.02295.pdf"><strong>[paper]</strong></a>  <a target="_blank" rel="noopener" href="https://github.com/wy1iu/LargeMargin_Softmax_Loss">[<strong>code</strong>]</a></p>
<p>Large-Margin Softmax Loss for Convolutional Neural Networks(L-Softmax loss)</p>
<p><strong>[14]SphereFaceÔºö</strong>  <strong>A-Softmax</strong>   <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.08063"><strong>[paper]</strong></a>  <a target="_blank" rel="noopener" href="https://github.com/wy1iu/sphereface">[<strong>code</strong>]</a></p>
<p>SphereFace: Deep Hypersphere Embedding for Face Recognition</p>
<p><strong>[15]AM-Softmax/cosFace</strong>     <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1801.05599.pdf"><strong>[paper AM-Softmax]</strong></a>       <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1801.09414.pdf"><strong>[paper cosFace]</strong></a>        <a target="_blank" rel="noopener" href="https://github.com/happynear/AMSoftmax">[<strong>AM-softmax code</strong>]</a></p>
<p>AM : Additive Margin Softmax for Face Verification</p>
<p>CosFace: Large Margin Cosine Loss for Deep Face Recognition(Tencent AI Lab)</p>
<p><strong>[16] ArcFace:</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1801.07698.pdf"><strong>[paper]</strong></a>  <a target="_blank" rel="noopener" href="https://github.com/deepinsight/insightface"><strong>[code]</strong></a></p>
<p>ArcFace: Additive Angular Margin Loss for Deep Face Recognition</p>
<p><img src="/2019/04/18/awesome-face/cos_loss.png" srcset="/img/loading.gif" alt="cos_loss"></p>
<h4 id="Face-Detection"><a href="#Face-Detection" class="headerlink" title="Face Detection"></a>Face Detection</h4><p><img src="/2019/04/18/awesome-face/face_detection.png" srcset="/img/loading.gif" alt></p>
<p><strong>[1] Cascade CNN</strong>  <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7299170/"><strong>[paper]</strong></a>   <a target="_blank" rel="noopener" href="https://github.com/anson0910/CNN_face_detection"><strong>[code]</strong></a>    </p>
<p>A Convolutional Neural Network Cascade for Face Detection</p>
<p><strong>[2] MTCNN</strong>   <a target="_blank" rel="noopener" href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/"><strong>[Paper]</strong></a>    <a target="_blank" rel="noopener" href="https://github.com/kpzhang93/MTCNN_face_detection_alignment"><strong>[code]</strong></a>  </p>
<p>Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</p>
<p><strong>[3] ICC - CNN</strong>  <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8237606"><strong>[paper]</strong></a></p>
<p>Detecting Faces Using Inside Cascaded Contextual CNN</p>
<p><strong>[4] Face R-CNN</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.01061.pdf"><strong>[Paper]</strong></a></p>
<p>Face R-CNN</p>
<p><strong>[5] Deep-IR</strong><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.08289"><strong>[Paper]</strong></a></p>
<p>Face Detection using Deep Learning: An Improved Faster RCNN Approach</p>
<p><strong>[6] SSH</strong>     <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.03979.pdf"><strong>[paper]</strong></a>    <a target="_blank" rel="noopener" href="https://github.com/mahyarnajibi/SSH"><strong>[code]</strong></a></p>
<p>SSH: Single Stage Headless Face Detector</p>
<p><strong>[7] S3FD</strong>   <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.05237"><strong>[paper]</strong></a></p>
<p>Single Shot Scale-invariant Face Detector</p>
<p><strong>[8] FaceBoxes</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.05234.pdf"><strong>[paper]</strong></a>     <a target="_blank" rel="noopener" href="https://github.com/sfzhang15/FaceBoxes"><strong>[code]</strong></a></p>
<p>Faceboxes: A CPU Real-time Face Detector with High Accuracy</p>
<p><strong>[9] Scaleface</strong>     <a target="_blank" rel="noopener" href="http://cn.arxiv.org/abs/1706.02863"><strong>[paper]</strong></a></p>
<p>Face Detection through Scale-Friendly Deep Convolutional Networks</p>
<p><strong>[10] HR</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.04402"><strong>[paper]</strong></a>  <a target="_blank" rel="noopener" href="https://github.com/peiyunh/tiny"><strong>[code]</strong></a></p>
<p>Finding Tiny Faces</p>
<p><strong>[11] FAN</strong>   <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1712.00721"><strong>[paper]</strong></a></p>
<p>Feature Agglomeration Networks for Single Stage Face Detection.</p>
<p><strong>[12] PyramidBox</strong>    <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.07737?context=cs"><strong>[paper]</strong></a> <a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/face_detection/README_cn.md"><strong>[code]</strong></a></p>
<p>PyramidBox: A Context-assisted Single Shot Face Detector</p>
<p><strong>[13] SRN</strong>     <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.02693"><strong>[paper]</strong></a> </p>
<p>Selective Refinement Network for High Performance Face Detection.</p>
<p><strong>[14] DSFD</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.10220"><strong>[paper]</strong></a></p>
<p>DSFD: Dual Shot Face Detector</p>
<p><strong>[15] VIM FD</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02350"><strong>[paper]</strong></a></p>
<p>Robust and High Performance Face Detector</p>
<p><strong>[16] ISRN</strong>  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.06651"><strong>[paper]</strong></a></p>
<p>Improved Selective Refinement Network for Face Detection</p>
<h4 id="Face-Alignment"><a href="#Face-Alignment" class="headerlink" title="Face Alignment"></a>Face Alignment</h4><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.10483">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</a>[Wayne Wu al., 2018]</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.10859.pdf">PFLD: A Practical Facial Landmark Detector</a>[Xiaojie Guo al., 2019]</li>
</ul>
<h2 id="‚öô-Open-source-lib"><a href="#‚öô-Open-source-lib" class="headerlink" title="‚öô Open source lib"></a>‚öô Open source lib</h2><h4 id="face-recognition"><a href="#face-recognition" class="headerlink" title="face recognition"></a>face recognition</h4><ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/ZhaoJ9014/face.evoLVe.PyTorch">face.evoLVe.</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/grib0ed0v/face_recognition.pytorch">face_recognition.pytorch</a></p>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/deepinsight/insightface">insightface</a></li>
</ul>
<h4 id="face-detection"><a href="#face-detection" class="headerlink" title="face detection"></a>face detection</h4><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ShiqiYu/libfacedetection">libfaccedetection</a></li>
</ul>
<h2 id="üì¶-Datasets"><a href="#üì¶-Datasets" class="headerlink" title="üì¶ Datasets"></a>üì¶ Datasets</h2><h4 id="2D-Face-Recognition-1"><a href="#2D-Face-Recognition-1" class="headerlink" title="2D Face Recognition"></a>2D Face Recognition</h4><div class="table-container">
<table>
<thead>
<tr>
<th>Datasets</th>
<th>Description</th>
<th>Links</th>
<th>Publish Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CASIA-WebFace</strong></td>
<td><strong>10,575</strong> subjects and <strong>494,414</strong> images</td>
<td><a target="_blank" rel="noopener" href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html">Download</a></td>
<td>2014</td>
</tr>
<tr>
<td><strong>MegaFace</strong>üèÖ</td>
<td><strong>1 million</strong> faces, <strong>690K</strong> identities</td>
<td><a target="_blank" rel="noopener" href="http://megaface.cs.washington.edu/">Download</a></td>
<td>2016</td>
</tr>
<tr>
<td><strong>MS-Celeb-1M</strong>üèÖ</td>
<td>about <strong>10M</strong> images for <strong>100K</strong> celebrities   Concrete measurement to evaluate the performance of recognizing one million celebrities</td>
<td><a target="_blank" rel="noopener" href="http://www.msceleb.org">Download</a></td>
<td>2016</td>
</tr>
<tr>
<td><strong>LFW</strong>üèÖ</td>
<td><strong>13,000</strong> images of faces collected from the web. Each face has been labeled with the name of the person pictured.  <strong>1680</strong> of the people pictured have two or more distinct photos in the data set.</td>
<td><a target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/lfw/">Download</a></td>
<td>2007</td>
</tr>
<tr>
<td><strong>VGG Face2</strong>üèÖ</td>
<td>The dataset contains <strong>3.31 million</strong> images of <strong>9131</strong> subjects (identities), with an average of 362.6 images for each subject.</td>
<td><a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/">Download</a></td>
<td>2017</td>
</tr>
<tr>
<td><strong>UMDFaces Dataset-image</strong></td>
<td><strong>367,888 face annotations</strong> for <strong>8,277 subjects.</strong></td>
<td><a target="_blank" rel="noopener" href="http://www.umdfaces.io">Download</a></td>
<td>2016</td>
</tr>
<tr>
<td><strong>Trillion Pairs</strong>üèÖ</td>
<td>Train: <strong>MS-Celeb-1M-v1c</strong> &amp;  <strong>Asian-Celeb</strong> Test: <strong>ELFW&amp;DELFW</strong></td>
<td><a target="_blank" rel="noopener" href="http://trillionpairs.deepglint.com/overview">Download</a></td>
<td>2018</td>
</tr>
<tr>
<td><strong>FaceScrub</strong></td>
<td>It comprises a total of <strong>106,863</strong> face images of male and female <strong>530</strong> celebrities, with about <strong>200 images per person</strong>.</td>
<td><a target="_blank" rel="noopener" href="http://vintage.winklerbros.net/facescrub.html">Download</a></td>
<td>2014</td>
</tr>
<tr>
<td><strong>Mut1ny</strong>üèÖ</td>
<td>head/face segmentation dataset contains over 17.3k labeled images</td>
<td><a target="_blank" rel="noopener" href="http://www.mut1ny.com/face-headsegmentation-dataset">Download</a></td>
<td>2018</td>
</tr>
<tr>
<td><strong>IMDB-Face</strong></td>
<td>The dataset contains about 1.7 million faces, 59k identities, which is manually cleaned from 2.0 million raw images.</td>
<td><a target="_blank" rel="noopener" href="https://github.com/fwang91/IMDb-Face">Download</a></td>
<td>2018</td>
</tr>
</tbody>
</table>
</div>
<h4 id="video-face-recognition"><a href="#video-face-recognition" class="headerlink" title="video face recognition"></a>video face recognition</h4><div class="table-container">
<table>
<thead>
<tr>
<th>Datasets</th>
<th>Description</th>
<th>Links</th>
<th>Publish Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>YouTube Face</strong>üèÖ</td>
<td>The data set contains <strong>3,425</strong> videos of <strong>1,595</strong> different people.</td>
<td><a target="_blank" rel="noopener" href="http://www.cs.tau.ac.il/%7Ewolf/ytfaces/">Download</a></td>
<td>2011</td>
</tr>
<tr>
<td><strong>UMDFaces Dataset-video</strong>üèÖ</td>
<td>Over <strong>3.7 million</strong> annotated video frames from over <strong>22,000</strong> videos of <strong>3100 subjects.</strong></td>
<td><a target="_blank" rel="noopener" href="http://www.umdfaces.io">Download</a></td>
<td>2017</td>
</tr>
<tr>
<td><strong>PaSC</strong></td>
<td>The challenge includes 9,376 still images and 2,802 videos of 293 people.</td>
<td><a target="_blank" rel="noopener" href="https://www.nist.gov/programs-projects/point-and-shoot-face-recognition-challenge-pasc">Download</a></td>
<td>2013</td>
</tr>
<tr>
<td><strong>YTC</strong></td>
<td>The data consists of two parts: video clips (1910 sequences of 47 subjects) and initialization data(initial frame face bounding boxes, manually marked).</td>
<td><a target="_blank" rel="noopener" href="http://seqamlab.com/youtube-celebrities-face-tracking-and-recognition-dataset/">Download</a></td>
<td>2008</td>
</tr>
<tr>
<td><strong>iQIYI-VID</strong>üèÖ</td>
<td>The iQIYI-VID dataset <strong>contains 500,000 videos clips of 5,000 celebrities, adding up to 1000 hours</strong>. This dataset supplies multi-modal cues, including face, cloth, voice, gait, and subtitles, for character identification.</td>
<td><a target="_blank" rel="noopener" href="http://challenge.ai.iqiyi.com/detail?raceId=5b1129e42a360316a898ff4f">Download</a></td>
<td>2018</td>
</tr>
</tbody>
</table>
</div>
<h4 id="3D-face-recognition"><a href="#3D-face-recognition" class="headerlink" title="3D face recognition"></a>3D face recognition</h4><div class="table-container">
<table>
<thead>
<tr>
<th>Datasets</th>
<th>Description</th>
<th>Links</th>
<th>Publish Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bosphorus</strong>üèÖ</td>
<td>105 subjects and 4666 faces 2D &amp; 3D face data</td>
<td><a target="_blank" rel="noopener" href="http://bosphorus.ee.boun.edu.tr/default.aspx">Download</a></td>
<td>2008</td>
</tr>
<tr>
<td><strong>BD-3DFE</strong></td>
<td>Analyzing <strong>Facial Expressions</strong> in <strong>3D</strong> Space</td>
<td><a target="_blank" rel="noopener" href="http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html">Download</a></td>
<td>2006</td>
</tr>
<tr>
<td><strong>ND-2006</strong></td>
<td>422 subjects and 9443 faces 3D Face Recognition</td>
<td><a target="_blank" rel="noopener" href="https://sites.google.com/a/nd.edu/public-cvrl/data-sets">Download</a></td>
<td>2006</td>
</tr>
<tr>
<td><strong>FRGC V2.0</strong></td>
<td>466 subjects and 4007 of 3D Face, Visible Face Images</td>
<td><a target="_blank" rel="noopener" href="https://sites.google.com/a/nd.edu/public-cvrl/data-sets">Download</a></td>
<td>2005</td>
</tr>
<tr>
<td><strong>B3D(AC)^2</strong></td>
<td><strong>1000</strong> high quality, dynamic <strong>3D scans</strong> of faces, recorded while pronouncing a set of English sentences.</td>
<td><a target="_blank" rel="noopener" href="http://www.vision.ee.ethz.ch/datasets/b3dac2.en.html">Download</a></td>
<td>2010</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Anti-spoofing"><a href="#Anti-spoofing" class="headerlink" title="Anti-spoofing"></a>Anti-spoofing</h4><div class="table-container">
<table>
<thead>
<tr>
<th>Datasets</th>
<th style="text-align:center"># of subj. / # of sess.</th>
<th>Links</th>
<th>Year</th>
<th>Spoof attacks attacks</th>
<th>Publish Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>NUAA</strong></td>
<td style="text-align:center">15/3</td>
<td><a target="_blank" rel="noopener" href="http://parnec.nuaa.edu.cn/xtan/data/nuaaimposterdb.html">Download</a></td>
<td>2010</td>
<td><strong>Print</strong></td>
<td>2010</td>
</tr>
<tr>
<td><strong>CASIA-MFSD</strong></td>
<td style="text-align:center">50/3</td>
<td>Download(link failed)</td>
<td>2012</td>
<td><strong>Print, Replay</strong></td>
<td>2012</td>
</tr>
<tr>
<td><strong>Replay-Attack</strong></td>
<td style="text-align:center">50/1</td>
<td><a target="_blank" rel="noopener" href="https://www.idiap.ch/dataset/replayattack">Download</a></td>
<td>2012</td>
<td><strong>Print, 2 Replay</strong></td>
<td>2012</td>
</tr>
<tr>
<td><strong>MSU-MFSD</strong></td>
<td style="text-align:center">35/1</td>
<td><a target="_blank" rel="noopener" href="https://www.cse.msu.edu/rgroups/biometrics/Publications/Databases/MSUMobileFaceSpoofing/index.htm">Download</a></td>
<td>2015</td>
<td><strong>Print, 2 Replay</strong></td>
<td>2015</td>
</tr>
<tr>
<td><strong>MSU-USSA</strong></td>
<td style="text-align:center">1140/1</td>
<td><a target="_blank" rel="noopener" href="http://biometrics.cse.msu.edu/Publications/Databases/MSU_USSA/">Download</a></td>
<td>2016</td>
<td><strong>2 Print, 6 Replay</strong></td>
<td>2016</td>
</tr>
<tr>
<td><strong>Oulu-NPU</strong></td>
<td style="text-align:center">55/3</td>
<td><a target="_blank" rel="noopener" href="https://sites.google.com/site/oulunpudatabase/">Download</a></td>
<td>2017</td>
<td><strong>2 Print, 6 Replay</strong></td>
<td>2017</td>
</tr>
<tr>
<td><strong>Siw</strong></td>
<td style="text-align:center">165/4</td>
<td><a target="_blank" rel="noopener" href="http://cvlab.cse.msu.edu/spoof-in-the-wild-siw-face-anti-spoofing-database.html">Download</a></td>
<td>2018</td>
<td><strong>2 Print, 4 Replay</strong></td>
<td>2018</td>
</tr>
</tbody>
</table>
</div>
<h4 id="cross-age-and-cross-pose"><a href="#cross-age-and-cross-pose" class="headerlink" title="cross age and cross pose"></a>cross age and cross pose</h4><div class="table-container">
<table>
<thead>
<tr>
<th>Datasets</th>
<th style="text-align:left">Description</th>
<th>Links</th>
<th>Publish Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CACD2000</strong></td>
<td style="text-align:left">The dataset contains more than 160,000 images of 2,000 celebrities with <strong>age ranging from 16 to 62</strong>.</td>
<td><a target="_blank" rel="noopener" href="http://bcsiriuschen.github.io/CARC/">Download</a></td>
<td>2014</td>
</tr>
<tr>
<td><strong>FGNet</strong></td>
<td style="text-align:left">The dataset contains more than 1002 images of 82 people with <strong>age ranging from 0 to 69</strong>.</td>
<td><a target="_blank" rel="noopener" href="http://www-prima.inrialpes.fr/FGnet/html/benchmarks.html">Download</a></td>
<td>2000</td>
</tr>
<tr>
<td><strong>MPRPH</strong></td>
<td style="text-align:left">The MORPH database contains <strong>55,000</strong> images of more than <strong>13,000</strong> people within the age ranges of <strong>16</strong> to <strong>77</strong></td>
<td><a target="_blank" rel="noopener" href="http://www.faceaginggroup.com/morph/">Download</a></td>
<td>2016</td>
</tr>
<tr>
<td><strong>CPLFW</strong></td>
<td style="text-align:left">we construct a Cross-Pose LFW (CPLFW) which deliberately searches and selects <strong>3,000 positive face pairs</strong> with <strong>pose difference</strong> to add pose variation to intra-class variance.</td>
<td><a target="_blank" rel="noopener" href="http://www.whdeng.cn/cplfw/index.html">Download</a></td>
<td>2017</td>
</tr>
<tr>
<td><strong>CALFW</strong></td>
<td style="text-align:left">Thereby we construct a Cross-Age LFW (CALFW) which deliberately searches and selects <strong>3,000 positive face pairs</strong> with <strong>age gaps</strong> to add aging process intra-class variance.</td>
<td><a target="_blank" rel="noopener" href="http://www.whdeng.cn/calfw/index.html">Download</a></td>
<td>2017</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Face-Detection-1"><a href="#Face-Detection-1" class="headerlink" title="Face Detection"></a>Face Detection</h4><div class="table-container">
<table>
<thead>
<tr>
<th>Datasets</th>
<th>Description</th>
<th>Links</th>
<th>Publish Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>FDDB</strong>üèÖ</td>
<td><strong>5171</strong> faces in a set of <strong>2845</strong> images</td>
<td><a target="_blank" rel="noopener" href="http://vis-www.cs.umass.edu/fddb/index.html">Download</a></td>
<td>2010</td>
</tr>
<tr>
<td><strong>Wider-face</strong> üèÖ</td>
<td><strong>32,203</strong> images and label <strong>393,703</strong> faces with a high degree of variability in scale, pose and occlusion, organized based on <strong>61</strong> event classes</td>
<td><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/">Download</a></td>
<td>2015</td>
</tr>
<tr>
<td><strong>AFW</strong></td>
<td>AFW dataset is built using Flickr images. It has <strong>205</strong> images with <strong>473</strong> labeled faces. For each face, annotations include a rectangular <strong>bounding box</strong>, <strong>6 landmarks</strong> and the <strong>pose angles</strong>.</td>
<td><a target="_blank" rel="noopener" href="http://www.ics.uci.edu/~xzhu/face/">Download</a></td>
<td>2013</td>
</tr>
<tr>
<td><strong>MALF</strong></td>
<td>MALF is the first face detection dataset that supports fine-gained evaluation. MALF consists of <strong>5,250</strong> images and <strong>11,931</strong> faces.</td>
<td><a target="_blank" rel="noopener" href="http://www.cbsr.ia.ac.cn/faceevaluation/">Download</a></td>
<td>2015</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Face-Attributes"><a href="#Face-Attributes" class="headerlink" title="Face Attributes"></a>Face Attributes</h4><div class="table-container">
<table>
<thead>
<tr>
<th>Datasets</th>
<th>Description</th>
<th>Links</th>
<th>Key features</th>
<th>Publish Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CelebA</strong></td>
<td><strong>10,177</strong> number of <strong>identities</strong>,  <strong>202,599</strong> number of <strong>face images</strong>, and  <strong>5 landmark locations</strong>, <strong>40 binary attributes</strong> annotations per image.</td>
<td><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">Download</a></td>
<td><strong>attribute &amp; landmark</strong></td>
<td>2015</td>
</tr>
<tr>
<td><strong>IMDB-WIKI</strong></td>
<td>500k+ face images with <strong>age</strong> and <strong>gender</strong> labels</td>
<td><a target="_blank" rel="noopener" href="https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/">Download</a></td>
<td><strong>age &amp; gender</strong></td>
<td>2015</td>
</tr>
<tr>
<td><strong>Adience</strong></td>
<td>Unfiltered faces for <strong>gender</strong> and <strong>age</strong> classification</td>
<td><a target="_blank" rel="noopener" href="http://www.openu.ac.il/home/hassner/Adience/data.html">Download</a></td>
<td><strong>age &amp; gender</strong></td>
<td>2014</td>
</tr>
<tr>
<td><strong>WFLW</strong>üèÖ</td>
<td>WFLW contains <strong>10000 faces</strong> (7500 for training and 2500 for testing) with <strong>98 fully manual annotated landmarks</strong>.</td>
<td><a target="_blank" rel="noopener" href="https://wywu.github.io/projects/LAB/WFLW.html">Download</a></td>
<td><strong>landmarks</strong></td>
<td>2018</td>
</tr>
<tr>
<td><strong>Caltech10k Web Faces</strong></td>
<td>The dataset has 10,524 human faces of various resolutions and in <strong>different settings</strong></td>
<td><a target="_blank" rel="noopener" href="http://www.vision.caltech.edu/Image_Datasets/Caltech_10K_WebFaces/#Description">Download</a></td>
<td><strong>landmarks</strong></td>
<td>2005</td>
</tr>
<tr>
<td><strong>EmotioNet</strong></td>
<td>The EmotioNet database includes<strong>950,000 images</strong> with <strong>annotated AUs</strong>.  A <strong>subset</strong> of the images in the EmotioNet database correspond to <strong>basic and compound emotions.</strong></td>
<td><a target="_blank" rel="noopener" href="http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/index.html#overview">Download</a></td>
<td><strong>AU and Emotion</strong></td>
<td>2017</td>
</tr>
<tr>
<td><strong>RAF( Real-world Affective Faces)</strong></td>
<td><strong>29672</strong> number of <strong>real-world images</strong>,  including <strong>7</strong> classes of basic emotions and <strong>12</strong> classes of compound emotions,  <strong>5 accurate landmark locations</strong>,  <strong>37 automatic landmark locations</strong>, <strong>race, age range</strong> and  <strong>gender</strong> <strong>attributes</strong> annotations per image</td>
<td><a target="_blank" rel="noopener" href="http://www.whdeng.cn/RAF/model1.html">Download</a></td>
<td><strong>Emotions„ÄÅlandmark„ÄÅrace„ÄÅage and gender</strong></td>
<td>2017</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h4><div class="table-container">
<table>
<thead>
<tr>
<th>Datasets</th>
<th>Description</th>
<th>Links</th>
<th>Publish Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>IJB C/B/A</strong>üèÖ</td>
<td>IJB C/B/A is currently running <strong>three challenges</strong> related to  <strong>face detection, verification, identification, and identity clustering.</strong></td>
<td><a target="_blank" rel="noopener" href="https://www.nist.gov/programs-projects/face-challenges">Download</a></td>
<td>2015</td>
</tr>
<tr>
<td><strong>MOBIO</strong></td>
<td><strong>bi-modal</strong> (<strong>audio</strong> and <strong>video</strong>) data taken from 152 people.</td>
<td><a target="_blank" rel="noopener" href="https://www.idiap.ch/dataset/mobio">Download</a></td>
<td>2012</td>
</tr>
<tr>
<td><strong>BANCA</strong></td>
<td>The BANCA database was captured in four European languages in <strong>two modalities</strong> (<strong>face</strong> and <strong>voice</strong>).</td>
<td><a target="_blank" rel="noopener" href="http://www.ee.surrey.ac.uk/CVSSP/banca/">Download</a></td>
<td>2014</td>
</tr>
<tr>
<td><strong>3D Mask Attack</strong></td>
<td><strong>76500</strong> frames of <strong>17</strong> persons using Kinect RGBD with eye positions (Sebastien Marcel).</td>
<td><a target="_blank" rel="noopener" href="https://www.idiap.ch/dataset/3dmad">Download</a></td>
<td>2013</td>
</tr>
<tr>
<td><strong>WebCaricature</strong></td>
<td><strong>6042</strong> <strong>caricatures</strong> and <strong>5974 photographs</strong> from <strong>252 persons</strong> collected from the web</td>
<td><a target="_blank" rel="noopener" href="https://cs.nju.edu.cn/rl/WebCaricature.htm">Download</a></td>
<td>2018</td>
</tr>
</tbody>
</table>
</div>
<h2 id="üè†-Research-home-conf-amp-workshop-amp-trans"><a href="#üè†-Research-home-conf-amp-workshop-amp-trans" class="headerlink" title="üè† Research home(conf &amp; workshop &amp; trans)"></a>üè† Research home(conf &amp; workshop &amp; trans)</h2><p><img src="/2019/04/18/awesome-face/research_home.png" srcset="/img/loading.gif" alt></p>
<p><strong>ICCV</strong>: <a target="_blank" rel="noopener" href="http://iccv2019.thecvf.com">IEEE International Conference on Computer Vision</a></p>
<p><strong>CVPR</strong>: <a target="_blank" rel="noopener" href="http://cvpr2018.thecvf.com/">IEEE Conference on Computer Vision and Pattern Recognition</a></p>
<p><strong>ECCV</strong>: <a target="_blank" rel="noopener" href="https://eccv2018.org">European Conference on Computer Vision</a></p>
<p><strong>FG</strong>: <a target="_blank" rel="noopener" href="http://dblp.uni-trier.de/db/conf/fgr/">IEEE International Conference on Automatic Face and Gesture Recognition</a></p>
<p><strong>BMVC:</strong> <a target="_blank" rel="noopener" href="http://www.bmva.org/bmvc/?id=bmvc">The British Machine Vision Conference</a></p>
<p><strong>IJCB[ICB+BTAS]</strong>:International Joint Conference on Biometrics</p>
<ul>
<li><p><strong>ICB</strong>: <a target="_blank" rel="noopener" href="http://icb2018.org">International Conference on Biometrics</a></p>
</li>
<li><p><strong>BTAS</strong>: <a target="_blank" rel="noopener" href="https://www.isi.edu/events/btas2018/home">IEEE International Conference on Biometrics: Theory, Applications and Systems</a></p>
</li>
</ul>
<p><strong>AMFG</strong>: IEEE workshop on Analysis and Modeling of Faces and Gestures</p>
<p><strong>CVPR Workshop on Biometrics</strong></p>
<p><strong>TPAMI:</strong> <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE Transactions on Pattern Analysis and Machine Intelligence</a></p>
<p><strong>IJCV:</strong> <a target="_blank" rel="noopener" href="https://link.springer.com/journal/11263">International Journal of Computer Vision</a> </p>
<p><strong>TIP:</strong> <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing</a></p>
<p><strong>TIFS:</strong> <a href="IEEE Transactions on Information Forensics and Security">IEEE Transactions on Information Forensics and Security</a></p>
<p><strong>PR:</strong> <a target="_blank" rel="noopener" href="https://www.journals.elsevier.com/pattern-recognition/">Pattern Recognition</a></p>
<h2 id="üè∑-References"><a href="#üè∑-References" class="headerlink" title="üè∑ References:"></a>üè∑ References:</h2><p>[1] <a target="_blank" rel="noopener" href="https://github.com/RiweiChen/DeepFace/tree/master/FaceDataset">https://github.com/RiweiChen/DeepFace/tree/master/FaceDataset</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/33505655?sort=created">https://www.zhihu.com/question/33505655?sort=created</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://github.com/betars/Face-Resources">https://github.com/betars/Face-Resources</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33288325">https://zhuanlan.zhihu.com/p/33288325</a></p>
<p>[5] <a target="_blank" rel="noopener" href="https://github.com/L706077/DNN-Face-Recognition-Papers">https://github.com/L706077/DNN-Face-Recognition-Papers</a></p>
<p>[6] <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/67919300">https://www.zhihu.com/question/67919300</a></p>
<p>[7] <a target="_blank" rel="noopener" href="https://jackietseng.github.io/conference_call_for_paper/2018-2019-conferences.html">https://jackietseng.github.io/conference_call_for_paper/2018-2019-conferences.html</a></p>
<p>[8]<a target="_blank" rel="noopener" href="http://history.ccf.org.cn/sites/ccf/biaodan.jsp?contentId=2903940690839">http://history.ccf.org.cn/sites/ccf/biaodan.jsp?contentId=2903940690839</a></p>
<p>[9]<a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/WiderFace_Results.html">http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/WiderFace_Results.html</a></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%9F%BA%E6%9C%AC%E6%96%B9%E5%90%91/">Âü∫Êú¨ÊñπÂêë</a>
                    
                  </div>
                
                
              </div>
              
                <p class="note note-warning">Êú¨ÂçöÂÆ¢ÊâÄÊúâÊñáÁ´†Èô§ÁâπÂà´Â£∞ÊòéÂ§ñÔºåÂùáÈááÁî® <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 ÂçèËÆÆ</a> ÔºåËΩ¨ËΩΩËØ∑Ê≥®ÊòéÂá∫Â§ÑÔºÅ</p>
              
              
                <div class="post-prevnext row">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2019/04/20/GCN/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">GCN</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2019/04/17/pytorch%E5%B8%B8%E8%A7%81%E5%B7%A5%E5%85%B7%E7%AE%B1/">
                        <span class="hidden-mobile">pytorchÂ∏∏ËßÅÂ∑•ÂÖ∑ÁÆ±</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '#post-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "awesome-face&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script>

  
















</body>
</html>
