

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="zhichao zhao">
  <meta name="keywords" content="">
  <title>pytorch API - 假欢畅 又何妨 无人共享</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/tomorrow.min.css" />
    
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_pf9vaxs7x7b.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.1.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>假欢畅，又何妨，无人共享</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container page-header text-center fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2019-03-08 18:46" pubdate>
        March 8, 2019 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4.7k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      75
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto" id="post">
            <!-- SEO header -->
            <h1 style="display: none">pytorch API</h1>
            
            <div class="markdown-body" id="post-body">
              <p>Pytorch API 汇总整理</p>
<a id="more"></a>
<h3 id="1-import-torch"><a href="#1-import-torch" class="headerlink" title="1. import torch"></a>1. import torch</h3><p>import &amp; vision</p>
<pre><code class="hljs python"><span class="hljs-keyword">import</span> torch 
print(torch.__version__)</code></pre>
<h3 id="2-Tensor-type-🌟"><a href="#2-Tensor-type-🌟" class="headerlink" title="2. Tensor type 🌟"></a>2. Tensor type 🌟</h3><p>Pytorch 给出了 9 种 CPU Tensor 类型和 9 种 GPU Tensor 类型。Pytorch 中默认的数据类型是 torch.FloatTensor, 即 torch.Tensor 等同于 torch.FloatTensor。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Data type</th>
<th>dtype</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td>32-bit floating point</td>
<td>torch.float32 or torch.float</td>
<td>torch.FloatTensor</td>
<td>torch.cuda.FloatTensor</td>
</tr>
<tr>
<td>64-bit floating point</td>
<td>torch.float64 or torch.double</td>
<td>torch.DoubleTensor</td>
<td>torch.cuda.DoubleTensor</td>
</tr>
<tr>
<td>16-bit floating point</td>
<td>torch.float16 or torch.half</td>
<td>torch.HalfTensor</td>
<td>torch.cuda.HalfTensor</td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td>torch.uint8</td>
<td>torch.ByteTensor</td>
<td>torch.cuda.ByteTensor</td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td>torch.int8</td>
<td>torch.CharTensor</td>
<td>torch.cuda.CharTensor</td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td>torch.int16 or torch.short</td>
<td>torch.ShortTensor</td>
<td>torch.cuda.ShortTensor</td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td>torch.int32 or torch.int</td>
<td>torch.IntTensor</td>
<td>torch.cuda.IntTensor</td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td>torch.int64 or torch.long</td>
<td>torch.LongTensor</td>
<td>torch.cuda.LongTensor</td>
</tr>
<tr>
<td>Boolean</td>
<td>torch.bool</td>
<td>torch.BoolTensor</td>
<td>torch.cuda.BoolTensor</td>
</tr>
</tbody>
</table>
</div>
<h5 id="设置默认Tensor-类型"><a href="#设置默认Tensor-类型" class="headerlink" title="设置默认Tensor 类型"></a>设置默认Tensor 类型</h5><p>Pytorch 可以通过 <code>set_default_tensor_type</code> 函数<strong>设置默认使用的Tensor类型</strong>， 在局部使用完后如果需要其他类型，则还需要重新设置会所需的类型 </p>
<pre><code class="hljs elm"><span class="hljs-title">torch</span>.set_default_tensor_<span class="hljs-keyword">type</span>(&#x27;torch.<span class="hljs-type">DoubleTensor</span>&#x27;)</code></pre>
<h5 id="CPU-GPU-互转"><a href="#CPU-GPU-互转" class="headerlink" title="CPU/GPU 互转"></a>CPU/GPU 互转</h5><p>CPU Tensor 和 GPU Tensor 的区别在于， 前者存储在内存中，而后者存储在显存中。两者之间的转换可以通过 <code>.cpu()</code>、<code>.cuda()</code>和 <code>.to(device)</code> 来完成  </p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)  
<span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.rand(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>a = a.cuda() <span class="hljs-comment"># CPU -&gt; GPU</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a.type()
<span class="hljs-string">&#x27;torch.cuda.FloatTensor&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a = a.cpu() <span class="hljs-comment"># GPU -&gt; CPU</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a.type()
<span class="hljs-string">&#x27;torch.FloatTensor&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a = a.to(device) <span class="hljs-comment"># to device</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a.type()
<span class="hljs-string">&#x27;torch.cuda.FloatTensor&#x27;</span></code></pre>
<h5 id="判定-Tensor-类型的几种方式"><a href="#判定-Tensor-类型的几种方式" class="headerlink" title="判定 Tensor 类型的几种方式:"></a>判定 Tensor 类型的几种方式:</h5><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a
tensor([[<span class="hljs-number">0.6065</span>, <span class="hljs-number">0.0122</span>, <span class="hljs-number">0.4473</span>],
        [<span class="hljs-number">0.5937</span>, <span class="hljs-number">0.5530</span>, <span class="hljs-number">0.4663</span>]], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>a.is_cuda  <span class="hljs-comment"># 可以显示是否在显存中</span>
<span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a.dtype   <span class="hljs-comment"># Tensor 内部data的类型</span>
torch.float32
<span class="hljs-meta">&gt;&gt;&gt; </span>a.type()
<span class="hljs-string">&#x27;torch.cuda.FloatTensor&#x27;</span>  <span class="hljs-comment"># 可以直接显示 Tensor 类型 = is_cuda + dtype</span></code></pre>
<h5 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h5><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a
tensor([[<span class="hljs-number">0.6065</span>, <span class="hljs-number">0.0122</span>, <span class="hljs-number">0.4473</span>],
        [<span class="hljs-number">0.5937</span>, <span class="hljs-number">0.5530</span>, <span class="hljs-number">0.4663</span>]], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>a.type(torch.DoubleTensor)   <span class="hljs-comment"># 使用 type() 函数进行转换</span>
tensor([[<span class="hljs-number">0.6065</span>, <span class="hljs-number">0.0122</span>, <span class="hljs-number">0.4473</span>],
        [<span class="hljs-number">0.5937</span>, <span class="hljs-number">0.5530</span>, <span class="hljs-number">0.4663</span>]], dtype=torch.float64)
<span class="hljs-meta">&gt;&gt;&gt; </span>a = a.double()  <span class="hljs-comment"># 直接使用 int()、long() 、float() 、和 double() 等直接进行数据类型转换进行</span>
tensor([[<span class="hljs-number">0.6065</span>, <span class="hljs-number">0.0122</span>, <span class="hljs-number">0.4473</span>],
        [<span class="hljs-number">0.5937</span>, <span class="hljs-number">0.5530</span>, <span class="hljs-number">0.4663</span>]], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>, dtype=torch.float64)
<span class="hljs-meta">&gt;&gt;&gt; </span>b = torch.randn(<span class="hljs-number">4</span>,<span class="hljs-number">5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>b.type_as(a)  <span class="hljs-comment"># 使用 type_as 函数, 并不需要明确具体是哪种类型</span>
tensor([[ <span class="hljs-number">0.2129</span>,  <span class="hljs-number">0.1877</span>, <span class="hljs-number">-0.0626</span>,  <span class="hljs-number">0.4607</span>, <span class="hljs-number">-1.0375</span>],
        [ <span class="hljs-number">0.7222</span>, <span class="hljs-number">-0.3502</span>,  <span class="hljs-number">0.1288</span>,  <span class="hljs-number">0.6786</span>,  <span class="hljs-number">0.5062</span>],
        [<span class="hljs-number">-0.4956</span>, <span class="hljs-number">-0.0793</span>,  <span class="hljs-number">0.7590</span>, <span class="hljs-number">-1.0932</span>, <span class="hljs-number">-0.1084</span>],
        [<span class="hljs-number">-2.2198</span>,  <span class="hljs-number">0.3827</span>,  <span class="hljs-number">0.2735</span>,  <span class="hljs-number">0.5642</span>,  <span class="hljs-number">0.6771</span>]], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>,
       dtype=torch.float64)</code></pre>
<h5 id="numpy-array-与-torch-Tensor-互转"><a href="#numpy-array-与-torch-Tensor-互转" class="headerlink" title="numpy array 与　torch Tensor　互转"></a>numpy array 与　torch Tensor　互转</h5><pre><code class="hljs python">torch.Tensor 与 np.ndarray 转换
<span class="hljs-comment"># torch.Tensor -&gt; np.ndarray.</span>
ndarray = tensor.cpu().numpy()

<span class="hljs-comment"># np.ndarray -&gt; torch.Tensor.</span>
tensor = torch.from_numpy(ndarray).float()
tensor = torch.from_numpy(ndarray.copy()).float()  <span class="hljs-comment"># If ndarray has negative stride</span></code></pre>
<h5 id="Tensor-相关信息获取"><a href="#Tensor-相关信息获取" class="headerlink" title="Tensor 相关信息获取"></a>Tensor 相关信息获取</h5><pre><code class="hljs python">t.size()/t、.shape   <span class="hljs-comment"># 两者等价， 返回 t 的形状, 可以使用 t.size()[1] 或 t.size(1) 查看列数</span>
t.numel() / t.nelement()  <span class="hljs-comment"># 两者等价, 返回 tensor 中元素总个数</span>
t.item()  <span class="hljs-comment"># 取出单个 tensor 的值</span>
t.dim()  <span class="hljs-comment"># 维度</span></code></pre>
<h3 id="3-Tensor-Create"><a href="#3-Tensor-Create" class="headerlink" title="3. Tensor Create"></a>3. Tensor Create</h3><h5 id="最基本的Tensor创建方式"><a href="#最基本的Tensor创建方式" class="headerlink" title="最基本的Tensor创建方式"></a>最基本的Tensor创建方式</h5><pre><code class="hljs python">troch.Tensor(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># 会使用默认的类型创建 Tensor, </span>
                   <span class="hljs-comment"># 可以通过 torch.set_default_tensor_type(&#x27;torch.DoubleTensor&#x27;) 进行修改</span>
torch.DoubleTensor(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># 指定类型创建 Tensor</span>

torch.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])  <span class="hljs-comment"># 通过 list 创建 Tensor</span>
                                <span class="hljs-comment"># 将 Tensor转换为list可以使用: t.tolist()</span>
torch.from_numpy(np.array([<span class="hljs-number">2</span>, <span class="hljs-number">3.3</span>]) ) <span class="hljs-comment"># 通过 numpy array 创建 tensor</span></code></pre>
<h5 id="确定初始值的方式创建"><a href="#确定初始值的方式创建" class="headerlink" title="确定初始值的方式创建"></a>确定初始值的方式创建</h5><pre><code class="hljs python">torch.ones(sizes)  <span class="hljs-comment"># 全 1 Tensor     </span>
torch.zeros(sizes)  <span class="hljs-comment"># 全 0 Tensor</span>
torch.eye(sizes)  <span class="hljs-comment"># 对角线为1，不要求行列一致</span>
torch.full(sizes, value) <span class="hljs-comment"># 指定 value</span></code></pre>
<h5 id="分布"><a href="#分布" class="headerlink" title="分布"></a>分布</h5><pre><code class="hljs python">torch.rand(sizes)  <span class="hljs-comment"># 均匀分布   </span>
torch.randn(sizes)   <span class="hljs-comment"># 标准分布</span>
<span class="hljs-comment"># 正态分布: 返回一个张量，包含从给定参数 means, std 的离散正态分布中抽取随机数。 </span>
<span class="hljs-comment"># 均值 means 是一个张量，包含每个输出元素相关的正态分布的均值 -&gt; 以此张量的均值作为均值</span>
<span class="hljs-comment"># 标准差 std 是一个张量，包含每个输出元素相关的正态分布的标准差 -&gt; 以此张量的标准差作为标准差。 </span>
<span class="hljs-comment"># 均值和标准差的形状不须匹配，但每个张量的元素个数须相同</span>
torch.normal(mean=torch.arange(<span class="hljs-number">1.</span>, <span class="hljs-number">11.</span>), std=torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">-0.1</span>))
tensor([<span class="hljs-number">-0.1987</span>,  <span class="hljs-number">3.1957</span>,  <span class="hljs-number">3.5459</span>,  <span class="hljs-number">2.8150</span>,  <span class="hljs-number">5.5398</span>,  <span class="hljs-number">5.6116</span>,  <span class="hljs-number">7.5512</span>,  <span class="hljs-number">7.8650</span>,
         <span class="hljs-number">9.3151</span>, <span class="hljs-number">10.1827</span>])
torch.uniform(<span class="hljs-keyword">from</span>,to) <span class="hljs-comment"># 均匀分布 </span>

torch.arange(s, e, steps)  <span class="hljs-comment"># 从 s 到 e，步长为 step</span>
torch.linspace(s, e, num)   <span class="hljs-comment"># 从 s 到 e, 均匀切分为 num 份</span>
<span class="hljs-comment"># ! 注意linespace和arange的区别，前者的最后一个参数是生成的Tensor中元素的数量，而后者的最后一个参数是步长。</span>
torch.randperm(m) <span class="hljs-comment"># 0 到 m-1 的随机序列</span>
<span class="hljs-comment"># ! shuffle 操作</span>
tensor[torch.randperm(tensor.size(<span class="hljs-number">0</span>))]</code></pre>
<h5 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h5><p>Pytorch 有几种不同的复制方式，注意区分</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Operation</th>
<th>New/Shared memory</th>
<th>Still in computation graph</th>
</tr>
</thead>
<tbody>
<tr>
<td>tensor.clone()</td>
<td>New</td>
<td>Yes</td>
</tr>
<tr>
<td>tensor.detach()</td>
<td>Shared</td>
<td>No</td>
</tr>
<tr>
<td>tensor.detach.clone()</td>
<td>New</td>
<td>No</td>
</tr>
</tbody>
</table>
</div>
<h3 id="4-索引、比较、排序"><a href="#4-索引、比较、排序" class="headerlink" title="4. 索引、比较、排序"></a>4. 索引、比较、排序</h3><h5 id="索引操作"><a href="#索引操作" class="headerlink" title="索引操作"></a>索引操作</h5><pre><code class="hljs python">a.item() <span class="hljs-comment">#　从只包含一个元素的张量中提取值</span>

a[row, column]   <span class="hljs-comment"># row 行， cloumn 列</span>
a[index]   <span class="hljs-comment"># 第index 行</span>
a[:,index]   <span class="hljs-comment"># 第 index 列</span>

a[<span class="hljs-number">0</span>, <span class="hljs-number">-1</span>]  <span class="hljs-comment"># 第零行， 最后一个元素</span>
a[:index]  <span class="hljs-comment"># 前 index 行</span>
a[:row, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>]  <span class="hljs-comment"># 前 row 行， 0和1列</span>

a[a&gt;<span class="hljs-number">1</span>]  <span class="hljs-comment"># 选择 a &gt; 1的元素， 等价于 a.masked_select(a&gt;1)</span>
torch.nonzero(a) <span class="hljs-comment"># 选择非零元素的坐标，并返回</span>
a.clamp(x, y)  <span class="hljs-comment"># 对 Tensor 元素进行限制， 小于x用x代替， 大于y用y代替</span>
torch.where(condition, x, y)  <span class="hljs-comment"># 满足condition 的位置输出x， 否则输出y</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a
tensor([[ <span class="hljs-number">6.</span>, <span class="hljs-number">-2.</span>],
        [ <span class="hljs-number">8.</span>,  <span class="hljs-number">0.</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.where(a &gt; <span class="hljs-number">1</span>, torch.full_like(a, <span class="hljs-number">1</span>), a)  <span class="hljs-comment"># 大于1 的部分直接用1代替， 其他保留原值</span>
tensor([[ <span class="hljs-number">1.</span>, <span class="hljs-number">-2.</span>],
        [ <span class="hljs-number">1.</span>,  <span class="hljs-number">0.</span>]])

<span class="hljs-comment">#　得到非零元素</span>
torch.nonzero(tensor)               <span class="hljs-comment"># 非零元素的索引</span>
torch.nonzero(tensor == <span class="hljs-number">0</span>)          <span class="hljs-comment"># 零元素的索引</span>
torch.nonzero(tensor).size(<span class="hljs-number">0</span>)       <span class="hljs-comment"># 非零元素的个数</span>
torch.nonzero(tensor == <span class="hljs-number">0</span>).size(<span class="hljs-number">0</span>)  <span class="hljs-comment"># 零元素的个数</span></code></pre>
<h5 id="比较操作"><a href="#比较操作" class="headerlink" title="比较操作"></a>比较操作</h5><pre><code class="hljs python">gt &gt;    lt &lt;     ge &gt;=     le &lt;=   eq ==    ne != 
topk(input, k) -&gt; (Tensor, LongTensor)
sort(input) -&gt; (Tensor, LongTensor)
max/min =&gt; max(tensor)      max(tensor, dim)    max(tensor1, tensor2)</code></pre>
<p>sort 函数接受两个参数, 其中 参数 0 为按照行排序、1为按照列排序: True 为降序， False 为升序， 返回值有两个， 第一个是排序结果， 第二个是排序序号</p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>a
tensor([[<span class="hljs-number">-1.8500</span>, <span class="hljs-number">-0.2005</span>,  <span class="hljs-number">1.4475</span>],
        [<span class="hljs-number">-1.7795</span>, <span class="hljs-number">-0.4968</span>, <span class="hljs-number">-1.8965</span>],
        [ <span class="hljs-number">0.5798</span>, <span class="hljs-number">-0.1554</span>,  <span class="hljs-number">1.6395</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>a.sort(<span class="hljs-number">0</span>, <span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>] 
tensor([[ <span class="hljs-number">0.5798</span>, <span class="hljs-number">-0.1554</span>,  <span class="hljs-number">1.6395</span>],
        [<span class="hljs-number">-1.7795</span>, <span class="hljs-number">-0.2005</span>,  <span class="hljs-number">1.4475</span>],
        [<span class="hljs-number">-1.8500</span>, <span class="hljs-number">-0.4968</span>, <span class="hljs-number">-1.8965</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>a.sort(<span class="hljs-number">0</span>, <span class="hljs-literal">True</span>)[<span class="hljs-number">1</span>]
tensor([[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>a.sort(<span class="hljs-number">1</span>, <span class="hljs-literal">True</span>)[<span class="hljs-number">1</span>]
tensor([[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>a.sort(<span class="hljs-number">1</span>, <span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
tensor([[ <span class="hljs-number">1.4475</span>, <span class="hljs-number">-0.2005</span>, <span class="hljs-number">-1.8500</span>],
        [<span class="hljs-number">-0.4968</span>, <span class="hljs-number">-1.7795</span>, <span class="hljs-number">-1.8965</span>],
        [ <span class="hljs-number">1.6395</span>,  <span class="hljs-number">0.5798</span>, <span class="hljs-number">-0.1554</span>]])</code></pre>
<h3 id="5-Element-wise-和-归并操作"><a href="#5-Element-wise-和-归并操作" class="headerlink" title="5. Element-wise 和 归并操作"></a>5. Element-wise 和 归并操作</h3><p>Element-wise：输出的 Tensor 形状与原始的形状一致</p>
<pre><code class="hljs python">abs / sqrt / div / exp / fmod / log / pow...
cos / sin / asin / atan2 / cosh...
ceil / round / floor / trunc
clamp(input, min, max)
sigmoid / tanh...</code></pre>
<p>归并操作：输出的 Tensor 形状小于原始的 Tensor形状</p>
<pre><code class="hljs python">mean/sum/median/mode   <span class="hljs-comment"># 均值/和/ 中位数/众数</span>
norm/dist  <span class="hljs-comment"># 范数/距离</span>
std/var  <span class="hljs-comment"># 标准差/方差</span>
cumsum/cumprd <span class="hljs-comment"># 累加/累乘</span></code></pre>
<h3 id="6-变形操作"><a href="#6-变形操作" class="headerlink" title="6. 变形操作"></a>6. 变形操作</h3><h5 id="view-resize-reshape-调整Tensor的形状"><a href="#view-resize-reshape-调整Tensor的形状" class="headerlink" title="view/resize/reshape  调整Tensor的形状"></a>view/resize/reshape  调整Tensor的形状</h5><ul>
<li>元素总数必须相同  </li>
<li>view 和 reshape 可以使用 -1 自动计算维度</li>
<li>共享内存</li>
</ul>
<p>!!!  <code>view()</code> 操作是需要 Tensor 在内存中连续的， 这种情况下需要使用 <code>contiguous()</code> 操作先将内存变为连续。 对于reshape 操作， 可以看做是 <code>Tensor.contiguous().view()</code>.   🌟</p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.Tensor(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>a
tensor([[<span class="hljs-number">6.0000e+00</span>, <span class="hljs-number">8.0000e+00</span>],
        [<span class="hljs-number">1.0000e+00</span>, <span class="hljs-number">1.8367e-40</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>a.resize(<span class="hljs-number">4</span>, <span class="hljs-number">1</span>)
tensor([[<span class="hljs-number">6.0000e+00</span>],
        [<span class="hljs-number">8.0000e+00</span>],
        [<span class="hljs-number">1.0000e+00</span>],
        [<span class="hljs-number">1.8367e-40</span>]])</code></pre>
<h5 id="transpose-permute-各维度之间的变换"><a href="#transpose-permute-各维度之间的变换" class="headerlink" title="transpose / permute  各维度之间的变换"></a>transpose / permute  各维度之间的变换</h5><p>transpose 可以将指定的两个维度的元素进行转置， permute 则可以按照指定的维度进行维度变换</p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>x
tensor([[[<span class="hljs-number">-0.9699</span>, <span class="hljs-number">-0.3375</span>, <span class="hljs-number">-0.0178</span>]],
        [[ <span class="hljs-number">1.4260</span>, <span class="hljs-number">-0.2305</span>, <span class="hljs-number">-0.2883</span>]]])

<span class="hljs-meta">&gt;&gt;&gt; </span>x.shape
torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>x.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># shape =&gt; torch.Size([1, 2, 3])</span>
tensor([[[<span class="hljs-number">-0.9699</span>, <span class="hljs-number">-0.3375</span>, <span class="hljs-number">-0.0178</span>],
         [ <span class="hljs-number">1.4260</span>, <span class="hljs-number">-0.2305</span>, <span class="hljs-number">-0.2883</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>x.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># shape =&gt; torch.Size([1, 2, 3])</span>
tensor([[[<span class="hljs-number">-0.9699</span>, <span class="hljs-number">-0.3375</span>, <span class="hljs-number">-0.0178</span>],
         [ <span class="hljs-number">1.4260</span>, <span class="hljs-number">-0.2305</span>, <span class="hljs-number">-0.2883</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span></code></pre>
<h5 id="squeeze-dim-unsquence-dim-🌟"><a href="#squeeze-dim-unsquence-dim-🌟" class="headerlink" title="squeeze(dim) / unsquence(dim)   🌟"></a>squeeze(dim) / unsquence(dim)   🌟</h5><p>处理 size 为 1 的维度， 前者用于去除 size 为 1 的维度， 而后者则是将指定的维度的size变为1</p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>a
tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]) <span class="hljs-comment"># shape =&gt; torch.Size([3])</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># shape =&gt; torch.Size([1, 3])</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a.unqueeze(<span class="hljs-number">0</span>).squeeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># shape =&gt; torch.Size([3])</span></code></pre>
<h5 id="expand-expand-as-repeat复制元素来扩展维度"><a href="#expand-expand-as-repeat复制元素来扩展维度" class="headerlink" title="expand / expand_as / repeat复制元素来扩展维度"></a>expand / expand_as / repeat复制元素来扩展维度</h5><p>有时需要采用复制的形式来扩展 Tensor 的维度， 这时可以使用 <code>expand</code>， <code>expand()</code> 函数将 size 为 1的维度复制扩展为指定大小， 也可以用 <code>expand_as()</code>函数指定为 示例 Tensor 的维度。</p>
<p>!! <code>expand</code> 扩大 tensor 不需要分配新内存，只是仅仅新建一个 tensor 的视图，其中通过将 stride 设为0，一维将会扩展位更高维。</p>
<p><code>repeat</code> 沿着指定的维度重复 tensor。 不同于 <code>expand()</code>，复制的是 tensor 中的数据。</p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>a
tensor([[[<span class="hljs-number">0.3094</span>],
         [<span class="hljs-number">0.4812</span>]],

        [[<span class="hljs-number">0.0950</span>],
         [<span class="hljs-number">0.8652</span>]]])
<span class="hljs-meta">&gt;&gt;&gt; </span>a.expand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>) <span class="hljs-comment"># 将第2维的维度由1变为3， 则复制该维的元素，并扩展为3</span>
tensor([[[<span class="hljs-number">0.3094</span>, <span class="hljs-number">0.3094</span>, <span class="hljs-number">0.3094</span>],
         [<span class="hljs-number">0.4812</span>, <span class="hljs-number">0.4812</span>, <span class="hljs-number">0.4812</span>]],

        [[<span class="hljs-number">0.0950</span>, <span class="hljs-number">0.0950</span>, <span class="hljs-number">0.0950</span>],
         [<span class="hljs-number">0.8652</span>, <span class="hljs-number">0.8652</span>, <span class="hljs-number">0.8652</span>]]])

<span class="hljs-meta">&gt;&gt;&gt; </span>a.repeat(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># 将第二位复制一次</span>
tensor([[[<span class="hljs-number">0.3094</span>],
         [<span class="hljs-number">0.4812</span>],
         [<span class="hljs-number">0.3094</span>],
         [<span class="hljs-number">0.4812</span>]],

        [[<span class="hljs-number">0.0950</span>],
         [<span class="hljs-number">0.8652</span>],
         [<span class="hljs-number">0.0950</span>],
         [<span class="hljs-number">0.8652</span>]]])</code></pre>
<h5 id="使用切片操作扩展多个维度-🌟"><a href="#使用切片操作扩展多个维度-🌟" class="headerlink" title="使用切片操作扩展多个维度 🌟"></a>使用切片操作扩展多个维度 🌟</h5><pre><code class="hljs fortran">b = a[:,<span class="hljs-keyword">None</span>, <span class="hljs-keyword">None</span>,:] # <span class="hljs-keyword">None</span> 处的维度为１</code></pre>
<h3 id="7-组合与分块"><a href="#7-组合与分块" class="headerlink" title="7. 组合与分块"></a>7. 组合与分块</h3><p><strong>组合操作</strong> 是将不同的 Tensor 叠加起来。 主要有 <code>cat()</code> 和 <code>torch.stack()</code> 两个函数，cat 即 concatenate 的意思， 是指沿着已有的数据的某一维度进行拼接， 操作后的数据的总维数不变， 在进行拼接时， 除了拼接的维度之外， 其他维度必须相同。 而<code>torch. stack()</code> 函数会新增一个维度， 并按照指定的维度进行叠加。</p>
<pre><code class="hljs shell">torch.cat(list_of_tensors, dim=0)　  # k 个 (m,n) -&gt; (k*m, n)
torch.stack(list_of_tensors, dim=0)   # k 个 (m,n) -&gt; (k*m*n)</code></pre>
<p><strong>分块操作</strong> 是指将 Tensor 分割成不同的子 Tensor，主要有 <code>torch.chunk()</code> 与 <code>torch.split()</code> 两个函数，前者需要指定分块的数量，而后者则需要指定每一块的大小，以整形或者list来表示。</p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.Tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>], [<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.chunk(a, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)
(tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>]]), tensor([[<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]]))
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.chunk(a, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)
(tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>],
        [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>]]), tensor([[<span class="hljs-number">3.</span>],
        [<span class="hljs-number">6.</span>]]))
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.split(a, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>)
(tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
        [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]]),)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.split(a, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], <span class="hljs-number">1</span>)
(tensor([[<span class="hljs-number">1.</span>],
        [<span class="hljs-number">4.</span>]]), tensor([[<span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
        [<span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]]))</code></pre>
<h3 id="8-聚合-与-分散"><a href="#8-聚合-与-分散" class="headerlink" title="8.  聚合 与 分散"></a>8.  聚合 与 分散</h3><pre><code class="hljs python">torch.gather(input, dim, index, out=<span class="hljs-literal">None</span>)  <span class="hljs-comment"># 根据 index 和 dim, 寻找 input 对应的索引位置, 得到 output</span>
Tensor.scatter_(dim, index, src)   <span class="hljs-comment"># 根据 dim 和 index, 将 src 指定位置上的值， 分配给 output 对应索引位置。</span></code></pre>
<h3 id="9-linear-algebra"><a href="#9-linear-algebra" class="headerlink" title="9. linear algebra"></a>9. linear algebra</h3><pre><code class="hljs python">trace  <span class="hljs-comment"># 对角线元素之和(矩阵的迹)</span>
diag  <span class="hljs-comment"># 对角线元素</span>
triu/tril  <span class="hljs-comment"># 矩阵的上三角/下三角</span>
addmm/addbmm/addmv/addr/badbmm...  <span class="hljs-comment"># 矩阵运算</span>
t <span class="hljs-comment"># 转置</span>
dor/cross <span class="hljs-comment"># 内积/外积</span>
inverse <span class="hljs-comment"># 矩阵求逆</span>
svd  <span class="hljs-comment"># 奇异值分解</span>

torch.mm(tensor1, tensor2)   <span class="hljs-comment"># 矩阵乘法  (m*n) * (n*p) -&gt; (m*p)</span>
torch.bmm(tensor1, tensor2) <span class="hljs-comment"># batch的矩阵乘法: (b*m*n) * (b*n*p) -&gt; (b*m*p).</span>
torch.mv(tensor, vec) <span class="hljs-comment">#　矩阵向量乘法 (m*n) * (n) = (m)</span>
tensor1 * tensor2 <span class="hljs-comment"># Element-wise multiplication.</span></code></pre>
<h3 id="10-基本机制"><a href="#10-基本机制" class="headerlink" title="10. 基本机制"></a>10. 基本机制</h3><h5 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h5><p>不同形状的 Tensor 进行计算时， 可以自动扩展到较大的相同形状再进行计算。 广播机制的前提是一个 Tensor  至少有一个维度，且从尾部遍历 Tensor 时，两者维度必须相等， 其中七个要么是1， 要么不存在</p>
<h5 id="向量化操作"><a href="#向量化操作" class="headerlink" title="向量化操作"></a>向量化操作</h5><p>可以在同一时间进行批量地并行计算，例如矩阵运算，以达到更高的计算效率的一种方式:</p>
<h5 id="共享内存机制"><a href="#共享内存机制" class="headerlink" title="共享内存机制"></a>共享内存机制</h5><p>(1) 直接通过 Tensor 来初始化另一个 Tensor， 或者通过 Tensor 的组合、分块、索引、变形来初始化另一个Tensor， 则这两个 Tensor 共享内存:</p>
<pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>b = a
<span class="hljs-meta">&gt;&gt;&gt; </span>c = a.view(<span class="hljs-number">6</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>b[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>] = <span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>c[<span class="hljs-number">3</span>] = <span class="hljs-number">4</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>a
tensor([[ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.3898</span>, <span class="hljs-number">-0.7641</span>],
        [ <span class="hljs-number">4.0000</span>,  <span class="hljs-number">0.6859</span>, <span class="hljs-number">-1.5179</span>]])</code></pre>
<p>(2) 对于一些操作通过加后缀  “_”  实现 inplace 操作， 如 <code>add_()</code> 和 <code>resize_()</code> 等， 这样操作只要被执行， 本身的 Tensor 就会被改变。</p>
<pre><code class="hljs angelscript">&gt;&gt;&gt; a
tensor([[ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.3898</span>, <span class="hljs-number">-0.7641</span>],
        [ <span class="hljs-number">4.0000</span>,  <span class="hljs-number">0.6859</span>, <span class="hljs-number">-1.5179</span>]])
&gt;&gt;&gt; a.add_(a)
tensor([[ <span class="hljs-number">0.0000</span>,  <span class="hljs-number">0.7796</span>, <span class="hljs-number">-1.5283</span>],
        [ <span class="hljs-number">8.0000</span>,  <span class="hljs-number">1.3719</span>, <span class="hljs-number">-3.0358</span>]])</code></pre>
<p>(3) Tensor与 Numpy 可以高效的完成转换， 并且转换前后的变量共享内存。在进行 Pytorch 不支持的操作的时候， 甚至可以曲线救国， 将 Tensor 转换为 Numpy 类型，操作后再转化为 Tensor</p>
<pre><code class="hljs clean"># tensor &lt;--&gt; numpy
b = a.numpy() # tensor -&gt; numpy
a = torch.from_numpy(a) # numpy -&gt; tensor</code></pre>
<p>!!! 需要注意的是，<code>torch.tensor()</code> 总是会进行数据拷贝，新 tensor 和原来的数据不再共享内存。所以如果你想共享内存的话，建议使用 <code>torch.from_numpy()</code> 或者 <code>tensor.detach()</code> 来新建一个 tensor, 二者共享内存。</p>
<h3 id="11-nn"><a href="#11-nn" class="headerlink" title="11. nn"></a>11. nn</h3><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F</code></pre>
<h5 id="pad-填充"><a href="#pad-填充" class="headerlink" title="pad 填充"></a>pad 填充</h5><pre><code class="hljs python">nn.ConstantPad2d(padding, value)</code></pre>
<h5 id="卷积和反卷积"><a href="#卷积和反卷积" class="headerlink" title="卷积和反卷积"></a>卷积和反卷积</h5><pre><code class="hljs python">nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, groups=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>)
nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, output_padding=<span class="hljs-number">0</span>, groups=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>, dilation=<span class="hljs-number">1</span>)</code></pre>
<pre><code class="hljs python"><span class="hljs-comment">#　最常用的两种卷积层设计 3x3 &amp; 1x1</span>
conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>)
conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, bias=<span class="hljs-literal">True</span>)</code></pre>
<h5 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h5><pre><code class="hljs python">nn.MaxPool2d(kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, dilation=<span class="hljs-number">1</span>, return_indices=<span class="hljs-literal">False</span>, ceil_mode=<span class="hljs-literal">False</span>)
nn.AvgPool2d(kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>, ceil_mode=<span class="hljs-literal">False</span>, count_include_pad=<span class="hljs-literal">True</span>)
nn.AdaptiveMaxPool2d(output_size, return_indices=<span class="hljs-literal">False</span>)
nn.AdaptiveAvgPool2d(output_size)  <span class="hljs-comment"># global avg pool: output_size=1</span>
nn.MaxUnpool2d(kernel_size, stride=<span class="hljs-literal">None</span>, padding=<span class="hljs-number">0</span>)</code></pre>
<h5 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h5><pre><code class="hljs python">nn.Linear(in_features, out_features, bias=<span class="hljs-literal">True</span>)</code></pre>
<h5 id="防止过拟合相关层"><a href="#防止过拟合相关层" class="headerlink" title="防止过拟合相关层"></a>防止过拟合相关层</h5><pre><code class="hljs python">nn.Dropout2d(p=<span class="hljs-number">0.5</span>, inplace=<span class="hljs-literal">False</span>)
nn.AlphaDropout(p=<span class="hljs-number">0.5</span>)
nn.BatchNorm2d(num_features, eps=<span class="hljs-number">1e-05</span>, momentum=<span class="hljs-number">0.1</span>, affine=<span class="hljs-literal">True</span>, track_running_stats=<span class="hljs-literal">True</span>)</code></pre>
<h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><pre><code class="hljs python">nn.Softplus(beta=<span class="hljs-number">1</span>, threshold=<span class="hljs-number">20</span>)
nn.Tanh()
nn.ReLU(inplace=<span class="hljs-literal">False</span>)    
nn.ReLU6(inplace=<span class="hljs-literal">False</span>)
nn.LeakyReLU(negative_slope=<span class="hljs-number">0.01</span>, inplace=<span class="hljs-literal">False</span>)
nn.PReLU(num_parameters=<span class="hljs-number">1</span>, init=<span class="hljs-number">0.25</span>)
nn.SELU(inplace=<span class="hljs-literal">False</span>)
nn.ELU(alpha=<span class="hljs-number">1.0</span>, inplace=<span class="hljs-literal">False</span>)</code></pre>
<h5 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h5><pre><code class="hljs python">nn.RNNCell(input_size, hidden_size, bias=<span class="hljs-literal">True</span>, nonlinearity=<span class="hljs-string">&#x27;tanh&#x27;</span>)
nn.RNN(*args, **kwargs)
nn.LSTMCell(input_size, hidden_size, bias=<span class="hljs-literal">True</span>)
nn.LSTM(*args, **kwargs)
nn.GRUCell(input_size, hidden_size, bias=<span class="hljs-literal">True</span>)
nn.GRU(*args, **kwargs)</code></pre>
<h5 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h5><pre><code class="hljs python">nn.Embedding(num_embeddings, embedding_dim, padding_idx=<span class="hljs-literal">None</span>, max_norm=<span class="hljs-literal">None</span>, norm_type=<span class="hljs-number">2</span>, scale_grad_by_freq=<span class="hljs-literal">False</span>, sparse=<span class="hljs-literal">False</span>, _weight=<span class="hljs-literal">None</span>)</code></pre>
<h5 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h5><pre><code class="hljs python">nn.Sequential(*args)</code></pre>
<h5 id="loss-functon"><a href="#loss-functon" class="headerlink" title="loss functon"></a>loss functon</h5><pre><code class="hljs python">nn.BCELoss(weight=<span class="hljs-literal">None</span>, size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)
nn.CrossEntropyLoss(weight=<span class="hljs-literal">None</span>, size_average=<span class="hljs-literal">True</span>, ignore_index=<span class="hljs-number">-100</span>, reduce=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># CrossEntropyLoss 等价于 log_softmax + NLLLoss</span>
nn.L1Loss(size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)
nn.KLDivLoss(size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)
nn.MSELoss(size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)
nn.NLLLoss(weight=<span class="hljs-literal">None</span>, size_average=<span class="hljs-literal">True</span>, ignore_index=<span class="hljs-number">-100</span>, reduce=<span class="hljs-literal">True</span>)
nn.NLLLoss2d(weight=<span class="hljs-literal">None</span>, size_average=<span class="hljs-literal">True</span>, ignore_index=<span class="hljs-number">-100</span>, reduce=<span class="hljs-literal">True</span>)
nn.SmoothL1Loss(size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)
nn.SoftMarginLoss(size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)
nn.TripletMarginLoss(margin=<span class="hljs-number">1.0</span>, p=<span class="hljs-number">2</span>, eps=<span class="hljs-number">1e-06</span>, swap=<span class="hljs-literal">False</span>, size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)
nn.CosineEmbeddingLoss(margin=<span class="hljs-number">0</span>, size_average=<span class="hljs-literal">True</span>, reduce=<span class="hljs-literal">True</span>)</code></pre>
<h5 id="functional-🌟"><a href="#functional-🌟" class="headerlink" title="functional    🌟"></a>functional    🌟</h5><pre><code class="hljs python">nn.functional <span class="hljs-comment"># nn中的大多数layer，在functional中都有一个与之相对应的函数。</span>
              <span class="hljs-comment"># nn.functional中的函数和nn.Module的主要区别在于，</span>
              <span class="hljs-comment"># 用nn.Module实现的layers是一个特殊的类，都是由 class layer(nn.Module)定义，</span>
              <span class="hljs-comment"># 会自动提取可学习的参数。而nn.functional中的函数更像是纯函数，</span>
              <span class="hljs-comment"># 由def function(input)定义。</span></code></pre>
<h5 id="init"><a href="#init" class="headerlink" title="init"></a>init</h5><pre><code class="hljs python">torch.nn.init.uniform
torch.nn.init.normal
torch.nn.init.kaiming_uniform
torch.nn.init.kaiming_normal
torch.nn.init.xavier_normal
torch.nn.init.xavier_uniform
torch.nn.init.sparse</code></pre>
<h5 id="net"><a href="#net" class="headerlink" title="net"></a>net</h5><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">net_name</span>(<span class="hljs-params">nn.Module</span>):</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span>
        super(net_name, self).__init__()
        self.layer_name = xxxx

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span> 
        x = self.layer_name(x)        
        <span class="hljs-keyword">return</span> x

net.parameters()   <span class="hljs-comment"># 获取参数 </span>
net.named_parameters  <span class="hljs-comment"># 获取参数及名称</span>
net.zero_grad()  <span class="hljs-comment"># 网络所有梯度清零, grad 在反向传播过程中是累加的(accumulated)，</span>
                 <span class="hljs-comment"># 这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以反向传播之前需把梯度清零。</span></code></pre>
<h3 id="12-optim-gt-form-torch-import-optim"><a href="#12-optim-gt-form-torch-import-optim" class="headerlink" title="12. optim -&gt; form torch import optim"></a>12. optim -&gt; form torch import optim</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim

optim.SGD(params, lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0</span>, dampening=<span class="hljs-number">0</span>, weight_decay=<span class="hljs-number">0</span>, nesterov=<span class="hljs-literal">False</span>)
optim.ASGD(params, lr=<span class="hljs-number">0.01</span>, lambd=<span class="hljs-number">0.0001</span>, alpha=<span class="hljs-number">0.75</span>, t0=<span class="hljs-number">1000000.0</span>, weight_decay=<span class="hljs-number">0</span>)
optim.LBFGS(params, lr=<span class="hljs-number">1</span>, max_iter=<span class="hljs-number">20</span>, max_eval=<span class="hljs-literal">None</span>, tolerance_grad=<span class="hljs-number">1e-05</span>, tolerance_change=<span class="hljs-number">1e-09</span>, history_size=<span class="hljs-number">100</span>, line_search_fn=<span class="hljs-literal">None</span>)
optim.RMSprop(params, lr=<span class="hljs-number">0.01</span>, alpha=<span class="hljs-number">0.99</span>, eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>, momentum=<span class="hljs-number">0</span>, centered=<span class="hljs-literal">False</span>)
optim.Rprop(params, lr=<span class="hljs-number">0.01</span>, etas=(<span class="hljs-number">0.5</span>, <span class="hljs-number">1.2</span>), step_sizes=(<span class="hljs-number">1e-06</span>, <span class="hljs-number">50</span>))
optim.Adadelta(params, lr=<span class="hljs-number">1.0</span>, rho=<span class="hljs-number">0.9</span>, eps=<span class="hljs-number">1e-06</span>, weight_decay=<span class="hljs-number">0</span>)
optim.Adagrad(params, lr=<span class="hljs-number">0.01</span>, lr_decay=<span class="hljs-number">0</span>, weight_decay=<span class="hljs-number">0</span>, initial_accumulator_value=<span class="hljs-number">0</span>)
optim.Adam(params, lr=<span class="hljs-number">0.001</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>, amsgrad=<span class="hljs-literal">False</span>)
optim.Adamax(params, lr=<span class="hljs-number">0.002</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>, weight_decay=<span class="hljs-number">0</span>)
optim.SparseAdam(params, lr=<span class="hljs-number">0.001</span>, betas=(<span class="hljs-number">0.9</span>, <span class="hljs-number">0.999</span>), eps=<span class="hljs-number">1e-08</span>)
optim.Optimizer(params, defaults)

optimizer.zero_grad()  <span class="hljs-comment"># 等价于 net.zero_grad() </span>
optimizer.step()</code></pre>
<h3 id="13-learning-rate"><a href="#13-learning-rate" class="headerlink" title="13.  learning rate"></a>13.  learning rate</h3><pre><code class="hljs python"><span class="hljs-comment"># Reduce learning rate when validation accuarcy plateau.</span>
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="hljs-string">&#x27;max&#x27;</span>, patience=<span class="hljs-number">5</span>, verbose=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># Cosine annealing learning rate.</span>
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=<span class="hljs-number">80</span>)
<span class="hljs-comment"># Reduce learning rate by 10 at given epochs.</span>
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class="hljs-number">50</span>, <span class="hljs-number">70</span>], gamma=<span class="hljs-number">0.1</span>)
<span class="hljs-comment"># Learning rate warmup by 10 epochs.</span>
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=<span class="hljs-keyword">lambda</span> t: t / <span class="hljs-number">10</span>)

<span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>):
    scheduler.step()
    train(...); val(...)</code></pre>
<h3 id="14-save-and-load-model"><a href="#14-save-and-load-model" class="headerlink" title="14. save and load model"></a>14. save and load model</h3><pre><code class="hljs python">torch.save(model.state_dict(), <span class="hljs-string">&#x27;xxxx_params.pth&#x27;</span>)
model.load_state_dict(t.load(<span class="hljs-string">&#x27;xxxx_params.pth&#x27;</span>))

torch.save(model, <span class="hljs-string">&#x27;xxxx.pth&#x27;</span>)
model.torch.load(<span class="hljs-string">&#x27;xxxx.pth&#x27;</span>)

all_data = dict(
    optimizer = optimizer.state_dict(),
    model = model.state_dict(),
    info = <span class="hljs-string">u&#x27;model and optim parameter&#x27;</span>
)

t.save(all_data, <span class="hljs-string">&#x27;xxx.pth&#x27;</span>)
all_data = t.load(<span class="hljs-string">&#x27;xxx.pth&#x27;</span>)
all_data.keys()</code></pre>
<h3 id="15-torchvision"><a href="#15-torchvision" class="headerlink" title="15. torchvision"></a>15. torchvision</h3><h5 id="models"><a href="#models" class="headerlink" title="models"></a>models</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models
resnet34 = models.resnet34(pretrained=<span class="hljs-literal">True</span>, num_classes=<span class="hljs-number">1000</span>)</code></pre>
<h5 id="data-augmentation"><a href="#data-augmentation" class="headerlink" title="data augmentation"></a>data augmentation</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms

<span class="hljs-comment"># transforms.CenterCrop           transforms.Grayscale           transforms.ColorJitter          </span>
<span class="hljs-comment"># transforms.Lambda               transforms.Compose             transforms.LinearTransformation </span>
<span class="hljs-comment"># transforms.FiveCrop             transforms.Normalize           transforms.functional           </span>
<span class="hljs-comment"># transforms.Pad                  transforms.RandomAffine        transforms.RandomHorizontalFlip  </span>
<span class="hljs-comment"># transforms.RandomApply          transforms.RandomOrder         transforms.RandomChoice         </span>
<span class="hljs-comment"># transforms.RandomResizedCrop    transforms.RandomCrop          transforms.RandomRotation        </span>
<span class="hljs-comment"># transforms.RandomGrayscale      transforms.RandomSizedCrop     transforms.RandomVerticalFlip   </span>
<span class="hljs-comment"># transforms.ToTensor             transforms.Resize              transforms.transforms                                           </span>
<span class="hljs-comment"># transforms.TenCrop              transforms.Scale               transforms.ToPILImage</span></code></pre>
<h5 id="自定义-dataset"><a href="#自定义-dataset" class="headerlink" title="自定义 dataset"></a>自定义 dataset</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">my_data</span>(<span class="hljs-params">Dataset</span>):</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, image_path, annotation_path, transform=None</span>):</span>
        <span class="hljs-comment"># 初始化， 读取数据集  🌟</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span>  
        <span class="hljs-comment"># 获取数据集的总大小  🌟</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span>(<span class="hljs-params">self, id</span>):</span>  🌟
        <span class="hljs-comment"># 对于制定的 id, 读取该数据并返回    </span></code></pre>
<p><strong>datasets</strong></p>
<pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, Dataloader
<span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms

transform = transforms.Compose([
        transforms.ToTensor(), <span class="hljs-comment"># convert to Tensor</span>
        transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))]) <span class="hljs-comment"># normalization</span>

dataset = ImageFolder(root, transform=transform, target_transform=<span class="hljs-literal">None</span>, loader=default_loader)
dataloader = DataLoader(dataset, <span class="hljs-number">2</span>, collate_fn=my_collate_fn, num_workers=<span class="hljs-number">1</span>,shuffle=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">for</span> batch_datas, batch_labels <span class="hljs-keyword">in</span> dataloader:
    ...</code></pre>
<h5 id="img-process"><a href="#img-process" class="headerlink" title="img process"></a>img process</h5><pre><code class="hljs python">img = make_grid(next(dataiter)[<span class="hljs-number">0</span>], <span class="hljs-number">4</span>) 
save_image(img, <span class="hljs-string">&#x27;a.png&#x27;</span>)</code></pre>
<h5 id="data-Visualization"><a href="#data-Visualization" class="headerlink" title="data Visualization"></a>data Visualization</h5><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToPILImage

show = ToPILImage()  <span class="hljs-comment"># 可以把Tensor转成Image，方便可视化</span>

(data, label) = trainset[<span class="hljs-number">100</span>]
show((data + <span class="hljs-number">1</span>) / <span class="hljs-number">2</span>).resize((<span class="hljs-number">100</span>, <span class="hljs-number">100</span>))  <span class="hljs-comment"># 应该会自动乘以 255 的</span></code></pre>
<h3 id="16-Code-Samples"><a href="#16-Code-Samples" class="headerlink" title="16. Code Samples"></a>16. Code Samples</h3><pre><code class="hljs python"><span class="hljs-comment"># torch.device object used throughout this script</span>
device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> use_cuda <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)

model = MyRNN().to(device)

<span class="hljs-comment"># train</span>
total_loss = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> input, target <span class="hljs-keyword">in</span> train_loader:
    input, target = input.to(device), target.to(device)
    hidden = input.new_zeros(*h_shape)  <span class="hljs-comment"># has the same device &amp; dtype as `input`</span>
    ...  <span class="hljs-comment"># get loss and optimize</span>
    total_loss += loss.item()           <span class="hljs-comment"># get Python number from 1-element Tensor</span>

<span class="hljs-comment"># evaluate</span>
<span class="hljs-keyword">with</span> torch.no_grad():                   <span class="hljs-comment"># operations inside don&#x27;t track history</span>
    <span class="hljs-keyword">for</span> input, target <span class="hljs-keyword">in</span> test_loader:
        ...</code></pre>
<h3 id="17-jit-amp-torchscript"><a href="#17-jit-amp-torchscript" class="headerlink" title="17. jit &amp; torchscript"></a>17. jit &amp; torchscript</h3><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.jit <span class="hljs-keyword">import</span> script, trace
torch.jit.trace(model, torch.rand(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)) 　<span class="hljs-comment"># export model</span>
<span class="hljs-meta">@torch.jit.script</span></code></pre>
<pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;torch/torch.h&gt;</span></span>
<span class="hljs-meta">#<span class="hljs-meta-keyword">include</span> <span class="hljs-meta-string">&lt;torch/script.h&gt;</span></span>

<span class="hljs-meta"># img blob -&gt; img tensor</span>
torch::Tensor img_tensor = torch::from_blob(image.data, &#123;<span class="hljs-number">1</span>, image.rows, image.cols, <span class="hljs-number">3</span>&#125;, torch::kByte);
img_tensor = img_tensor.permute(&#123;<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>&#125;);
img_tensor = img_tensor.toType(torch::kFloat);
img_tensor = img_tensor.div(<span class="hljs-number">255</span>);
<span class="hljs-meta"># load model</span>
<span class="hljs-built_in">std</span>::<span class="hljs-built_in">shared_ptr</span>&lt;torch::jit::script::Module&gt; <span class="hljs-keyword">module</span> = torch::jit::load(<span class="hljs-string">&quot;resnet.pt&quot;</span>);
<span class="hljs-meta"># forward</span>
torch::Tensor output = <span class="hljs-keyword">module</span>-&gt;forward(&#123;img_tensor&#125;).toTensor();</code></pre>
<h3 id="18-onnx"><a href="#18-onnx" class="headerlink" title="18. onnx"></a>18. onnx</h3><pre><code class="hljs python">torch.onnx.export(model, dummy data, xxxx.proto) <span class="hljs-comment"># exports an ONNX formatted</span>

model = onnx.load(<span class="hljs-string">&quot;alexnet.proto&quot;</span>)               <span class="hljs-comment"># load an ONNX model</span>
onnx.checker.check_model(model)                  <span class="hljs-comment"># check that the model</span>

onnx.helper.printable_graph(model.graph)         <span class="hljs-comment"># print a human readable　representation of the graph</span></code></pre>
<h3 id="19-Distributed-Training"><a href="#19-Distributed-Training" class="headerlink" title="19. Distributed Training"></a>19. Distributed Training</h3><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist          <span class="hljs-comment"># distributed communication</span>
<span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> Process       <span class="hljs-comment"># memory sharing processes</span></code></pre>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E8%AF%AD%E8%A8%80%E5%92%8C%E5%BA%93/">语言和库</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/pytorch/">pytorch</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2019/03/10/model-evaluation/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">model_evaluation</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2019/03/08/The-next-step-of-machine-learning/">
                        <span class="hidden-mobile">The next step of machine learning</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '#post-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "pytorch API&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script>

  
















</body>
</html>
